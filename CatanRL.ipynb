{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1745272047711,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"QAdl_8jWB0x7"},"outputs":[],"source":["# Add this to the first cell of your notebook\n","%load_ext autoreload\n","%autoreload 2  # Reload all modules (except those excluded) before executing code"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1745272047724,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"7qh7Y0T7_t6i"},"outputs":[],"source":["import os\n","# os.environ['CUDA_VISIBLE_DEVICES'] = ''   # comment this line if you want GPU again"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1745272047755,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"pnU2iekEOITt"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23731,"status":"ok","timestamp":1745272071488,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"pyqs4AMC9RBn","outputId":"e9f46e95-3105-4e8c-f02c-99c780ae6631"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5046,"status":"ok","timestamp":1745272076537,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"NdAJJ_up9TaG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa81f1bc-0b51-40a4-ee6b-eb3297a1e00b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install tqdm psutil plotly kaleido --quiet\n","import os\n","import sys\n","import random\n","import time\n","import threading\n","import IPython\n","from google.colab import output\n","from datetime import datetime"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137460,"status":"ok","timestamp":1745272213999,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"sIKijZZvPOCr","outputId":"d42c2a23-dd53-4e63-ccec-f49670f4eeda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing compatible package versions...\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mPyTorch post-install: 2.5.1+cu124\n","NumPy post-install: 2.0.1\n","CUDA setup: available=True, device count=1\n","Current CUDA device: 0, name: Tesla T4\n"]}],"source":["# Ensure version compatibility with local setup\n","print(\"Installing compatible package versions...\")\n","\n","# Install specific versions to match local setup\n","!pip install torch==2.5.1 numpy==2.0.1 --quiet\n","import numpy as np\n","\n","\n","# Verify PyTorch and NumPy versions after installation\n","!python -c \"import torch; print(f'PyTorch post-install: {torch.__version__}')\"\n","!python -c \"import numpy; print(f'NumPy post-install: {numpy.__version__}')\"\n","\n","# Force CUDA setup for PyTorch\n","import torch\n","print(f\"CUDA setup: available={torch.cuda.is_available()}, device count={torch.cuda.device_count() if torch.cuda.is_available() else 0}\")\n","if torch.cuda.is_available():\n","    print(f\"Current CUDA device: {torch.cuda.current_device()}, name: {torch.cuda.get_device_name()}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":788,"status":"ok","timestamp":1745272214789,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"VTL-hrwJ9eno","outputId":"573cac6c-ecdb-445f-fe5f-7c3797afaa7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CatanRL\n"]}],"source":["# Set path to your project on Google Drive\n","DRIVE_PATH = '/content/drive/MyDrive/CatanRL'\n","\n","# Change to the project directory\n","%cd {DRIVE_PATH}\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1745272214811,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"f2u4zvSB_4S-","outputId":"ba082124-5820-41e0-b1c2-ee781d13784f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting keep‑alive thread …\n"]}],"source":["import time, threading\n","from google.colab import output\n","\n","# 1. Define a dummy no‑op Python callback.\n","def _noop():\n","    return \"ok\"\n","\n","# 2. Register it once – gives us a handle \"keep_alive\"\n","output.register_callback('keep_alive', _noop)\n","\n","def keep_colab_alive(interval_sec: int = 60):\n","    \"\"\"Ping the front‑end every <interval_sec> seconds.\n","\n","    Works in 2025‑04 Colab because it uses the same mechanism Colab widgets use.\n","    \"\"\"\n","    while True:\n","        try:\n","            # JS in the page calls the Python no‑op; the round‑trip is what matters\n","            output.eval_js('google.colab.kernel.invokeFunction(\"keep_alive\", [], {})')\n","            print(\"♥\", end=\"\", flush=True)\n","        except Exception:\n","            # If the socket was momentarily closed, ignore and retry\n","            pass\n","        time.sleep(interval_sec)\n","\n","print(\"Starting keep‑alive thread …\")\n","threading.Thread(target=keep_colab_alive, daemon=True).start()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOm4-LNE9GuY","outputId":"c90b8973-67a0-4aef-dd94-2585acc1e953"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in OVERNIGHT mode\n","\n","=== AlphaZero Catan Training ===\n","Iterations: 200\n","Self-play games per iteration: 14\n","MCTS simulations per move: 150\n","Resume from: models/model_iter_116.pt\n","[2025-04-21 21:50:37] AlphaZero Catan Training started at 20250421_215037\n","[2025-04-21 21:50:37] Configuration: {'state_dim': 992, 'action_dim': 200, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_iterations': 200, 'self_play_games': 14, 'eval_games': 10, 'epochs': 10, 'batch_size': 256, 'buffer_size': 100000, 'num_simulations': 150, 'c_puct': 1.5, 'mcts_batch_size': 12, 'max_moves': 200, 'device': 'cpu', 'model_dir': 'models'}\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/CatanRL/AlphaZero/training/training_pipeline.py:291: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(path)\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 21:50:51] Replay buffer loaded: 49361 examples, 199.1 MB\n","[2025-04-21 21:50:51] Checkpoint loaded from models/model_iter_116.pt, resuming from iteration 116\n","[2025-04-21 21:50:51] Resuming training from iteration 116\n","[2025-04-21 21:50:53] \n","=== Iteration 117/200 ===\n","[2025-04-21 21:50:53] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9467.668560\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:33<20:10, 93.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:38<01:29, 12.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  71%|███████▏  | 10/14 [04:10<01:29, 22.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  79%|███████▊  | 11/14 [04:19<00:55, 18.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:21<00:00, 18.69s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 21:55:14] Self-play completed in 261.66s, generated 3166 examples (12.1 games/s)\n","[2025-04-21 21:55:14] Training network...\n","Epoch 1/10: Loss 1.7880 (Value 0.0218, Policy 1.7661)\n","Epoch 2/10: Loss 1.7819 (Value 0.0147, Policy 1.7672)\n","Epoch 3/10: Loss 1.7949 (Value 0.0113, Policy 1.7836)\n","Epoch 4/10: Loss 1.7715 (Value 0.0069, Policy 1.7646)\n","Epoch 5/10: Loss 1.7261 (Value 0.0050, Policy 1.7211)\n","Epoch 6/10: Loss 1.7501 (Value 0.0043, Policy 1.7459)\n","Epoch 7/10: Loss 1.7409 (Value 0.0036, Policy 1.7373)\n","Epoch 8/10: Loss 1.7374 (Value 0.0037, Policy 1.7337)\n","Epoch 9/10: Loss 1.7450 (Value 0.0033, Policy 1.7416)\n","Epoch 10/10: Loss 1.7267 (Value 0.0032, Policy 1.7235)\n","[2025-04-21 21:55:33] Training completed in 18.75s\n","[2025-04-21 21:55:33] Iteration 117 done in 280.42s\n","[2025-04-21 21:55:33] Resource usage: CPU 46.4%, RAM 6.9%, GPU peak memory 0.00 GB\n","[2025-04-21 21:55:33] Network parameter sum after training: 9445.940418\n","[2025-04-21 21:55:33] \n","=== Iteration 118/200 ===\n","[2025-04-21 21:55:33] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9445.940479\n","Win reward: base=1.00, time_bonus=0.37 (moves: 53/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [00:49<10:48, 49.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▍        | 2/14 [01:51<11:18, 56.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.03 (moves: 189/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▌      | 5/14 [02:11<02:25, 16.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [04:20<01:08, 22.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:38<00:00, 19.92s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:00:12] Self-play completed in 278.92s, generated 3221 examples (11.5 games/s)\n","[2025-04-21 22:00:12] Training network...\n","Epoch 1/10: Loss 1.8169 (Value 0.0210, Policy 1.7960)\n","Epoch 2/10: Loss 1.7875 (Value 0.0112, Policy 1.7764)\n","Epoch 3/10: Loss 1.7802 (Value 0.0077, Policy 1.7725)\n","Epoch 4/10: Loss 1.7585 (Value 0.0076, Policy 1.7509)\n","Epoch 5/10: Loss 1.7826 (Value 0.0056, Policy 1.7770)\n","Epoch 6/10: Loss 1.7627 (Value 0.0050, Policy 1.7577)\n","Epoch 7/10: Loss 1.7646 (Value 0.0046, Policy 1.7600)\n","Epoch 8/10: Loss 1.7333 (Value 0.0049, Policy 1.7284)\n","Epoch 9/10: Loss 1.7283 (Value 0.0039, Policy 1.7244)\n","Epoch 10/10: Loss 1.7503 (Value 0.0033, Policy 1.7470)\n","[2025-04-21 22:00:31] Training completed in 18.73s\n","[2025-04-21 22:00:31] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:11<01:44, 11.56s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:00:42] Game 1: duration=11.56s, moves=200, our_VP=2, winner=1 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:24<01:37, 12.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:00:55] Game 2: duration=12.55s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:39<01:34, 13.57s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:01:10] Game 3: duration=15.27s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:56<01:30, 15.02s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:01:28] Game 4: duration=17.24s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:14<01:20, 16.04s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:01:45] Game 5: duration=17.84s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:27<00:59, 14.90s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:01:58] Game 6: duration=12.70s, moves=200, our_VP=4, winner=1 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:46<00:49, 16.38s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:02:17] Game 7: duration=19.41s, moves=200, our_VP=4, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:58<00:29, 14.84s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:02:29] Game 8: duration=11.54s, moves=200, our_VP=2, winner=1 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:14<00:15, 15.19s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:02:45] Game 9: duration=15.97s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:25<00:00, 14.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:02:56] Game 10: duration=11.25s, moves=200, our_VP=4, winner=0 VP=4\n","[2025-04-21 22:02:56] Evaluated 10 games in 145.34s (0.07 games/s)\n","[2025-04-21 22:02:56] Evaluation results: win_rate=0.60, avg_vp=5.00, avg_length=200.00, total_moves=2000\n","[2025-04-21 22:02:56] Eval resource usage: CPU 59.6%, RAM 7.0%, GPU peak memory 0.00 GB\n","[2025-04-21 22:02:56] Evaluation completed in 145.35s\n","[2025-04-21 22:02:56] Iteration 118 done in 443.00s\n","[2025-04-21 22:02:56] Resource usage: CPU 0.0%, RAM 7.0%, GPU peak memory 0.00 GB\n","[2025-04-21 22:02:56] Network parameter sum after training: 9558.900196\n","[2025-04-21 22:02:56] \n","=== Iteration 119/200 ===\n","[2025-04-21 22:02:56] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9558.899800\n","Win reward: base=1.00, time_bonus=0.12 (moves: 153/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  43%|████▎     | 6/14 [02:42<02:21, 17.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 7/14 [02:44<01:29, 12.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.34 (moves: 65/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [03:49<02:05, 25.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  71%|███████▏  | 10/14 [03:51<01:11, 17.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:24<00:12, 12.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:29<00:00, 19.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:07:25] Self-play completed in 269.03s, generated 3116 examples (11.6 games/s)\n","[2025-04-21 22:07:25] Training network...\n","Epoch 1/10: Loss 1.8320 (Value 0.0176, Policy 1.8143)\n","Epoch 2/10: Loss 1.7793 (Value 0.0127, Policy 1.7665)\n","Epoch 3/10: Loss 1.7742 (Value 0.0085, Policy 1.7657)\n","Epoch 4/10: Loss 1.7679 (Value 0.0086, Policy 1.7593)\n","Epoch 5/10: Loss 1.7611 (Value 0.0059, Policy 1.7552)\n","Epoch 6/10: Loss 1.7780 (Value 0.0050, Policy 1.7731)\n","Epoch 7/10: Loss 1.7428 (Value 0.0042, Policy 1.7386)\n","Epoch 8/10: Loss 1.7527 (Value 0.0040, Policy 1.7487)\n","Epoch 9/10: Loss 1.7370 (Value 0.0037, Policy 1.7332)\n","Epoch 10/10: Loss 1.7274 (Value 0.0032, Policy 1.7242)\n","[2025-04-21 22:07:44] Training completed in 18.59s\n","[2025-04-21 22:07:44] Iteration 119 done in 287.62s\n","[2025-04-21 22:07:44] Resource usage: CPU 80.0%, RAM 7.0%, GPU peak memory 0.00 GB\n","[2025-04-21 22:07:44] Network parameter sum after training: 9558.287609\n","[2025-04-21 22:07:44] \n","=== Iteration 120/200 ===\n","[2025-04-21 22:07:44] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9558.288052\n","Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:08<14:53, 68.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  21%|██▏       | 3/14 [02:09<06:36, 36.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  43%|████▎     | 6/14 [02:26<01:55, 14.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.02 (moves: 193/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [03:29<02:25, 24.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▍   | 9/14 [03:49<01:56, 23.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:31<00:00, 19.42s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:12:16] Self-play completed in 271.82s, generated 3314 examples (12.2 games/s)\n","[2025-04-21 22:12:16] Training network...\n","Epoch 1/10: Loss 1.8084 (Value 0.0221, Policy 1.7862)\n","Epoch 2/10: Loss 1.7807 (Value 0.0152, Policy 1.7654)\n","Epoch 3/10: Loss 1.7878 (Value 0.0135, Policy 1.7743)\n","Epoch 4/10: Loss 1.7601 (Value 0.0089, Policy 1.7512)\n","Epoch 5/10: Loss 1.7887 (Value 0.0078, Policy 1.7809)\n","Epoch 6/10: Loss 1.7516 (Value 0.0064, Policy 1.7451)\n","Epoch 7/10: Loss 1.7539 (Value 0.0052, Policy 1.7487)\n","Epoch 8/10: Loss 1.7638 (Value 0.0052, Policy 1.7587)\n","Epoch 9/10: Loss 1.7395 (Value 0.0042, Policy 1.7353)\n","Epoch 10/10: Loss 1.7424 (Value 0.0041, Policy 1.7383)\n","[2025-04-21 22:12:34] Training completed in 18.74s\n","[2025-04-21 22:12:34] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:13<01:57, 13.04s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:12:48] Game 1: duration=13.04s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:27<01:52, 14.10s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:13:02] Game 2: duration=14.85s, moves=200, our_VP=2, winner=1 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:43<01:43, 14.78s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:13:18] Game 3: duration=15.59s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:56<01:25, 14.21s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:13:31] Game 4: duration=13.33s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:16<01:20, 16.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:13:51] Game 5: duration=19.56s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:38<01:13, 18.33s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:14:13] Game 6: duration=22.57s, moves=191, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:56<00:54, 18.17s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:14:31] Game 7: duration=17.85s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [02:12<00:34, 17.28s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:14:47] Game 8: duration=15.36s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:29<00:17, 17.24s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:15:04] Game 9: duration=17.14s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:42<00:00, 16.25s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:15:17] Game 10: duration=13.24s, moves=200, our_VP=4, winner=0 VP=4\n","[2025-04-21 22:15:17] Evaluated 10 games in 162.55s (0.06 games/s)\n","[2025-04-21 22:15:17] Evaluation results: win_rate=0.90, avg_vp=6.60, avg_length=199.10, total_moves=1991\n","[2025-04-21 22:15:17] Eval resource usage: CPU 60.1%, RAM 7.0%, GPU peak memory 0.00 GB\n","[2025-04-21 22:15:17] Evaluation completed in 162.55s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:15:20] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-21 22:15:20] New best model at iteration 120 (win_rate=0.90)\n","[2025-04-21 22:15:20] Checkpoint saved: models/model_iter_120.pt\n","[2025-04-21 22:15:24] Best model saved: models/best_model.pt\n","[2025-04-21 22:15:26] Replay buffer saved: models/latest_buffer.pkl (62178 examples, 250.8 MB)\n","[2025-04-21 22:15:26] Iteration 120 done in 461.73s\n","[2025-04-21 22:15:26] Resource usage: CPU 16.4%, RAM 7.2%, GPU peak memory 0.00 GB\n","[2025-04-21 22:15:26] Network parameter sum after training: 9765.630958\n","[2025-04-21 22:15:26] \n","=== Iteration 121/200 ===\n","[2025-04-21 22:15:26] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9765.630974\n","Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:25<18:28, 85.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.18 (moves: 129/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  21%|██▏       | 3/14 [01:50<05:21, 29.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  29%|██▊       | 4/14 [02:15<04:37, 27.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  43%|████▎     | 6/14 [02:38<02:18, 17.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 7/14 [02:40<01:27, 12.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▌ | 12/14 [04:21<00:29, 14.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:28<00:00, 19.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:19:54] Self-play completed in 268.74s, generated 3157 examples (11.7 games/s)\n","[2025-04-21 22:19:54] Training network...\n","Epoch 1/10: Loss 1.8263 (Value 0.0153, Policy 1.8109)\n","Epoch 2/10: Loss 1.7911 (Value 0.0106, Policy 1.7805)\n","Epoch 3/10: Loss 1.7954 (Value 0.0096, Policy 1.7859)\n","Epoch 4/10: Loss 1.7886 (Value 0.0072, Policy 1.7814)\n","Epoch 5/10: Loss 1.7877 (Value 0.0060, Policy 1.7817)\n","Epoch 6/10: Loss 1.7599 (Value 0.0044, Policy 1.7555)\n","Epoch 7/10: Loss 1.7693 (Value 0.0048, Policy 1.7644)\n","Epoch 8/10: Loss 1.7731 (Value 0.0049, Policy 1.7682)\n","Epoch 9/10: Loss 1.7377 (Value 0.0043, Policy 1.7334)\n","Epoch 10/10: Loss 1.7518 (Value 0.0043, Policy 1.7475)\n","[2025-04-21 22:20:14] Training completed in 19.50s\n","[2025-04-21 22:20:14] Iteration 121 done in 288.24s\n","[2025-04-21 22:20:14] Resource usage: CPU 83.1%, RAM 7.2%, GPU peak memory 0.00 GB\n","[2025-04-21 22:20:14] Network parameter sum after training: 9793.192503\n","[2025-04-21 22:20:14] \n","=== Iteration 122/200 ===\n","[2025-04-21 22:20:14] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9793.192701\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▍        | 2/14 [02:18<11:52, 59.34s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▌      | 5/14 [02:29<02:15, 15.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  43%|████▎     | 6/14 [02:41<01:52, 14.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [04:17<03:39, 36.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [04:52<00:54, 18.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [05:00<00:11, 11.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [05:07<00:00, 21.98s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:25:22] Self-play completed in 307.77s, generated 3671 examples (11.9 games/s)\n","[2025-04-21 22:25:22] Training network...\n","Epoch 1/10: Loss 1.8431 (Value 0.0186, Policy 1.8245)\n","Epoch 2/10: Loss 1.8204 (Value 0.0133, Policy 1.8071)\n","Epoch 3/10: Loss 1.7703 (Value 0.0107, Policy 1.7597)\n","Epoch 4/10: Loss 1.7745 (Value 0.0100, Policy 1.7645)\n","Epoch 5/10: Loss 1.7992 (Value 0.0075, Policy 1.7917)\n","Epoch 6/10: Loss 1.7710 (Value 0.0069, Policy 1.7641)\n","Epoch 7/10: Loss 1.7941 (Value 0.0062, Policy 1.7879)\n","Epoch 8/10: Loss 1.7609 (Value 0.0055, Policy 1.7553)\n","Epoch 9/10: Loss 1.7400 (Value 0.0057, Policy 1.7343)\n","Epoch 10/10: Loss 1.7509 (Value 0.0046, Policy 1.7463)\n","[2025-04-21 22:25:41] Training completed in 19.21s\n","[2025-04-21 22:25:41] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:09<01:29,  9.94s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:25:51] Game 1: duration=9.94s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:26<01:48, 13.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:26:07] Game 2: duration=16.16s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:40<01:36, 13.78s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:26:21] Game 3: duration=13.99s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:52<01:19, 13.31s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:26:34] Game 4: duration=12.59s, moves=200, our_VP=2, winner=3 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:12<01:17, 15.49s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:26:53] Game 5: duration=19.36s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:25<00:58, 14.65s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:27:06] Game 6: duration=13.01s, moves=200, our_VP=5, winner=2 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:39<00:43, 14.62s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:27:21] Game 7: duration=14.57s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:56<00:30, 15.42s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:27:38] Game 8: duration=17.14s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:08<00:14, 14.34s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:27:50] Game 9: duration=11.96s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:27<00:00, 14.71s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:28:08] Game 10: duration=18.36s, moves=200, our_VP=5, winner=0 VP=5\n","[2025-04-21 22:28:08] Evaluated 10 games in 147.10s (0.07 games/s)\n","[2025-04-21 22:28:08] Evaluation results: win_rate=0.80, avg_vp=5.20, avg_length=200.00, total_moves=2000\n","[2025-04-21 22:28:08] Eval resource usage: CPU 65.6%, RAM 7.3%, GPU peak memory 0.00 GB\n","[2025-04-21 22:28:08] Evaluation completed in 147.10s\n","[2025-04-21 22:28:08] New best model at iteration 122 (win_rate=0.80)\n","[2025-04-21 22:28:08] Checkpoint saved: models/model_iter_122.pt\n","[2025-04-21 22:28:08] Best model saved: models/best_model.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:28:10] Replay buffer saved: models/latest_buffer.pkl (69006 examples, 278.3 MB)\n","[2025-04-21 22:28:10] Iteration 122 done in 476.44s\n","[2025-04-21 22:28:10] Resource usage: CPU 13.4%, RAM 7.3%, GPU peak memory 0.00 GB\n","[2025-04-21 22:28:10] Network parameter sum after training: 9876.583415\n","[2025-04-21 22:28:10] \n","=== Iteration 123/200 ===\n","[2025-04-21 22:28:10] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9876.583614\n","Win reward: base=1.00, time_bonus=0.12 (moves: 153/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▍        | 2/14 [02:01<10:04, 50.41s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  21%|██▏       | 3/14 [02:06<05:25, 29.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [03:50<02:56, 29.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▍   | 9/14 [04:08<02:09, 25.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [04:24<00:48, 16.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:47<00:00, 20.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:32:58] Self-play completed in 287.52s, generated 3413 examples (11.9 games/s)\n","[2025-04-21 22:32:58] Training network...\n","Epoch 1/10: Loss 1.8186 (Value 0.0204, Policy 1.7982)\n","Epoch 2/10: Loss 1.8231 (Value 0.0141, Policy 1.8091)\n","Epoch 3/10: Loss 1.7744 (Value 0.0104, Policy 1.7640)\n","Epoch 4/10: Loss 1.7779 (Value 0.0085, Policy 1.7693)\n","Epoch 5/10: Loss 1.7988 (Value 0.0060, Policy 1.7928)\n","Epoch 6/10: Loss 1.7800 (Value 0.0061, Policy 1.7739)\n","Epoch 7/10: Loss 1.7835 (Value 0.0058, Policy 1.7777)\n","Epoch 8/10: Loss 1.7705 (Value 0.0056, Policy 1.7650)\n","Epoch 9/10: Loss 1.7706 (Value 0.0046, Policy 1.7660)\n","Epoch 10/10: Loss 1.7334 (Value 0.0044, Policy 1.7290)\n","[2025-04-21 22:33:17] Training completed in 19.42s\n","[2025-04-21 22:33:17] Iteration 123 done in 306.94s\n","[2025-04-21 22:33:17] Resource usage: CPU 84.2%, RAM 7.3%, GPU peak memory 0.00 GB\n","[2025-04-21 22:33:17] Network parameter sum after training: 9891.276048\n","[2025-04-21 22:33:17] \n","=== Iteration 124/200 ===\n","[2025-04-21 22:33:17] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9891.275682\n","Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:33<20:17, 93.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  21%|██▏       | 3/14 [01:58<05:47, 31.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [03:16<02:19, 23.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  71%|███████▏  | 10/14 [03:44<01:11, 17.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:13<00:12, 12.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:16<00:00, 18.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:37:34] Self-play completed in 256.57s, generated 3035 examples (11.8 games/s)\n","[2025-04-21 22:37:34] Training network...\n","Epoch 1/10: Loss 1.8032 (Value 0.0137, Policy 1.7895)\n","Epoch 2/10: Loss 1.8184 (Value 0.0092, Policy 1.8092)\n","Epoch 3/10: Loss 1.8170 (Value 0.0085, Policy 1.8085)\n","Epoch 4/10: Loss 1.8148 (Value 0.0062, Policy 1.8087)\n","Epoch 5/10: Loss 1.7908 (Value 0.0058, Policy 1.7850)\n","Epoch 6/10: Loss 1.7949 (Value 0.0054, Policy 1.7895)\n","Epoch 7/10: Loss 1.7870 (Value 0.0052, Policy 1.7818)\n","Epoch 8/10: Loss 1.7792 (Value 0.0047, Policy 1.7745)\n","Epoch 9/10: Loss 1.7576 (Value 0.0042, Policy 1.7534)\n","Epoch 10/10: Loss 1.7744 (Value 0.0044, Policy 1.7700)\n","[2025-04-21 22:37:53] Training completed in 19.19s\n","[2025-04-21 22:37:53] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:17<02:39, 17.67s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:38:11] Game 1: duration=17.67s, moves=178, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:30<01:56, 14.57s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:38:23] Game 2: duration=12.40s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:40<01:29, 12.80s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:38:34] Game 3: duration=10.68s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:57<01:25, 14.19s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:38:50] Game 4: duration=16.32s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:12<01:12, 14.45s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:39:05] Game 5: duration=14.92s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:27<00:58, 14.71s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:39:20] Game 6: duration=15.21s, moves=200, our_VP=4, winner=3 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:39<00:41, 13.82s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:39:32] Game 7: duration=11.97s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:50<00:25, 12.97s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:39:44] Game 8: duration=11.16s, moves=200, our_VP=5, winner=2 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:04<00:13, 13.46s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:39:58] Game 9: duration=14.52s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:17<00:00, 13.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:40:11] Game 10: duration=12.72s, moves=200, our_VP=2, winner=1 VP=4\n","[2025-04-21 22:40:11] Evaluated 10 games in 137.60s (0.07 games/s)\n","[2025-04-21 22:40:11] Evaluation results: win_rate=0.70, avg_vp=5.70, avg_length=197.80, total_moves=1978\n","[2025-04-21 22:40:11] Eval resource usage: CPU 61.9%, RAM 7.3%, GPU peak memory 0.00 GB\n","[2025-04-21 22:40:11] Evaluation completed in 137.60s\n","[2025-04-21 22:40:11] Iteration 124 done in 413.36s\n","[2025-04-21 22:40:11] Resource usage: CPU 0.0%, RAM 7.3%, GPU peak memory 0.00 GB\n","[2025-04-21 22:40:11] Network parameter sum after training: 9912.325491\n","[2025-04-21 22:40:11] \n","=== Iteration 125/200 ===\n","[2025-04-21 22:40:11] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9912.325033\n","Win reward: base=1.00, time_bonus=0.13 (moves: 149/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:14<03:27, 20.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:52<01:57, 16.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [03:52<02:04, 24.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  71%|███████▏  | 10/14 [04:02<01:20, 20.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.09 (moves: 165/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▌ | 12/14 [04:26<00:32, 16.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:48<00:00, 20.59s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:44:59] Self-play completed in 288.27s, generated 3327 examples (11.5 games/s)\n","[2025-04-21 22:44:59] Training network...\n","Epoch 1/10: Loss 1.8266 (Value 0.0129, Policy 1.8136)\n","Epoch 2/10: Loss 1.8190 (Value 0.0079, Policy 1.8112)\n","Epoch 3/10: Loss 1.7998 (Value 0.0084, Policy 1.7914)\n","Epoch 4/10: Loss 1.8087 (Value 0.0055, Policy 1.8032)\n","Epoch 5/10: Loss 1.7786 (Value 0.0053, Policy 1.7734)\n","Epoch 6/10: Loss 1.7799 (Value 0.0038, Policy 1.7761)\n","Epoch 7/10: Loss 1.7919 (Value 0.0044, Policy 1.7875)\n","Epoch 8/10: Loss 1.7685 (Value 0.0040, Policy 1.7645)\n","Epoch 9/10: Loss 1.7571 (Value 0.0037, Policy 1.7534)\n","Epoch 10/10: Loss 1.7552 (Value 0.0031, Policy 1.7521)\n","[2025-04-21 22:45:20] Training completed in 21.29s\n","[2025-04-21 22:45:21] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-21 22:45:21] Checkpoint saved: models/model_iter_125.pt\n","[2025-04-21 22:45:23] Replay buffer saved: models/latest_buffer.pkl (78781 examples, 317.7 MB)\n","[2025-04-21 22:45:23] Iteration 125 done in 312.69s\n","[2025-04-21 22:45:23] Resource usage: CPU 78.9%, RAM 7.4%, GPU peak memory 0.00 GB\n","[2025-04-21 22:45:23] Network parameter sum after training: 9980.458586\n","[2025-04-21 22:45:24] \n","=== Iteration 126/200 ===\n","[2025-04-21 22:45:24] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 9980.458144\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:49<23:49, 110.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:11<03:28, 20.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [03:53<03:17, 32.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:41<00:00, 20.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:50:05] Self-play completed in 281.05s, generated 3328 examples (11.8 games/s)\n","[2025-04-21 22:50:05] Training network...\n","Epoch 1/10: Loss 1.8171 (Value 0.0140, Policy 1.8031)\n","Epoch 2/10: Loss 1.8385 (Value 0.0107, Policy 1.8279)\n","Epoch 3/10: Loss 1.7989 (Value 0.0089, Policy 1.7899)\n","Epoch 4/10: Loss 1.7843 (Value 0.0077, Policy 1.7766)\n","Epoch 5/10: Loss 1.7911 (Value 0.0067, Policy 1.7843)\n","Epoch 6/10: Loss 1.7877 (Value 0.0064, Policy 1.7812)\n","Epoch 7/10: Loss 1.7863 (Value 0.0061, Policy 1.7802)\n","Epoch 8/10: Loss 1.7882 (Value 0.0046, Policy 1.7837)\n","Epoch 9/10: Loss 1.7758 (Value 0.0047, Policy 1.7711)\n","Epoch 10/10: Loss 1.7660 (Value 0.0038, Policy 1.7622)\n","[2025-04-21 22:50:24] Training completed in 19.01s\n","[2025-04-21 22:50:24] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:14<02:12, 14.71s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:50:38] Game 1: duration=14.71s, moves=130, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:32<02:11, 16.49s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:50:56] Game 2: duration=17.74s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:47<01:50, 15.79s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:51:11] Game 3: duration=14.94s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:00<01:29, 14.91s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:51:25] Game 4: duration=13.57s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:13<01:09, 13.97s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:51:37] Game 5: duration=12.31s, moves=117, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:26<00:54, 13.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:51:50] Game 6: duration=12.87s, moves=200, our_VP=2, winner=1 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:40<00:41, 13.92s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:52:04] Game 7: duration=14.59s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:55<00:28, 14.22s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:52:19] Game 8: duration=14.87s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:09<00:14, 14.12s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:52:33] Game 9: duration=13.89s, moves=200, our_VP=2, winner=3 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:25<00:00, 14.57s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:52:49] Game 10: duration=16.20s, moves=200, our_VP=4, winner=0 VP=4\n","[2025-04-21 22:52:49] Evaluated 10 games in 145.70s (0.07 games/s)\n","[2025-04-21 22:52:49] Evaluation results: win_rate=0.80, avg_vp=5.30, avg_length=184.70, total_moves=1847\n","[2025-04-21 22:52:49] Eval resource usage: CPU 63.4%, RAM 7.4%, GPU peak memory 0.00 GB\n","[2025-04-21 22:52:49] Evaluation completed in 145.70s\n","[2025-04-21 22:52:49] New best model at iteration 126 (win_rate=0.80)\n","[2025-04-21 22:52:49] Checkpoint saved: models/model_iter_126.pt\n","[2025-04-21 22:52:49] Best model saved: models/best_model.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:52:52] Replay buffer saved: models/latest_buffer.pkl (82109 examples, 331.2 MB)\n","[2025-04-21 22:52:52] Iteration 126 done in 448.57s\n","[2025-04-21 22:52:52] Resource usage: CPU 12.8%, RAM 7.5%, GPU peak memory 0.00 GB\n","[2025-04-21 22:52:52] Network parameter sum after training: 10048.246908\n","[2025-04-21 22:52:52] \n","=== Iteration 127/200 ===\n","[2025-04-21 22:52:52] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10048.245977\n","Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [02:03<26:39, 123.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▍        | 2/14 [02:06<10:35, 52.97s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.01 (moves: 197/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [04:01<03:23, 33.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:33<00:00, 19.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 22:57:26] Self-play completed in 273.58s, generated 3294 examples (12.0 games/s)\n","[2025-04-21 22:57:26] Training network...\n","Epoch 1/10: Loss 1.8275 (Value 0.0164, Policy 1.8111)\n","Epoch 2/10: Loss 1.8160 (Value 0.0118, Policy 1.8042)\n","Epoch 3/10: Loss 1.7910 (Value 0.0087, Policy 1.7823)\n","Epoch 4/10: Loss 1.8072 (Value 0.0093, Policy 1.7979)\n","Epoch 5/10: Loss 1.7905 (Value 0.0076, Policy 1.7829)\n","Epoch 6/10: Loss 1.7849 (Value 0.0068, Policy 1.7781)\n","Epoch 7/10: Loss 1.7815 (Value 0.0061, Policy 1.7754)\n","Epoch 8/10: Loss 1.7775 (Value 0.0048, Policy 1.7727)\n","Epoch 9/10: Loss 1.7814 (Value 0.0040, Policy 1.7774)\n","Epoch 10/10: Loss 1.7769 (Value 0.0037, Policy 1.7731)\n","[2025-04-21 22:57:45] Training completed in 19.10s\n","[2025-04-21 22:57:45] Iteration 127 done in 292.68s\n","[2025-04-21 22:57:45] Resource usage: CPU 87.2%, RAM 7.5%, GPU peak memory 0.00 GB\n","[2025-04-21 22:57:45] Network parameter sum after training: 10078.889251\n","[2025-04-21 22:57:45] \n","=== Iteration 128/200 ===\n","[2025-04-21 22:57:45] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10078.888763\n","Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:24<18:16, 84.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  21%|██▏       | 3/14 [02:20<07:18, 39.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:30<01:03,  9.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  57%|█████▋    | 8/14 [03:22<02:03, 20.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.03 (moves: 189/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  71%|███████▏  | 10/14 [03:54<01:08, 17.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:16<00:10, 10.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:18<00:00, 18.47s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:02:03] Self-play completed in 258.60s, generated 3105 examples (12.0 games/s)\n","[2025-04-21 23:02:03] Training network...\n","Epoch 1/10: Loss 1.8368 (Value 0.0118, Policy 1.8250)\n","Epoch 2/10: Loss 1.7892 (Value 0.0082, Policy 1.7810)\n","Epoch 3/10: Loss 1.8014 (Value 0.0063, Policy 1.7951)\n","Epoch 4/10: Loss 1.7861 (Value 0.0069, Policy 1.7792)\n","Epoch 5/10: Loss 1.8084 (Value 0.0066, Policy 1.8018)\n","Epoch 6/10: Loss 1.7879 (Value 0.0069, Policy 1.7810)\n","Epoch 7/10: Loss 1.8001 (Value 0.0055, Policy 1.7946)\n","Epoch 8/10: Loss 1.7944 (Value 0.0049, Policy 1.7895)\n","Epoch 9/10: Loss 1.7773 (Value 0.0051, Policy 1.7723)\n","Epoch 10/10: Loss 1.7630 (Value 0.0037, Policy 1.7593)\n","[2025-04-21 23:02:23] Training completed in 19.15s\n","[2025-04-21 23:02:23] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:14<02:13, 14.86s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:02:37] Game 1: duration=14.86s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:32<02:14, 16.78s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:02:56] Game 2: duration=18.12s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:48<01:52, 16.11s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:03:11] Game 3: duration=15.31s, moves=200, our_VP=5, winner=2 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:06<01:40, 16.77s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:03:29] Game 4: duration=17.78s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:25<01:28, 17.68s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:03:48] Game 5: duration=19.29s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:33<00:57, 14.46s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:03:56] Game 6: duration=8.22s, moves=200, our_VP=2, winner=3 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:44<00:39, 13.32s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:04:07] Game 7: duration=10.97s, moves=200, our_VP=2, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:56<00:25, 12.86s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:04:19] Game 8: duration=11.87s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:09<00:12, 12.79s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:04:32] Game 9: duration=12.65s, moves=200, our_VP=3, winner=1 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:24<00:00, 14.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:04:47] Game 10: duration=15.36s, moves=200, our_VP=5, winner=0 VP=5\n","[2025-04-21 23:04:47] Evaluated 10 games in 144.44s (0.07 games/s)\n","[2025-04-21 23:04:47] Evaluation results: win_rate=0.60, avg_vp=5.20, avg_length=200.00, total_moves=2000\n","[2025-04-21 23:04:47] Eval resource usage: CPU 62.6%, RAM 7.5%, GPU peak memory 0.00 GB\n","[2025-04-21 23:04:47] Evaluation completed in 144.44s\n","[2025-04-21 23:04:47] Iteration 128 done in 422.20s\n","[2025-04-21 23:04:47] Resource usage: CPU 0.0%, RAM 7.5%, GPU peak memory 0.00 GB\n","[2025-04-21 23:04:47] Network parameter sum after training: 10184.446235\n","[2025-04-21 23:04:47] \n","=== Iteration 129/200 ===\n","[2025-04-21 23:04:47] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10184.446373\n","Win reward: base=1.00, time_bonus=0.34 (moves: 65/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [00:54<11:47, 54.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  21%|██▏       | 3/14 [02:08<07:48, 42.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  29%|██▊       | 4/14 [02:11<04:30, 27.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  36%|███▌      | 5/14 [02:14<02:45, 18.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:36<01:42, 14.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [03:42<02:11, 26.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [03:59<00:51, 17.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:37<00:00, 19.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:09:24] Self-play completed in 277.04s, generated 3160 examples (11.4 games/s)\n","[2025-04-21 23:09:24] Training network...\n","Epoch 1/10: Loss 1.8222 (Value 0.0139, Policy 1.8083)\n","Epoch 2/10: Loss 1.8033 (Value 0.0129, Policy 1.7904)\n","Epoch 3/10: Loss 1.8073 (Value 0.0085, Policy 1.7988)\n","Epoch 4/10: Loss 1.8018 (Value 0.0065, Policy 1.7953)\n","Epoch 5/10: Loss 1.7947 (Value 0.0080, Policy 1.7867)\n","Epoch 6/10: Loss 1.7994 (Value 0.0065, Policy 1.7930)\n","Epoch 7/10: Loss 1.7895 (Value 0.0054, Policy 1.7841)\n","Epoch 8/10: Loss 1.7893 (Value 0.0047, Policy 1.7847)\n","Epoch 9/10: Loss 1.7824 (Value 0.0045, Policy 1.7779)\n","Epoch 10/10: Loss 1.7671 (Value 0.0042, Policy 1.7629)\n","[2025-04-21 23:09:43] Training completed in 19.03s\n","[2025-04-21 23:09:43] Iteration 129 done in 296.07s\n","[2025-04-21 23:09:43] Resource usage: CPU 77.9%, RAM 7.6%, GPU peak memory 0.00 GB\n","[2025-04-21 23:09:43] Network parameter sum after training: 10231.386556\n","[2025-04-21 23:09:43] \n","=== Iteration 130/200 ===\n","[2025-04-21 23:09:43] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10231.387899\n","Win reward: base=1.00, time_bonus=0.24 (moves: 105/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▍        | 2/14 [01:46<09:22, 46.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  21%|██▏       | 3/14 [01:47<04:46, 26.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  71%|███████▏  | 10/14 [04:13<01:25, 21.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:41<00:00, 20.11s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:14:25] Self-play completed in 281.56s, generated 3275 examples (11.6 games/s)\n","[2025-04-21 23:14:25] Training network...\n","Epoch 1/10: Loss 1.8315 (Value 0.0126, Policy 1.8190)\n","Epoch 2/10: Loss 1.8015 (Value 0.0094, Policy 1.7922)\n","Epoch 3/10: Loss 1.8175 (Value 0.0102, Policy 1.8073)\n","Epoch 4/10: Loss 1.8282 (Value 0.0084, Policy 1.8198)\n","Epoch 5/10: Loss 1.8302 (Value 0.0069, Policy 1.8233)\n","Epoch 6/10: Loss 1.7867 (Value 0.0065, Policy 1.7803)\n","Epoch 7/10: Loss 1.7896 (Value 0.0063, Policy 1.7833)\n","Epoch 8/10: Loss 1.7981 (Value 0.0056, Policy 1.7925)\n","Epoch 9/10: Loss 1.7733 (Value 0.0053, Policy 1.7680)\n","Epoch 10/10: Loss 1.8135 (Value 0.0065, Policy 1.8069)\n","[2025-04-21 23:14:44] Training completed in 19.28s\n","[2025-04-21 23:14:44] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:16<02:25, 16.22s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:15:00] Game 1: duration=16.22s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:34<02:20, 17.53s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:15:19] Game 2: duration=18.44s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:46<01:44, 14.91s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:15:31] Game 3: duration=11.80s, moves=200, our_VP=4, winner=2 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:01<01:29, 14.98s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:15:46] Game 4: duration=15.08s, moves=186, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:16<01:14, 14.97s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:16:01] Game 5: duration=14.96s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:29<00:57, 14.28s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:16:13] Game 6: duration=12.92s, moves=200, our_VP=2, winner=1 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:43<00:42, 14.07s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:16:27] Game 7: duration=13.65s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:57<00:28, 14.12s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:16:41] Game 8: duration=14.22s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:15<00:15, 15.35s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:16:59] Game 9: duration=18.04s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:26<00:00, 14.63s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:17:10] Game 10: duration=10.99s, moves=156, our_VP=2, winner=2 VP=10\n","[2025-04-21 23:17:10] Evaluated 10 games in 146.35s (0.07 games/s)\n","[2025-04-21 23:17:10] Evaluation results: win_rate=0.70, avg_vp=5.30, avg_length=194.20, total_moves=1942\n","[2025-04-21 23:17:10] Eval resource usage: CPU 62.6%, RAM 7.6%, GPU peak memory 0.00 GB\n","[2025-04-21 23:17:10] Evaluation completed in 146.35s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:17:11] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-21 23:17:11] Checkpoint saved: models/model_iter_130.pt\n","[2025-04-21 23:17:14] Replay buffer saved: models/latest_buffer.pkl (94943 examples, 383.0 MB)\n","[2025-04-21 23:17:14] Iteration 130 done in 450.57s\n","[2025-04-21 23:17:14] Resource usage: CPU 13.4%, RAM 7.6%, GPU peak memory 0.00 GB\n","[2025-04-21 23:17:14] Network parameter sum after training: 10349.340423\n","[2025-04-21 23:17:14] \n","=== Iteration 131/200 ===\n","[2025-04-21 23:17:14] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10349.340469\n","Win reward: base=1.00, time_bonus=0.30 (moves: 81/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:05<14:08, 65.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  71%|███████▏  | 10/14 [04:03<01:26, 21.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  79%|███████▊  | 11/14 [04:08<00:49, 16.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:41<00:00, 20.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:21:56] Self-play completed in 281.82s, generated 3182 examples (11.3 games/s)\n","[2025-04-21 23:21:56] Training network...\n","Epoch 1/10: Loss 1.8142 (Value 0.0155, Policy 1.7988)\n","Epoch 2/10: Loss 1.8261 (Value 0.0114, Policy 1.8147)\n","Epoch 3/10: Loss 1.8184 (Value 0.0121, Policy 1.8063)\n","Epoch 4/10: Loss 1.8107 (Value 0.0080, Policy 1.8027)\n","Epoch 5/10: Loss 1.8133 (Value 0.0087, Policy 1.8046)\n","Epoch 6/10: Loss 1.8102 (Value 0.0085, Policy 1.8017)\n","Epoch 7/10: Loss 1.7712 (Value 0.0073, Policy 1.7640)\n","Epoch 8/10: Loss 1.8136 (Value 0.0049, Policy 1.8087)\n","Epoch 9/10: Loss 1.7980 (Value 0.0056, Policy 1.7923)\n","Epoch 10/10: Loss 1.8065 (Value 0.0054, Policy 1.8011)\n","[2025-04-21 23:22:15] Training completed in 19.56s\n","[2025-04-21 23:22:15] Iteration 131 done in 301.39s\n","[2025-04-21 23:22:15] Resource usage: CPU 80.8%, RAM 7.6%, GPU peak memory 0.00 GB\n","[2025-04-21 23:22:15] Network parameter sum after training: 10343.259210\n","[2025-04-21 23:22:15] \n","=== Iteration 132/200 ===\n","[2025-04-21 23:22:15] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10343.258370\n","Win reward: base=1.00, time_bonus=0.18 (moves: 129/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▍        | 2/14 [01:48<09:05, 45.48s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  43%|████▎     | 6/14 [02:30<01:48, 13.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 7/14 [02:53<01:55, 16.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.18 (moves: 129/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [04:11<02:14, 26.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [05:06<00:00, 21.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:27:22] Self-play completed in 306.40s, generated 3471 examples (11.3 games/s)\n","[2025-04-21 23:27:22] Training network...\n","Epoch 1/10: Loss 1.8503 (Value 0.0166, Policy 1.8337)\n","Epoch 2/10: Loss 1.8462 (Value 0.0126, Policy 1.8336)\n","Epoch 3/10: Loss 1.8416 (Value 0.0106, Policy 1.8310)\n","Epoch 4/10: Loss 1.8110 (Value 0.0091, Policy 1.8019)\n","Epoch 5/10: Loss 1.8214 (Value 0.0090, Policy 1.8124)\n","Epoch 6/10: Loss 1.7948 (Value 0.0078, Policy 1.7871)\n","Epoch 7/10: Loss 1.7953 (Value 0.0067, Policy 1.7886)\n","Epoch 8/10: Loss 1.8013 (Value 0.0070, Policy 1.7943)\n","Epoch 9/10: Loss 1.7994 (Value 0.0062, Policy 1.7933)\n","Epoch 10/10: Loss 1.7854 (Value 0.0053, Policy 1.7801)\n","[2025-04-21 23:27:41] Training completed in 18.98s\n","[2025-04-21 23:27:41] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:13<02:05, 13.89s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:27:54] Game 1: duration=13.89s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:26<01:45, 13.20s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:28:07] Game 2: duration=12.72s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:36<01:22, 11.75s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:28:17] Game 3: duration=10.02s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:49<01:13, 12.33s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:28:30] Game 4: duration=13.21s, moves=200, our_VP=2, winner=2 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:06<01:10, 14.06s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:28:48] Game 5: duration=17.14s, moves=200, our_VP=2, winner=1 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:20<00:55, 13.88s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:29:01] Game 6: duration=13.52s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:34<00:41, 13.77s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:29:15] Game 7: duration=13.54s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:45<00:26, 13.02s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:29:26] Game 8: duration=11.41s, moves=200, our_VP=2, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [01:57<00:12, 12.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:29:38] Game 9: duration=11.70s, moves=200, our_VP=2, winner=2 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:13<00:00, 13.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:29:54] Game 10: duration=16.47s, moves=196, our_VP=10, winner=0 VP=10\n","[2025-04-21 23:29:54] Evaluated 10 games in 133.63s (0.07 games/s)\n","[2025-04-21 23:29:54] Evaluation results: win_rate=0.60, avg_vp=4.50, avg_length=199.60, total_moves=1996\n","[2025-04-21 23:29:54] Eval resource usage: CPU 63.1%, RAM 7.7%, GPU peak memory 0.00 GB\n","[2025-04-21 23:29:54] Evaluation completed in 133.63s\n","[2025-04-21 23:29:54] Iteration 132 done in 459.01s\n","[2025-04-21 23:29:54] Resource usage: CPU 50.0%, RAM 7.7%, GPU peak memory 0.00 GB\n","[2025-04-21 23:29:54] Network parameter sum after training: 10426.084077\n","[2025-04-21 23:29:54] \n","=== Iteration 133/200 ===\n","[2025-04-21 23:29:54] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10426.083986\n","Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [04:11<00:48, 16.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  86%|████████▌ | 12/14 [04:15<00:24, 12.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:46<00:00, 20.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:34:40] Self-play completed in 286.20s, generated 3314 examples (11.6 games/s)\n","[2025-04-21 23:34:40] Training network...\n","Epoch 1/10: Loss 1.8319 (Value 0.0141, Policy 1.8178)\n","Epoch 2/10: Loss 1.8487 (Value 0.0103, Policy 1.8384)\n","Epoch 3/10: Loss 1.8024 (Value 0.0080, Policy 1.7944)\n","Epoch 4/10: Loss 1.8198 (Value 0.0068, Policy 1.8130)\n","Epoch 5/10: Loss 1.8571 (Value 0.0055, Policy 1.8516)\n","Epoch 6/10: Loss 1.8439 (Value 0.0055, Policy 1.8384)\n","Epoch 7/10: Loss 1.8051 (Value 0.0050, Policy 1.8001)\n","Epoch 8/10: Loss 1.7884 (Value 0.0042, Policy 1.7842)\n","Epoch 9/10: Loss 1.8250 (Value 0.0048, Policy 1.8202)\n","Epoch 10/10: Loss 1.7770 (Value 0.0040, Policy 1.7730)\n","[2025-04-21 23:35:00] Training completed in 20.00s\n","[2025-04-21 23:35:00] Iteration 133 done in 306.20s\n","[2025-04-21 23:35:00] Resource usage: CPU 80.7%, RAM 7.7%, GPU peak memory 0.00 GB\n","[2025-04-21 23:35:00] Network parameter sum after training: 10443.626084\n","[2025-04-21 23:35:00] \n","=== Iteration 134/200 ===\n","[2025-04-21 23:35:01] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10443.625336\n","Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:27<03:36, 21.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  36%|███▌      | 5/14 [02:29<02:09, 14.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  43%|████▎     | 6/14 [02:37<01:37, 12.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 7/14 [02:38<00:59,  8.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:40<00:12, 12.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:42<00:00, 20.18s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:39:43] Self-play completed in 282.56s, generated 3383 examples (12.0 games/s)\n","[2025-04-21 23:39:43] Training network...\n","Epoch 1/10: Loss 1.8570 (Value 0.0141, Policy 1.8428)\n","Epoch 2/10: Loss 1.8332 (Value 0.0131, Policy 1.8201)\n","Epoch 3/10: Loss 1.8540 (Value 0.0099, Policy 1.8441)\n","Epoch 4/10: Loss 1.8420 (Value 0.0084, Policy 1.8336)\n","Epoch 5/10: Loss 1.8283 (Value 0.0082, Policy 1.8201)\n","Epoch 6/10: Loss 1.8198 (Value 0.0087, Policy 1.8111)\n","Epoch 7/10: Loss 1.8308 (Value 0.0071, Policy 1.8237)\n","Epoch 8/10: Loss 1.8257 (Value 0.0065, Policy 1.8191)\n","Epoch 9/10: Loss 1.8156 (Value 0.0046, Policy 1.8110)\n","Epoch 10/10: Loss 1.8258 (Value 0.0052, Policy 1.8207)\n","[2025-04-21 23:40:03] Training completed in 19.75s\n","[2025-04-21 23:40:03] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:14<02:13, 14.80s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:40:18] Game 1: duration=14.80s, moves=200, our_VP=3, winner=3 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:32<02:10, 16.34s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:40:35] Game 2: duration=17.43s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:48<01:54, 16.35s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:40:51] Game 3: duration=16.35s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:02<01:31, 15.30s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:41:05] Game 4: duration=13.68s, moves=200, our_VP=4, winner=1 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:18<01:17, 15.49s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:41:21] Game 5: duration=15.82s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:28<00:55, 13.83s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:41:32] Game 6: duration=10.63s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:44<00:43, 14.40s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:41:47] Game 7: duration=15.55s, moves=136, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:55<00:26, 13.26s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:41:58] Game 8: duration=10.83s, moves=200, our_VP=4, winner=1 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:17<00:16, 16.19s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:42:21] Game 9: duration=22.64s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:26<00:00, 14.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:42:29] Game 10: duration=8.50s, moves=200, our_VP=2, winner=1 VP=5\n","[2025-04-21 23:42:29] Evaluated 10 games in 146.24s (0.07 games/s)\n","[2025-04-21 23:42:29] Evaluation results: win_rate=0.60, avg_vp=5.80, avg_length=193.60, total_moves=1936\n","[2025-04-21 23:42:29] Eval resource usage: CPU 63.3%, RAM 7.8%, GPU peak memory 0.00 GB\n","[2025-04-21 23:42:29] Evaluation completed in 146.25s\n","[2025-04-21 23:42:29] Iteration 134 done in 448.56s\n","[2025-04-21 23:42:29] Resource usage: CPU 50.0%, RAM 7.8%, GPU peak memory 0.00 GB\n","[2025-04-21 23:42:29] Network parameter sum after training: 10508.152777\n","[2025-04-21 23:42:29] \n","=== Iteration 135/200 ===\n","[2025-04-21 23:42:29] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10508.153753\n","Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  21%|██▏       | 3/14 [02:10<05:42, 31.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:46<00:00, 20.49s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:47:16] Self-play completed in 286.93s, generated 3486 examples (12.1 games/s)\n","[2025-04-21 23:47:16] Training network...\n","Epoch 1/10: Loss 1.8235 (Value 0.0175, Policy 1.8060)\n","Epoch 2/10: Loss 1.8430 (Value 0.0121, Policy 1.8309)\n","Epoch 3/10: Loss 1.8492 (Value 0.0121, Policy 1.8371)\n","Epoch 4/10: Loss 1.8161 (Value 0.0086, Policy 1.8075)\n","Epoch 5/10: Loss 1.8391 (Value 0.0070, Policy 1.8321)\n","Epoch 6/10: Loss 1.8104 (Value 0.0068, Policy 1.8036)\n","Epoch 7/10: Loss 1.8059 (Value 0.0064, Policy 1.7995)\n","Epoch 8/10: Loss 1.8279 (Value 0.0066, Policy 1.8213)\n","Epoch 9/10: Loss 1.8108 (Value 0.0051, Policy 1.8057)\n","Epoch 10/10: Loss 1.8230 (Value 0.0050, Policy 1.8180)\n","[2025-04-21 23:47:35] Training completed in 19.03s\n","[2025-04-21 23:47:35] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-21 23:47:35] Checkpoint saved: models/model_iter_135.pt\n","[2025-04-21 23:47:39] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 403.3 MB)\n","[2025-04-21 23:47:39] Iteration 135 done in 309.61s\n","[2025-04-21 23:47:39] Resource usage: CPU 86.0%, RAM 7.8%, GPU peak memory 0.00 GB\n","[2025-04-21 23:47:39] Network parameter sum after training: 10615.461460\n","[2025-04-21 23:47:39] \n","=== Iteration 136/200 ===\n","[2025-04-21 23:47:39] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10615.461841\n","Win reward: base=1.00, time_bonus=0.27 (moves: 93/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:16<16:33, 76.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▍        | 2/14 [01:22<07:03, 35.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  21%|██▏       | 3/14 [02:20<08:21, 45.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  29%|██▊       | 4/14 [02:29<05:08, 30.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  71%|███████▏  | 10/14 [04:19<01:38, 24.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▌ | 12/14 [04:26<00:27, 13.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.04 (moves: 185/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:32<00:00, 19.47s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:52:11] Self-play completed in 272.61s, generated 3269 examples (12.0 games/s)\n","[2025-04-21 23:52:11] Training network...\n","Epoch 1/10: Loss 1.8646 (Value 0.0122, Policy 1.8524)\n","Epoch 2/10: Loss 1.8554 (Value 0.0119, Policy 1.8436)\n","Epoch 3/10: Loss 1.8210 (Value 0.0109, Policy 1.8101)\n","Epoch 4/10: Loss 1.8306 (Value 0.0087, Policy 1.8219)\n","Epoch 5/10: Loss 1.8334 (Value 0.0092, Policy 1.8241)\n","Epoch 6/10: Loss 1.8349 (Value 0.0076, Policy 1.8273)\n","Epoch 7/10: Loss 1.8154 (Value 0.0062, Policy 1.8092)\n","Epoch 8/10: Loss 1.8063 (Value 0.0057, Policy 1.8006)\n","Epoch 9/10: Loss 1.8201 (Value 0.0052, Policy 1.8148)\n","Epoch 10/10: Loss 1.8080 (Value 0.0055, Policy 1.8024)\n","[2025-04-21 23:52:31] Training completed in 19.28s\n","[2025-04-21 23:52:31] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:15<02:15, 15.08s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:52:46] Game 1: duration=15.08s, moves=187, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:25<01:37, 12.20s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:52:56] Game 2: duration=10.19s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:43<01:43, 14.77s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:53:14] Game 3: duration=17.82s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:57<01:28, 14.74s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:53:28] Game 4: duration=14.69s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:14<01:17, 15.43s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:53:45] Game 5: duration=16.65s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:27<00:57, 14.47s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:53:58] Game 6: duration=12.61s, moves=128, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:42<00:44, 14.85s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:54:13] Game 7: duration=15.62s, moves=200, our_VP=2, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:59<00:30, 15.48s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:54:30] Game 8: duration=16.84s, moves=198, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:15<00:15, 15.57s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:54:46] Game 9: duration=15.76s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:27<00:00, 14.76s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:54:58] Game 10: duration=12.38s, moves=200, our_VP=6, winner=0 VP=6\n","[2025-04-21 23:54:58] Evaluated 10 games in 147.65s (0.07 games/s)\n","[2025-04-21 23:54:58] Evaluation results: win_rate=0.90, avg_vp=7.20, avg_length=191.30, total_moves=1913\n","[2025-04-21 23:54:58] Eval resource usage: CPU 63.2%, RAM 7.8%, GPU peak memory 0.00 GB\n","[2025-04-21 23:54:58] Evaluation completed in 147.65s\n","[2025-04-21 23:54:58] New best model at iteration 136 (win_rate=0.90)\n","[2025-04-21 23:54:58] Checkpoint saved: models/model_iter_136.pt\n","[2025-04-21 23:54:58] Best model saved: models/best_model.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:55:02] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 403.2 MB)\n","[2025-04-21 23:55:02] Iteration 136 done in 443.09s\n","[2025-04-21 23:55:02] Resource usage: CPU 24.6%, RAM 7.8%, GPU peak memory 0.00 GB\n","[2025-04-21 23:55:02] Network parameter sum after training: 10638.088706\n","[2025-04-21 23:55:02] \n","=== Iteration 137/200 ===\n","[2025-04-21 23:55:02] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10638.088523\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▌      | 5/14 [02:34<02:19, 15.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:42<01:03,  9.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.18 (moves: 129/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  57%|█████▋    | 8/14 [04:15<03:33, 35.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.18 (moves: 129/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▍   | 9/14 [04:22<02:13, 26.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.12 (moves: 153/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:49<00:00, 20.66s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-21 23:59:51] Self-play completed in 289.22s, generated 3435 examples (11.9 games/s)\n","[2025-04-21 23:59:51] Training network...\n","Epoch 1/10: Loss 1.8509 (Value 0.0123, Policy 1.8386)\n","Epoch 2/10: Loss 1.8698 (Value 0.0114, Policy 1.8585)\n","Epoch 3/10: Loss 1.8567 (Value 0.0104, Policy 1.8463)\n","Epoch 4/10: Loss 1.8322 (Value 0.0090, Policy 1.8233)\n","Epoch 5/10: Loss 1.8205 (Value 0.0091, Policy 1.8113)\n","Epoch 6/10: Loss 1.8475 (Value 0.0084, Policy 1.8391)\n","Epoch 7/10: Loss 1.8321 (Value 0.0070, Policy 1.8252)\n","Epoch 8/10: Loss 1.8386 (Value 0.0066, Policy 1.8320)\n","Epoch 9/10: Loss 1.8194 (Value 0.0067, Policy 1.8126)\n","Epoch 10/10: Loss 1.8141 (Value 0.0066, Policy 1.8075)\n","[2025-04-22 00:00:10] Training completed in 18.86s\n","[2025-04-22 00:00:10] Iteration 137 done in 308.07s\n","[2025-04-22 00:00:10] Resource usage: CPU 87.2%, RAM 7.8%, GPU peak memory 0.00 GB\n","[2025-04-22 00:00:10] Network parameter sum after training: 10697.085094\n","[2025-04-22 00:00:10] \n","=== Iteration 138/200 ===\n","[2025-04-22 00:00:10] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10697.085064\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:49<01:17, 11.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.17 (moves: 133/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  57%|█████▋    | 8/14 [03:19<01:43, 17.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.27 (moves: 93/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▍   | 9/14 [03:47<01:42, 20.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.25 (moves: 101/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  71%|███████▏  | 10/14 [03:54<01:05, 16.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  79%|███████▊  | 11/14 [03:54<00:34, 11.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  86%|████████▌ | 12/14 [04:03<00:21, 10.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.17 (moves: 133/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:33<00:00, 19.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:04:44] Self-play completed in 273.63s, generated 3062 examples (11.2 games/s)\n","[2025-04-22 00:04:44] Training network...\n","Epoch 1/10: Loss 1.8404 (Value 0.0146, Policy 1.8258)\n","Epoch 2/10: Loss 1.8517 (Value 0.0128, Policy 1.8390)\n","Epoch 3/10: Loss 1.8409 (Value 0.0112, Policy 1.8297)\n","Epoch 4/10: Loss 1.8537 (Value 0.0100, Policy 1.8437)\n","Epoch 5/10: Loss 1.8297 (Value 0.0091, Policy 1.8206)\n","Epoch 6/10: Loss 1.8279 (Value 0.0086, Policy 1.8192)\n","Epoch 7/10: Loss 1.8070 (Value 0.0081, Policy 1.7989)\n","Epoch 8/10: Loss 1.8333 (Value 0.0084, Policy 1.8248)\n","Epoch 9/10: Loss 1.8185 (Value 0.0069, Policy 1.8117)\n","Epoch 10/10: Loss 1.8446 (Value 0.0066, Policy 1.8380)\n","[2025-04-22 00:05:02] Training completed in 18.85s\n","[2025-04-22 00:05:02] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:12<01:49, 12.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:05:15] Game 1: duration=12.14s, moves=200, our_VP=4, winner=2 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:28<01:59, 14.88s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:05:31] Game 2: duration=16.80s, moves=163, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:45<01:49, 15.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:05:48] Game 3: duration=16.46s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:58<01:27, 14.58s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:06:01] Game 4: duration=13.02s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:11<01:10, 14.02s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:06:14] Game 5: duration=13.03s, moves=185, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:25<00:56, 14.03s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:06:28] Game 6: duration=14.05s, moves=200, our_VP=2, winner=2 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:41<00:44, 14.75s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:06:44] Game 7: duration=16.22s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:53<00:27, 13.90s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:06:56] Game 8: duration=12.07s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:06<00:13, 13.65s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:07:09] Game 9: duration=13.12s, moves=200, our_VP=2, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:18<00:00, 13.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:07:21] Game 10: duration=12.02s, moves=200, our_VP=4, winner=0 VP=4\n","[2025-04-22 00:07:21] Evaluated 10 games in 138.94s (0.07 games/s)\n","[2025-04-22 00:07:21] Evaluation results: win_rate=0.70, avg_vp=5.60, avg_length=194.80, total_moves=1948\n","[2025-04-22 00:07:21] Eval resource usage: CPU 61.0%, RAM 7.9%, GPU peak memory 0.00 GB\n","[2025-04-22 00:07:21] Evaluation completed in 138.95s\n","[2025-04-22 00:07:21] Iteration 138 done in 431.43s\n","[2025-04-22 00:07:21] Resource usage: CPU 66.7%, RAM 7.9%, GPU peak memory 0.00 GB\n","[2025-04-22 00:07:21] Network parameter sum after training: 10856.806605\n","[2025-04-22 00:07:21] \n","=== Iteration 139/200 ===\n","[2025-04-22 00:07:21] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10856.806406\n","Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:20<17:20, 80.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [04:29<00:52, 17.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.02 (moves: 193/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:36<00:09,  9.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:55<00:00, 21.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:12:17] Self-play completed in 295.74s, generated 3460 examples (11.7 games/s)\n","[2025-04-22 00:12:17] Training network...\n","Epoch 1/10: Loss 1.8578 (Value 0.0179, Policy 1.8399)\n","Epoch 2/10: Loss 1.8469 (Value 0.0141, Policy 1.8328)\n","Epoch 3/10: Loss 1.8244 (Value 0.0122, Policy 1.8121)\n","Epoch 4/10: Loss 1.8491 (Value 0.0109, Policy 1.8382)\n","Epoch 5/10: Loss 1.8534 (Value 0.0099, Policy 1.8435)\n","Epoch 6/10: Loss 1.8380 (Value 0.0092, Policy 1.8288)\n","Epoch 7/10: Loss 1.8063 (Value 0.0092, Policy 1.7971)\n","Epoch 8/10: Loss 1.8063 (Value 0.0069, Policy 1.7994)\n","Epoch 9/10: Loss 1.8252 (Value 0.0073, Policy 1.8179)\n","Epoch 10/10: Loss 1.8264 (Value 0.0068, Policy 1.8197)\n","[2025-04-22 00:12:36] Training completed in 18.87s\n","[2025-04-22 00:12:36] Iteration 139 done in 314.61s\n","[2025-04-22 00:12:36] Resource usage: CPU 82.8%, RAM 7.9%, GPU peak memory 0.00 GB\n","[2025-04-22 00:12:36] Network parameter sum after training: 10838.698511\n","[2025-04-22 00:12:36] \n","=== Iteration 140/200 ===\n","[2025-04-22 00:12:36] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10838.697764\n","Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▌      | 5/14 [02:16<02:08, 14.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:32<01:16, 10.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.29 (moves: 85/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▌ | 12/14 [04:26<00:27, 13.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:47<00:00, 20.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:17:24] Self-play completed in 287.79s, generated 3297 examples (11.5 games/s)\n","[2025-04-22 00:17:24] Training network...\n","Epoch 1/10: Loss 1.8760 (Value 0.0151, Policy 1.8608)\n","Epoch 2/10: Loss 1.8521 (Value 0.0133, Policy 1.8388)\n","Epoch 3/10: Loss 1.8400 (Value 0.0120, Policy 1.8280)\n","Epoch 4/10: Loss 1.8650 (Value 0.0119, Policy 1.8530)\n","Epoch 5/10: Loss 1.8291 (Value 0.0097, Policy 1.8194)\n","Epoch 6/10: Loss 1.8594 (Value 0.0117, Policy 1.8477)\n","Epoch 7/10: Loss 1.8179 (Value 0.0099, Policy 1.8080)\n","Epoch 8/10: Loss 1.8323 (Value 0.0088, Policy 1.8235)\n","Epoch 9/10: Loss 1.8464 (Value 0.0089, Policy 1.8375)\n","Epoch 10/10: Loss 1.8226 (Value 0.0076, Policy 1.8150)\n","[2025-04-22 00:17:44] Training completed in 20.22s\n","[2025-04-22 00:17:44] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:12<01:49, 12.12s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:17:56] Game 1: duration=12.12s, moves=200, our_VP=4, winner=1 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:26<01:47, 13.45s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:18:11] Game 2: duration=14.38s, moves=188, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:42<01:41, 14.56s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:18:27] Game 3: duration=15.88s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:59<01:33, 15.59s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:18:44] Game 4: duration=17.16s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:13<01:14, 14.82s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:18:57] Game 5: duration=13.46s, moves=200, our_VP=4, winner=2 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:33<01:06, 16.63s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:19:17] Game 6: duration=20.13s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:54<00:54, 18.05s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:19:38] Game 7: duration=20.98s, moves=191, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [02:07<00:33, 16.51s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:19:51] Game 8: duration=13.23s, moves=146, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:18<00:14, 14.96s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:20:03] Game 9: duration=11.55s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:30<00:00, 15.00s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:20:14] Game 10: duration=11.14s, moves=200, our_VP=4, winner=0 VP=4\n","[2025-04-22 00:20:14] Evaluated 10 games in 150.04s (0.07 games/s)\n","[2025-04-22 00:20:14] Evaluation results: win_rate=0.80, avg_vp=7.00, avg_length=192.50, total_moves=1925\n","[2025-04-22 00:20:14] Eval resource usage: CPU 61.8%, RAM 7.9%, GPU peak memory 0.00 GB\n","[2025-04-22 00:20:14] Evaluation completed in 150.05s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:20:14] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 00:20:14] New best model at iteration 140 (win_rate=0.80)\n","[2025-04-22 00:20:15] Checkpoint saved: models/model_iter_140.pt\n","[2025-04-22 00:20:15] Best model saved: models/best_model.pt\n","[2025-04-22 00:20:18] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 403.3 MB)\n","[2025-04-22 00:20:18] Iteration 140 done in 461.97s\n","[2025-04-22 00:20:18] Resource usage: CPU 24.8%, RAM 8.0%, GPU peak memory 0.00 GB\n","[2025-04-22 00:20:18] Network parameter sum after training: 10918.261195\n","[2025-04-22 00:20:18] \n","=== Iteration 141/200 ===\n","[2025-04-22 00:20:18] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10918.261455\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:51<24:10, 111.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:20<03:40, 22.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [04:26<02:24, 28.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [05:04<00:00, 21.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:25:23] Self-play completed in 304.63s, generated 3408 examples (11.2 games/s)\n","[2025-04-22 00:25:23] Training network...\n","Epoch 1/10: Loss 1.8760 (Value 0.0195, Policy 1.8566)\n","Epoch 2/10: Loss 1.8610 (Value 0.0157, Policy 1.8453)\n","Epoch 3/10: Loss 1.8573 (Value 0.0140, Policy 1.8432)\n","Epoch 4/10: Loss 1.8215 (Value 0.0118, Policy 1.8097)\n","Epoch 5/10: Loss 1.8342 (Value 0.0119, Policy 1.8223)\n","Epoch 6/10: Loss 1.8526 (Value 0.0102, Policy 1.8424)\n","Epoch 7/10: Loss 1.8391 (Value 0.0097, Policy 1.8294)\n","Epoch 8/10: Loss 1.8416 (Value 0.0081, Policy 1.8335)\n","Epoch 9/10: Loss 1.8351 (Value 0.0081, Policy 1.8270)\n","Epoch 10/10: Loss 1.8305 (Value 0.0085, Policy 1.8220)\n","[2025-04-22 00:25:43] Training completed in 19.84s\n","[2025-04-22 00:25:43] Iteration 141 done in 324.48s\n","[2025-04-22 00:25:43] Resource usage: CPU 83.6%, RAM 7.9%, GPU peak memory 0.00 GB\n","[2025-04-22 00:25:43] Network parameter sum after training: 10934.026192\n","[2025-04-22 00:25:43] \n","=== Iteration 142/200 ===\n","[2025-04-22 00:25:43] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 10934.025917\n","Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:52<24:16, 112.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▌      | 5/14 [02:24<02:26, 16.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [03:59<03:23, 33.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:58<00:00, 21.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:30:42] Self-play completed in 298.92s, generated 3502 examples (11.7 games/s)\n","[2025-04-22 00:30:42] Training network...\n","Epoch 1/10: Loss 1.8798 (Value 0.0173, Policy 1.8624)\n","Epoch 2/10: Loss 1.8703 (Value 0.0161, Policy 1.8542)\n","Epoch 3/10: Loss 1.8629 (Value 0.0126, Policy 1.8503)\n","Epoch 4/10: Loss 1.8530 (Value 0.0102, Policy 1.8428)\n","Epoch 5/10: Loss 1.8410 (Value 0.0090, Policy 1.8321)\n","Epoch 6/10: Loss 1.8440 (Value 0.0076, Policy 1.8364)\n","Epoch 7/10: Loss 1.8397 (Value 0.0093, Policy 1.8304)\n","Epoch 8/10: Loss 1.8532 (Value 0.0067, Policy 1.8465)\n","Epoch 9/10: Loss 1.8351 (Value 0.0045, Policy 1.8306)\n","Epoch 10/10: Loss 1.8353 (Value 0.0052, Policy 1.8302)\n","[2025-04-22 00:31:01] Training completed in 19.29s\n","[2025-04-22 00:31:01] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:16<02:31, 16.87s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:31:18] Game 1: duration=16.87s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:33<02:13, 16.66s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:31:34] Game 2: duration=16.51s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:51<01:59, 17.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:31:52] Game 3: duration=17.70s, moves=200, our_VP=5, winner=1 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:32:07] Game 4: duration=15.40s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:20<01:18, 15.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:32:21] Game 5: duration=14.10s, moves=200, our_VP=4, winner=3 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:33<00:58, 14.54s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:32:34] Game 6: duration=12.47s, moves=200, our_VP=2, winner=2 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:49<00:45, 15.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:32:50] Game 7: duration=16.37s, moves=187, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [02:08<00:32, 16.46s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:33:10] Game 8: duration=19.30s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:22<00:15, 15.62s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:33:23] Game 9: duration=13.75s, moves=200, our_VP=2, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:43<00:00, 16.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:33:45] Game 10: duration=21.51s, moves=200, our_VP=9, winner=0 VP=9\n","[2025-04-22 00:33:45] Evaluated 10 games in 164.00s (0.06 games/s)\n","[2025-04-22 00:33:45] Evaluation results: win_rate=0.60, avg_vp=6.20, avg_length=198.70, total_moves=1987\n","[2025-04-22 00:33:45] Eval resource usage: CPU 62.3%, RAM 8.0%, GPU peak memory 0.00 GB\n","[2025-04-22 00:33:45] Evaluation completed in 164.00s\n","[2025-04-22 00:33:45] Iteration 142 done in 482.22s\n","[2025-04-22 00:33:45] Resource usage: CPU 0.0%, RAM 8.0%, GPU peak memory 0.00 GB\n","[2025-04-22 00:33:45] Network parameter sum after training: 11025.234900\n","[2025-04-22 00:33:45] \n","=== Iteration 143/200 ===\n","[2025-04-22 00:33:45] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11025.235006\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:58<25:36, 118.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  21%|██▏       | 3/14 [02:01<05:06, 27.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  29%|██▊       | 4/14 [02:03<02:53, 17.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [04:20<02:31, 30.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:32<00:09,  9.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:33<00:00, 19.57s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:38:19] Self-play completed in 273.96s, generated 3365 examples (12.3 games/s)\n","[2025-04-22 00:38:19] Training network...\n","Epoch 1/10: Loss 1.8505 (Value 0.0148, Policy 1.8358)\n","Epoch 2/10: Loss 1.8645 (Value 0.0123, Policy 1.8522)\n","Epoch 3/10: Loss 1.8735 (Value 0.0121, Policy 1.8613)\n","Epoch 4/10: Loss 1.8365 (Value 0.0079, Policy 1.8286)\n","Epoch 5/10: Loss 1.8472 (Value 0.0076, Policy 1.8396)\n","Epoch 6/10: Loss 1.8460 (Value 0.0053, Policy 1.8407)\n","Epoch 7/10: Loss 1.8423 (Value 0.0059, Policy 1.8364)\n","Epoch 8/10: Loss 1.8400 (Value 0.0057, Policy 1.8342)\n","Epoch 9/10: Loss 1.8588 (Value 0.0045, Policy 1.8542)\n","Epoch 10/10: Loss 1.8197 (Value 0.0051, Policy 1.8145)\n","[2025-04-22 00:38:38] Training completed in 18.98s\n","[2025-04-22 00:38:38] Iteration 143 done in 292.94s\n","[2025-04-22 00:38:38] Resource usage: CPU 87.3%, RAM 8.0%, GPU peak memory 0.00 GB\n","[2025-04-22 00:38:38] Network parameter sum after training: 11143.839325\n","[2025-04-22 00:38:38] \n","=== Iteration 144/200 ===\n","[2025-04-22 00:38:38] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11143.838456\n","Win reward: base=1.00, time_bonus=0.13 (moves: 149/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▍        | 2/14 [02:00<10:43, 53.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  43%|████▎     | 6/14 [02:34<01:34, 11.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 7/14 [02:54<01:41, 14.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.04 (moves: 185/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [03:58<01:54, 22.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:34<00:10, 10.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:51<00:00, 20.81s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:43:29] Self-play completed in 291.36s, generated 3325 examples (11.4 games/s)\n","[2025-04-22 00:43:29] Training network...\n","Epoch 1/10: Loss 1.8501 (Value 0.0137, Policy 1.8364)\n","Epoch 2/10: Loss 1.8538 (Value 0.0115, Policy 1.8422)\n","Epoch 3/10: Loss 1.8594 (Value 0.0092, Policy 1.8502)\n","Epoch 4/10: Loss 1.8365 (Value 0.0071, Policy 1.8295)\n","Epoch 5/10: Loss 1.8319 (Value 0.0066, Policy 1.8253)\n","Epoch 6/10: Loss 1.8394 (Value 0.0072, Policy 1.8322)\n","Epoch 7/10: Loss 1.8275 (Value 0.0062, Policy 1.8213)\n","Epoch 8/10: Loss 1.8255 (Value 0.0056, Policy 1.8199)\n","Epoch 9/10: Loss 1.8261 (Value 0.0053, Policy 1.8208)\n","Epoch 10/10: Loss 1.8308 (Value 0.0053, Policy 1.8255)\n","[2025-04-22 00:43:49] Training completed in 19.68s\n","[2025-04-22 00:43:49] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:07<01:06,  7.35s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:43:56] Game 1: duration=7.35s, moves=200, our_VP=2, winner=3 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:19<01:22, 10.36s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:44:09] Game 2: duration=12.46s, moves=129, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:37<01:36, 13.85s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:44:27] Game 3: duration=18.00s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:52<01:25, 14.20s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:44:41] Game 4: duration=14.74s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:10<01:17, 15.49s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:44:59] Game 5: duration=17.77s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:26<01:03, 15.80s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:45:16] Game 6: duration=16.40s, moves=178, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:46<00:51, 17.19s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:45:36] Game 7: duration=20.04s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [02:00<00:31, 15.93s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:45:49] Game 8: duration=13.25s, moves=125, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:13<00:15, 15.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:46:02] Game 9: duration=13.38s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:25<00:00, 14.56s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:46:14] Game 10: duration=12.17s, moves=200, our_VP=5, winner=0 VP=5\n","[2025-04-22 00:46:14] Evaluated 10 games in 145.57s (0.07 games/s)\n","[2025-04-22 00:46:14] Evaluation results: win_rate=0.90, avg_vp=6.90, avg_length=183.20, total_moves=1832\n","[2025-04-22 00:46:14] Eval resource usage: CPU 62.6%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 00:46:14] Evaluation completed in 145.58s\n","[2025-04-22 00:46:14] New best model at iteration 144 (win_rate=0.90)\n","[2025-04-22 00:46:15] Checkpoint saved: models/model_iter_144.pt\n","[2025-04-22 00:46:15] Best model saved: models/best_model.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:46:18] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 403.6 MB)\n","[2025-04-22 00:46:18] Iteration 144 done in 460.03s\n","[2025-04-22 00:46:18] Resource usage: CPU 23.0%, RAM 8.0%, GPU peak memory 0.00 GB\n","[2025-04-22 00:46:18] Network parameter sum after training: 11088.696408\n","[2025-04-22 00:46:18] \n","=== Iteration 145/200 ===\n","[2025-04-22 00:46:18] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11088.697491\n","Win reward: base=1.00, time_bonus=0.18 (moves: 129/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:28<19:05, 88.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▍        | 2/14 [01:49<09:45, 48.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  21%|██▏       | 3/14 [02:30<08:17, 45.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  29%|██▊       | 4/14 [02:31<04:37, 27.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.03 (moves: 189/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [04:04<02:02, 24.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  71%|███████▏  | 10/14 [04:41<01:53, 28.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.03 (moves: 189/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [05:03<00:00, 21.70s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:51:22] Self-play completed in 303.85s, generated 3536 examples (11.6 games/s)\n","[2025-04-22 00:51:22] Training network...\n","Epoch 1/10: Loss 1.8673 (Value 0.0151, Policy 1.8522)\n","Epoch 2/10: Loss 1.8624 (Value 0.0132, Policy 1.8493)\n","Epoch 3/10: Loss 1.8561 (Value 0.0118, Policy 1.8443)\n","Epoch 4/10: Loss 1.8581 (Value 0.0105, Policy 1.8476)\n","Epoch 5/10: Loss 1.8389 (Value 0.0090, Policy 1.8299)\n","Epoch 6/10: Loss 1.8263 (Value 0.0080, Policy 1.8183)\n","Epoch 7/10: Loss 1.8362 (Value 0.0071, Policy 1.8291)\n","Epoch 8/10: Loss 1.8423 (Value 0.0058, Policy 1.8365)\n","Epoch 9/10: Loss 1.8116 (Value 0.0064, Policy 1.8052)\n","Epoch 10/10: Loss 1.8523 (Value 0.0055, Policy 1.8467)\n","[2025-04-22 00:51:43] Training completed in 21.07s\n","[2025-04-22 00:51:43] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 00:51:43] Checkpoint saved: models/model_iter_145.pt\n","[2025-04-22 00:51:47] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 403.9 MB)\n","[2025-04-22 00:51:47] Iteration 145 done in 329.13s\n","[2025-04-22 00:51:47] Resource usage: CPU 83.4%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 00:51:47] Network parameter sum after training: 11160.733256\n","[2025-04-22 00:51:47] \n","=== Iteration 146/200 ===\n","[2025-04-22 00:51:47] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11160.732188\n","Win reward: base=1.00, time_bonus=0.33 (moves: 69/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:16<16:40, 76.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.13 (moves: 149/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▍        | 2/14 [01:53<10:37, 53.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:21<04:35, 27.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:55<02:08, 18.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.10 (moves: 161/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [04:15<00:50, 16.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:43<00:00, 20.24s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:56:30] Self-play completed in 283.33s, generated 3249 examples (11.5 games/s)\n","[2025-04-22 00:56:30] Training network...\n","Epoch 1/10: Loss 1.8757 (Value 0.0132, Policy 1.8624)\n","Epoch 2/10: Loss 1.8627 (Value 0.0127, Policy 1.8500)\n","Epoch 3/10: Loss 1.8444 (Value 0.0126, Policy 1.8319)\n","Epoch 4/10: Loss 1.8487 (Value 0.0097, Policy 1.8391)\n","Epoch 5/10: Loss 1.8431 (Value 0.0084, Policy 1.8346)\n","Epoch 6/10: Loss 1.8544 (Value 0.0092, Policy 1.8452)\n","Epoch 7/10: Loss 1.8355 (Value 0.0067, Policy 1.8288)\n","Epoch 8/10: Loss 1.8326 (Value 0.0071, Policy 1.8255)\n","Epoch 9/10: Loss 1.8307 (Value 0.0079, Policy 1.8229)\n","Epoch 10/10: Loss 1.8368 (Value 0.0084, Policy 1.8284)\n","[2025-04-22 00:56:49] Training completed in 18.81s\n","[2025-04-22 00:56:49] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:12<01:54, 12.76s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:57:02] Game 1: duration=12.76s, moves=200, our_VP=3, winner=3 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:30<02:04, 15.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:57:20] Game 2: duration=17.59s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:44<01:43, 14.75s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:57:33] Game 3: duration=13.74s, moves=143, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:59<01:29, 14.91s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:57:48] Game 4: duration=15.15s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:11<01:10, 14.08s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:58:01] Game 5: duration=12.60s, moves=200, our_VP=4, winner=3 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:27<00:58, 14.67s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:58:17] Game 6: duration=15.81s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:37<00:39, 13.09s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:58:27] Game 7: duration=9.85s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:51<00:26, 13.47s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:58:41] Game 8: duration=14.27s, moves=200, our_VP=2, winner=2 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:02<00:12, 12.64s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:58:52] Game 9: duration=10.83s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:15<00:00, 13.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 00:59:05] Game 10: duration=13.17s, moves=200, our_VP=3, winner=1 VP=8\n","[2025-04-22 00:59:05] Evaluated 10 games in 135.78s (0.07 games/s)\n","[2025-04-22 00:59:05] Evaluation results: win_rate=0.60, avg_vp=5.00, avg_length=194.30, total_moves=1943\n","[2025-04-22 00:59:05] Eval resource usage: CPU 63.3%, RAM 8.0%, GPU peak memory 0.00 GB\n","[2025-04-22 00:59:05] Evaluation completed in 135.79s\n","[2025-04-22 00:59:05] Iteration 146 done in 437.93s\n","[2025-04-22 00:59:05] Resource usage: CPU 0.0%, RAM 8.0%, GPU peak memory 0.00 GB\n","[2025-04-22 00:59:05] Network parameter sum after training: 11291.242774\n","[2025-04-22 00:59:05] \n","=== Iteration 147/200 ===\n","[2025-04-22 00:59:05] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11291.243552\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [02:00<26:08, 120.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  43%|████▎     | 6/14 [02:42<02:32, 19.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 7/14 [02:49<01:49, 15.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▌ | 12/14 [04:30<00:27, 13.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:55<00:00, 21.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:04:00] Self-play completed in 295.17s, generated 3366 examples (11.4 games/s)\n","[2025-04-22 01:04:00] Training network...\n","Epoch 1/10: Loss 1.8726 (Value 0.0184, Policy 1.8542)\n","Epoch 2/10: Loss 1.8693 (Value 0.0140, Policy 1.8552)\n","Epoch 3/10: Loss 1.8240 (Value 0.0117, Policy 1.8123)\n","Epoch 4/10: Loss 1.8509 (Value 0.0114, Policy 1.8395)\n","Epoch 5/10: Loss 1.8526 (Value 0.0092, Policy 1.8434)\n","Epoch 6/10: Loss 1.8466 (Value 0.0086, Policy 1.8380)\n","Epoch 7/10: Loss 1.8350 (Value 0.0104, Policy 1.8246)\n","Epoch 8/10: Loss 1.8373 (Value 0.0069, Policy 1.8304)\n","Epoch 9/10: Loss 1.8317 (Value 0.0061, Policy 1.8256)\n","Epoch 10/10: Loss 1.8346 (Value 0.0053, Policy 1.8293)\n","[2025-04-22 01:04:21] Training completed in 20.58s\n","[2025-04-22 01:04:21] Iteration 147 done in 315.74s\n","[2025-04-22 01:04:21] Resource usage: CPU 81.3%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:04:21] Network parameter sum after training: 11270.081127\n","[2025-04-22 01:04:21] \n","=== Iteration 148/200 ===\n","[2025-04-22 01:04:21] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11270.081173\n","Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:42<22:16, 102.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:19<03:37, 21.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:45<01:29, 12.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.25 (moves: 101/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:44<00:11, 11.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games: 100%|██████████| 14/14 [04:44<00:00, 20.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:09:06] Self-play completed in 284.89s, generated 3419 examples (12.0 games/s)\n","[2025-04-22 01:09:06] Training network...\n","Epoch 1/10: Loss 1.8435 (Value 0.0142, Policy 1.8292)\n","Epoch 2/10: Loss 1.8677 (Value 0.0099, Policy 1.8579)\n","Epoch 3/10: Loss 1.8367 (Value 0.0072, Policy 1.8295)\n","Epoch 4/10: Loss 1.8302 (Value 0.0075, Policy 1.8226)\n","Epoch 5/10: Loss 1.8311 (Value 0.0066, Policy 1.8245)\n","Epoch 6/10: Loss 1.8382 (Value 0.0053, Policy 1.8329)\n","Epoch 7/10: Loss 1.8006 (Value 0.0053, Policy 1.7953)\n","Epoch 8/10: Loss 1.8237 (Value 0.0055, Policy 1.8182)\n","Epoch 9/10: Loss 1.8173 (Value 0.0051, Policy 1.8122)\n","Epoch 10/10: Loss 1.8323 (Value 0.0055, Policy 1.8268)\n","[2025-04-22 01:09:25] Training completed in 19.20s\n","[2025-04-22 01:09:25] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:18<02:43, 18.20s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:09:43] Game 1: duration=18.20s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:36<02:23, 17.98s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:10:01] Game 2: duration=17.83s, moves=200, our_VP=4, winner=1 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:54<02:07, 18.27s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:10:20] Game 3: duration=18.60s, moves=200, our_VP=5, winner=3 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:08<01:40, 16.69s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:10:34] Game 4: duration=14.28s, moves=200, our_VP=2, winner=1 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:23<01:19, 15.98s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:10:49] Game 5: duration=14.73s, moves=200, our_VP=5, winner=2 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:33<00:55, 13.78s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:10:58] Game 6: duration=9.51s, moves=200, our_VP=2, winner=2 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:43<00:37, 12.58s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:11:08] Game 7: duration=10.11s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:50<00:21, 11.00s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:11:16] Game 8: duration=7.60s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:03<00:11, 11.45s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:11:28] Game 9: duration=12.45s, moves=200, our_VP=5, winner=1 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:17<00:00, 13.77s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:11:43] Game 10: duration=14.37s, moves=200, our_VP=7, winner=0 VP=7\n","[2025-04-22 01:11:43] Evaluated 10 games in 137.70s (0.07 games/s)\n","[2025-04-22 01:11:43] Evaluation results: win_rate=0.40, avg_vp=4.50, avg_length=200.00, total_moves=2000\n","[2025-04-22 01:11:43] Eval resource usage: CPU 64.7%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:11:43] Evaluation completed in 137.70s\n","[2025-04-22 01:11:43] Iteration 148 done in 441.78s\n","[2025-04-22 01:11:43] Resource usage: CPU 50.0%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:11:43] Network parameter sum after training: 11389.523823\n","[2025-04-22 01:11:43] \n","=== Iteration 149/200 ===\n","[2025-04-22 01:11:43] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11389.523502\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▍        | 2/14 [02:18<11:51, 59.33s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  21%|██▏       | 3/14 [02:27<06:38, 36.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▌      | 5/14 [02:29<02:11, 14.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  43%|████▎     | 6/14 [02:36<01:35, 11.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  57%|█████▋    | 8/14 [03:54<02:32, 25.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▌ | 12/14 [04:35<00:29, 14.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:49<00:00, 20.71s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:16:33] Self-play completed in 289.91s, generated 3426 examples (11.8 games/s)\n","[2025-04-22 01:16:33] Training network...\n","Epoch 1/10: Loss 1.8388 (Value 0.0134, Policy 1.8253)\n","Epoch 2/10: Loss 1.8628 (Value 0.0094, Policy 1.8534)\n","Epoch 3/10: Loss 1.8518 (Value 0.0080, Policy 1.8438)\n","Epoch 4/10: Loss 1.8193 (Value 0.0055, Policy 1.8138)\n","Epoch 5/10: Loss 1.8329 (Value 0.0052, Policy 1.8277)\n","Epoch 6/10: Loss 1.8377 (Value 0.0054, Policy 1.8323)\n","Epoch 7/10: Loss 1.8107 (Value 0.0050, Policy 1.8057)\n","Epoch 8/10: Loss 1.8459 (Value 0.0046, Policy 1.8413)\n","Epoch 9/10: Loss 1.8132 (Value 0.0038, Policy 1.8094)\n","Epoch 10/10: Loss 1.8442 (Value 0.0036, Policy 1.8406)\n","[2025-04-22 01:16:52] Training completed in 19.49s\n","[2025-04-22 01:16:52] Iteration 149 done in 309.40s\n","[2025-04-22 01:16:52] Resource usage: CPU 83.0%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:16:52] Network parameter sum after training: 11420.002018\n","[2025-04-22 01:16:52] \n","=== Iteration 150/200 ===\n","[2025-04-22 01:16:52] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11420.001514\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:55<25:02, 115.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:29<03:45, 22.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  36%|███▌      | 5/14 [02:34<02:25, 16.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [03:17<01:37, 16.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:43<00:00, 20.26s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:21:36] Self-play completed in 283.68s, generated 3336 examples (11.8 games/s)\n","[2025-04-22 01:21:36] Training network...\n","Epoch 1/10: Loss 1.8886 (Value 0.0146, Policy 1.8740)\n","Epoch 2/10: Loss 1.8439 (Value 0.0128, Policy 1.8310)\n","Epoch 3/10: Loss 1.8370 (Value 0.0105, Policy 1.8265)\n","Epoch 4/10: Loss 1.8402 (Value 0.0102, Policy 1.8300)\n","Epoch 5/10: Loss 1.8219 (Value 0.0078, Policy 1.8141)\n","Epoch 6/10: Loss 1.8343 (Value 0.0073, Policy 1.8269)\n","Epoch 7/10: Loss 1.8299 (Value 0.0062, Policy 1.8237)\n","Epoch 8/10: Loss 1.8308 (Value 0.0060, Policy 1.8247)\n","Epoch 9/10: Loss 1.8103 (Value 0.0047, Policy 1.8056)\n","Epoch 10/10: Loss 1.8080 (Value 0.0042, Policy 1.8038)\n","[2025-04-22 01:21:55] Training completed in 18.89s\n","[2025-04-22 01:21:55] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:11<01:40, 11.16s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:22:06] Game 1: duration=11.16s, moves=200, our_VP=2, winner=2 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:24<01:40, 12.56s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:22:19] Game 2: duration=13.53s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:38<01:30, 12.95s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:22:33] Game 3: duration=13.41s, moves=200, our_VP=4, winner=2 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:53<01:22, 13.75s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:22:48] Game 4: duration=14.99s, moves=125, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:08<01:11, 14.24s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:23:03] Game 5: duration=15.10s, moves=200, our_VP=2, winner=1 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:17<00:49, 12.47s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:23:12] Game 6: duration=9.04s, moves=200, our_VP=2, winner=3 VP=3\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:28<00:36, 12.21s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:23:24] Game 7: duration=11.65s, moves=200, our_VP=5, winner=3 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:42<00:25, 12.72s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:23:37] Game 8: duration=13.83s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [01:59<00:13, 13.91s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:23:54] Game 9: duration=16.50s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:11<00:00, 13.12s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:24:06] Game 10: duration=12.01s, moves=200, our_VP=5, winner=0 VP=5\n","[2025-04-22 01:24:06] Evaluated 10 games in 131.24s (0.08 games/s)\n","[2025-04-22 01:24:06] Evaluation results: win_rate=0.50, avg_vp=5.00, avg_length=192.50, total_moves=1925\n","[2025-04-22 01:24:06] Eval resource usage: CPU 65.3%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:24:06] Evaluation completed in 131.24s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:24:06] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 01:24:06] Checkpoint saved: models/model_iter_150.pt\n","[2025-04-22 01:24:09] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 403.5 MB)\n","[2025-04-22 01:24:09] Iteration 150 done in 437.25s\n","[2025-04-22 01:24:09] Resource usage: CPU 13.0%, RAM 8.2%, GPU peak memory 0.00 GB\n","[2025-04-22 01:24:09] Network parameter sum after training: 11506.427796\n","[2025-04-22 01:24:09] \n","=== Iteration 151/200 ===\n","[2025-04-22 01:24:09] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11506.427704\n","Win reward: base=1.00, time_bonus=0.27 (moves: 93/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:22<03:59, 23.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.02 (moves: 193/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:49<01:34, 13.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [03:41<01:32, 18.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  71%|███████▏  | 10/14 [03:53<01:06, 16.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:19<00:00, 18.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:28:29] Self-play completed in 259.79s, generated 3085 examples (11.9 games/s)\n","[2025-04-22 01:28:29] Training network...\n","Epoch 1/10: Loss 1.8679 (Value 0.0149, Policy 1.8530)\n","Epoch 2/10: Loss 1.8613 (Value 0.0108, Policy 1.8505)\n","Epoch 3/10: Loss 1.8501 (Value 0.0077, Policy 1.8424)\n","Epoch 4/10: Loss 1.8438 (Value 0.0066, Policy 1.8372)\n","Epoch 5/10: Loss 1.8088 (Value 0.0070, Policy 1.8018)\n","Epoch 6/10: Loss 1.8167 (Value 0.0069, Policy 1.8098)\n","Epoch 7/10: Loss 1.8302 (Value 0.0055, Policy 1.8246)\n","Epoch 8/10: Loss 1.8108 (Value 0.0048, Policy 1.8060)\n","Epoch 9/10: Loss 1.8392 (Value 0.0042, Policy 1.8350)\n","Epoch 10/10: Loss 1.8216 (Value 0.0031, Policy 1.8185)\n","[2025-04-22 01:28:49] Training completed in 19.40s\n","[2025-04-22 01:28:49] Iteration 151 done in 279.19s\n","[2025-04-22 01:28:49] Resource usage: CPU 84.9%, RAM 8.2%, GPU peak memory 0.00 GB\n","[2025-04-22 01:28:49] Network parameter sum after training: 11525.081453\n","[2025-04-22 01:28:49] \n","=== Iteration 152/200 ===\n","[2025-04-22 01:28:49] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11525.081164\n","Win reward: base=1.00, time_bonus=0.27 (moves: 93/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:06<14:28, 66.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▍        | 2/14 [02:13<13:23, 66.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:47<00:00, 20.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:33:36] Self-play completed in 287.83s, generated 3325 examples (11.6 games/s)\n","[2025-04-22 01:33:36] Training network...\n","Epoch 1/10: Loss 1.8564 (Value 0.0172, Policy 1.8392)\n","Epoch 2/10: Loss 1.8558 (Value 0.0123, Policy 1.8435)\n","Epoch 3/10: Loss 1.8236 (Value 0.0108, Policy 1.8128)\n","Epoch 4/10: Loss 1.8525 (Value 0.0109, Policy 1.8415)\n","Epoch 5/10: Loss 1.8590 (Value 0.0073, Policy 1.8516)\n","Epoch 6/10: Loss 1.8308 (Value 0.0066, Policy 1.8242)\n","Epoch 7/10: Loss 1.8302 (Value 0.0084, Policy 1.8218)\n","Epoch 8/10: Loss 1.8477 (Value 0.0064, Policy 1.8413)\n","Epoch 9/10: Loss 1.8171 (Value 0.0059, Policy 1.8112)\n","Epoch 10/10: Loss 1.8150 (Value 0.0048, Policy 1.8101)\n","[2025-04-22 01:33:56] Training completed in 19.14s\n","[2025-04-22 01:33:56] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:09<01:21,  9.07s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:34:05] Game 1: duration=9.07s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:23<01:37, 12.22s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:34:19] Game 2: duration=14.42s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:38<01:34, 13.44s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:34:34] Game 3: duration=14.89s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:53<01:24, 14.13s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:34:49] Game 4: duration=15.20s, moves=144, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:08<01:11, 14.25s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:35:04] Game 5: duration=14.47s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:19<00:53, 13.41s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:35:15] Game 6: duration=11.76s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:33<00:40, 13.56s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:35:29] Game 7: duration=13.86s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:49<00:28, 14.37s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:35:45] Game 8: duration=16.12s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:03<00:14, 14.18s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:35:59] Game 9: duration=13.74s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:17<00:00, 13.77s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:36:13] Game 10: duration=14.12s, moves=200, our_VP=2, winner=2 VP=5\n","[2025-04-22 01:36:13] Evaluated 10 games in 137.66s (0.07 games/s)\n","[2025-04-22 01:36:13] Evaluation results: win_rate=0.90, avg_vp=5.80, avg_length=194.40, total_moves=1944\n","[2025-04-22 01:36:13] Eval resource usage: CPU 62.8%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:36:13] Evaluation completed in 137.67s\n","[2025-04-22 01:36:13] New best model at iteration 152 (win_rate=0.90)\n","[2025-04-22 01:36:13] Checkpoint saved: models/model_iter_152.pt\n","[2025-04-22 01:36:13] Best model saved: models/best_model.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:36:16] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 403.4 MB)\n","[2025-04-22 01:36:16] Iteration 152 done in 447.80s\n","[2025-04-22 01:36:16] Resource usage: CPU 12.9%, RAM 8.2%, GPU peak memory 0.00 GB\n","[2025-04-22 01:36:16] Network parameter sum after training: 11588.423092\n","[2025-04-22 01:36:16] \n","=== Iteration 153/200 ===\n","[2025-04-22 01:36:16] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11588.421993\n","Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▍        | 2/14 [02:22<12:07, 60.60s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▌      | 5/14 [02:37<02:28, 16.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  43%|████▎     | 6/14 [02:38<01:30, 11.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  57%|█████▋    | 8/14 [03:52<02:47, 27.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▍   | 9/14 [04:01<01:50, 22.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  71%|███████▏  | 10/14 [04:13<01:15, 18.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  93%|█████████▎| 13/14 [04:37<00:11, 11.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:41<00:00, 20.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:40:58] Self-play completed in 281.42s, generated 3266 examples (11.6 games/s)\n","[2025-04-22 01:40:58] Training network...\n","Epoch 1/10: Loss 1.8678 (Value 0.0158, Policy 1.8519)\n","Epoch 2/10: Loss 1.8536 (Value 0.0124, Policy 1.8411)\n","Epoch 3/10: Loss 1.8406 (Value 0.0113, Policy 1.8293)\n","Epoch 4/10: Loss 1.8162 (Value 0.0092, Policy 1.8070)\n","Epoch 5/10: Loss 1.8363 (Value 0.0088, Policy 1.8275)\n","Epoch 6/10: Loss 1.8296 (Value 0.0092, Policy 1.8204)\n","Epoch 7/10: Loss 1.8144 (Value 0.0050, Policy 1.8094)\n","Epoch 8/10: Loss 1.8175 (Value 0.0070, Policy 1.8105)\n","Epoch 9/10: Loss 1.8219 (Value 0.0051, Policy 1.8168)\n","Epoch 10/10: Loss 1.8002 (Value 0.0047, Policy 1.7955)\n","[2025-04-22 01:41:17] Training completed in 19.15s\n","[2025-04-22 01:41:17] Iteration 153 done in 300.58s\n","[2025-04-22 01:41:17] Resource usage: CPU 84.7%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:41:17] Network parameter sum after training: 11665.709158\n","[2025-04-22 01:41:17] \n","=== Iteration 154/200 ===\n","[2025-04-22 01:41:17] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11665.710272\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:20<17:23, 80.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:40<01:25, 12.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▍   | 9/14 [03:50<01:50, 22.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:56<00:00, 21.18s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:46:14] Self-play completed in 296.48s, generated 3390 examples (11.4 games/s)\n","[2025-04-22 01:46:14] Training network...\n","Epoch 1/10: Loss 1.8679 (Value 0.0167, Policy 1.8512)\n","Epoch 2/10: Loss 1.8480 (Value 0.0136, Policy 1.8344)\n","Epoch 3/10: Loss 1.8389 (Value 0.0113, Policy 1.8277)\n","Epoch 4/10: Loss 1.8403 (Value 0.0084, Policy 1.8319)\n","Epoch 5/10: Loss 1.8323 (Value 0.0083, Policy 1.8240)\n","Epoch 6/10: Loss 1.8432 (Value 0.0059, Policy 1.8373)\n","Epoch 7/10: Loss 1.8095 (Value 0.0063, Policy 1.8032)\n","Epoch 8/10: Loss 1.8373 (Value 0.0053, Policy 1.8320)\n","Epoch 9/10: Loss 1.8334 (Value 0.0051, Policy 1.8283)\n","Epoch 10/10: Loss 1.7869 (Value 0.0044, Policy 1.7824)\n","[2025-04-22 01:46:34] Training completed in 20.07s\n","[2025-04-22 01:46:34] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:16<02:24, 16.01s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:46:50] Game 1: duration=16.01s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:34<02:20, 17.55s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:47:08] Game 2: duration=18.62s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:49<01:52, 16.10s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:47:23] Game 3: duration=14.39s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:02<01:30, 15.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:47:36] Game 4: duration=13.66s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:13<01:07, 13.46s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:47:47] Game 5: duration=10.47s, moves=200, our_VP=2, winner=1 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:28<00:56, 14.20s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:48:02] Game 6: duration=15.65s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:44<00:43, 14.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:48:18] Game 7: duration=15.41s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:55<00:27, 13.62s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:48:29] Game 8: duration=11.53s, moves=200, our_VP=2, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:09<00:13, 13.78s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:48:43] Game 9: duration=14.13s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:21<00:00, 14.14s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:48:55] Game 10: duration=11.51s, moves=200, our_VP=5, winner=0 VP=5\n","[2025-04-22 01:48:55] Evaluated 10 games in 141.38s (0.07 games/s)\n","[2025-04-22 01:48:55] Evaluation results: win_rate=0.80, avg_vp=5.30, avg_length=200.00, total_moves=2000\n","[2025-04-22 01:48:55] Eval resource usage: CPU 62.2%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:48:55] Evaluation completed in 141.39s\n","[2025-04-22 01:48:55] New best model at iteration 154 (win_rate=0.80)\n","[2025-04-22 01:48:55] Checkpoint saved: models/model_iter_154.pt\n","[2025-04-22 01:48:55] Best model saved: models/best_model.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:48:59] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 404.1 MB)\n","[2025-04-22 01:48:59] Iteration 154 done in 461.51s\n","[2025-04-22 01:48:59] Resource usage: CPU 19.8%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:48:59] Network parameter sum after training: 11696.365165\n","[2025-04-22 01:48:59] \n","=== Iteration 155/200 ===\n","[2025-04-22 01:48:59] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11696.363761\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [02:05<27:07, 125.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▍        | 2/14 [02:10<10:57, 54.78s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  29%|██▊       | 4/14 [02:21<03:34, 21.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  43%|████▎     | 6/14 [02:37<01:51, 13.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 7/14 [02:50<01:33, 13.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  79%|███████▊  | 11/14 [04:29<00:47, 15.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:58<00:00, 21.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:53:57] Self-play completed in 298.16s, generated 3447 examples (11.6 games/s)\n","[2025-04-22 01:53:57] Training network...\n","Epoch 1/10: Loss 1.8649 (Value 0.0174, Policy 1.8475)\n","Epoch 2/10: Loss 1.8473 (Value 0.0125, Policy 1.8348)\n","Epoch 3/10: Loss 1.8248 (Value 0.0093, Policy 1.8155)\n","Epoch 4/10: Loss 1.8433 (Value 0.0083, Policy 1.8349)\n","Epoch 5/10: Loss 1.8305 (Value 0.0065, Policy 1.8240)\n","Epoch 6/10: Loss 1.8064 (Value 0.0047, Policy 1.8016)\n","Epoch 7/10: Loss 1.8192 (Value 0.0050, Policy 1.8142)\n","Epoch 8/10: Loss 1.8379 (Value 0.0042, Policy 1.8337)\n","Epoch 9/10: Loss 1.8159 (Value 0.0041, Policy 1.8117)\n","Epoch 10/10: Loss 1.8072 (Value 0.0035, Policy 1.8036)\n","[2025-04-22 01:54:17] Training completed in 19.74s\n","[2025-04-22 01:54:17] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 01:54:17] Checkpoint saved: models/model_iter_155.pt\n","[2025-04-22 01:54:20] Replay buffer saved: models/latest_buffer.pkl (100000 examples, 404.0 MB)\n","[2025-04-22 01:54:20] Iteration 155 done in 321.36s\n","[2025-04-22 01:54:20] Resource usage: CPU 82.3%, RAM 8.1%, GPU peak memory 0.00 GB\n","[2025-04-22 01:54:20] Network parameter sum after training: 11710.967009\n","[2025-04-22 01:54:20] \n","=== Iteration 156/200 ===\n","[2025-04-22 01:54:20] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/14 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 11710.967238\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   7%|▋         | 1/14 [01:58<25:45, 118.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 7/14 [02:49<01:20, 11.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 14/14 [04:45<00:00, 20.42s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:59:06] Self-play completed in 285.96s, generated 3354 examples (11.7 games/s)\n","[2025-04-22 01:59:06] Training network...\n","Epoch 1/10: Loss 1.8716 (Value 0.0172, Policy 1.8544)\n","Epoch 2/10: Loss 1.8478 (Value 0.0111, Policy 1.8367)\n","Epoch 3/10: Loss 1.8303 (Value 0.0094, Policy 1.8209)\n","Epoch 4/10: Loss 1.8097 (Value 0.0099, Policy 1.7998)\n","Epoch 5/10: Loss 1.8303 (Value 0.0102, Policy 1.8201)\n","Epoch 6/10: Loss 1.7997 (Value 0.0065, Policy 1.7932)\n","Epoch 7/10: Loss 1.8043 (Value 0.0072, Policy 1.7971)\n","Epoch 8/10: Loss 1.8268 (Value 0.0063, Policy 1.8205)\n","Epoch 9/10: Loss 1.8122 (Value 0.0071, Policy 1.8051)\n","Epoch 10/10: Loss 1.8226 (Value 0.0060, Policy 1.8165)\n","[2025-04-22 01:59:26] Training completed in 19.75s\n","[2025-04-22 01:59:26] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:14<02:14, 14.95s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 01:59:41] Game 1: duration=14.95s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:35<02:24, 18.05s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:00:01] Game 2: duration=20.21s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:53<02:08, 18.31s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:00:20] Game 3: duration=18.63s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:09<01:42, 17.09s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:00:35] Game 4: duration=15.22s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:26<01:26, 17.35s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:00:53] Game 5: duration=17.82s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:42<01:06, 16.70s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:01:08] Game 6: duration=15.42s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:52<00:44, 14.69s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:01:19] Game 7: duration=10.55s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [02:09<00:30, 15.33s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:01:35] Game 8: duration=16.72s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:24<00:15, 15.25s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 02:01:50] Game 9: duration=15.08s, moves=164, our_VP=10, winner=0 VP=10\n"]}],"source":["timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","device = torch.device('cpu')\n","\n","# Set random seeds for reproducibility\n","def set_random_seeds(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    # if torch.cuda.is_available():\n","    #     torch.cuda.manual_seed(seed)\n","    #     torch.backends.cudnn.deterministic = True\n","    #     torch.backends.cudnn.benchmark = False\n","\n","set_random_seeds()\n","\n","# Step 5: Set up training parameters\n","# You can customize these parameters\n","import argparse\n","\n","# Parse arguments from command line or use defaults\n","# This allows you to change parameters when running the notebook\n","parser = argparse.ArgumentParser(description=\"AlphaZero Catan Training\")\n","parser.add_argument(\"--iterations\", type=int, default=50, help=\"Number of training iterations\")\n","parser.add_argument(\"--resume\", type=str, default=None, help=\"Path to checkpoint to resume from\")\n","parser.add_argument(\"--games\", type=int, default=20, help=\"Number of self-play games per iteration\")\n","parser.add_argument(\"--sims\", type=int, default=100, help=\"Number of MCTS simulations per move\")\n","parser.add_argument(\"--eval-games\", type=int, default=10, help=\"Number of evaluation games\")\n","parser.add_argument(\"--quick\", action=\"store_true\", help=\"Quick training (1 iteration, 2 games)\")\n","parser.add_argument(\"--medium\", action=\"store_true\", help=\"Medium training (10 iterations, 5 games)\")\n","parser.add_argument(\"--full\", action=\"store_true\", help=\"Full training (50 iterations, 20 games)\")\n","parser.add_argument(\"--overnight\", action=\"store_true\", help=\"Overnight training (100 iterations, 30 games)\")\n","\n","# Parse the arguments directly\n","args = parser.parse_args(['--overnight', '--resume', 'models/model_iter_116.pt'])\n","#just overnight no resume\n","# args = parser.parse_args(['--medium'])\n","# Configure training mode\n","if args.quick:\n","    print(\"Running in QUICK mode\")\n","    args.iterations = 1\n","    args.games = 2\n","    args.sims = 10\n","    args.eval_games = 2\n","elif args.medium:\n","    print(\"Running in MEDIUM mode\")\n","    args.iterations = 10\n","    args.games = 5\n","    args.sims = 50\n","    args.eval_games = 5\n","elif args.full:\n","    print(\"Running in FULL mode\")\n","    args.iterations = 50\n","    args.games = 20\n","    args.sims = 100\n","    args.eval_games = 10\n","elif args.overnight:\n","    print(\"Running in OVERNIGHT mode\")\n","    args.iterations = 200\n","    args.games = 14\n","    args.sims = 150\n","    args.eval_games = 10\n","\n","print(f\"\\n=== AlphaZero Catan Training ===\")\n","print(f\"Iterations: {args.iterations}\")\n","print(f\"Self-play games per iteration: {args.games}\")\n","print(f\"MCTS simulations per move: {args.sims}\")\n","print(f\"Resume from: {args.resume if args.resume else 'Starting fresh'}\")\n","\n","# Step 6: Get configuration and modify for GPU\n","from AlphaZero.utils.config import get_config\n","config = get_config()\n","\n","# Customize config with command line arguments\n","config['num_iterations'] = args.iterations\n","config['self_play_games'] = args.games\n","config['num_simulations'] = args.sims\n","config['eval_games'] = args.eval_games\n","config['device'] = 'cpu'\n","\n","# Step 7: Create logs and models directories\n","!mkdir -p logs\n","!mkdir -p models\n","!mkdir -p plots\n","\n","# Step 8: Start the training\n","from AlphaZero.training.training_pipeline import TrainingPipeline\n","\n","try:\n","    # Start time tracking\n","    start_time = time.time()\n","\n","    # Create the training pipeline\n","    pipeline = TrainingPipeline(config)\n","\n","    # Train for the specified iterations\n","    pipeline.train(args.iterations, resume_from=args.resume)\n","\n","    # Calculate total training time\n","    total_time = time.time() - start_time\n","    hours = int(total_time // 3600)\n","    minutes = int((total_time % 3600) // 60)\n","    seconds = int(total_time % 60)\n","\n","    print(f\"\\nTraining completed in {hours}h {minutes}m {seconds}s\")\n","\n","except KeyboardInterrupt:\n","    print(\"\\nTraining interrupted! Saving checkpoint...\")\n","    pipeline.save_model(pipeline.current_iteration)\n","    print(\"Checkpoint saved. You can resume with this checkpoint later.\")\n","except Exception as e:\n","    print(f\"Error during training: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","# Step 9: Copy results back to Google Drive\n","!mkdir -p {DRIVE_PATH}/models_{timestamp}\n","!mkdir -p {DRIVE_PATH}/logs_{timestamp}\n","!mkdir -p {DRIVE_PATH}/plots_{timestamp}\n","\n","!cp -r models/* {DRIVE_PATH}/models_{timestamp}/\n","!cp -r logs/* {DRIVE_PATH}/logs_{timestamp}/\n","!cp -r plots/* {DRIVE_PATH}/plots_{timestamp}/\n","\"\"\n","print(f\"\\nTraining results saved to Google Drive in folders with timestamp {timestamp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHuFkqUoxFQS"},"outputs":[],"source":["# ===== CPU model, core / thread counts, and base turbo freq =====\n","!lscpu | egrep 'Model name|Socket|Thread|Core|MHz'\n","\n","# ===== Current clock speed of every logical core (updates once) =====\n","!grep \\\"cpu MHz\\\" /proc/cpuinfo | head\n","\n","# ===== Simple “how fast is it?” micro‑benchmark =====\n","import time, numpy as np\n","N = 6000\n","a = np.random.randn(N, N).astype(np.float32)\n","b = np.random.randn(N, N).astype(np.float32)\n","\n","t0 = time.time()\n","c = a @ b          # single BLAS call – leverages all cores & any MKL/OPENBLAS\n","elapsed = time.time() - t0\n","gflops = 2*N**3 / elapsed / 1e9\n","\n","print(f\"\\n{elapsed:.3f} s   ≈ {gflops:.1f} GFLOP/s (single large mat‑mul)\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSyRcbrx8dKw"},"outputs":[],"source":["!grep -m1 'model name' /proc/cpuinfo\n","!nproc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTO50m079uNh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyO9T8EXCnbA/eoKEWDHaFmz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}