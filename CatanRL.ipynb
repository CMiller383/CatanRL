{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1745340163486,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"QAdl_8jWB0x7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"13852a74-bf46-4b3a-bb91-e397563f9a1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["# Add this to the first cell of your notebook\n","%load_ext autoreload\n","%autoreload 2  # Reload all modules (except those excluded) before executing code"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1745340163489,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"7qh7Y0T7_t6i"},"outputs":[],"source":["import os\n","# os.environ['CUDA_VISIBLE_DEVICES'] = ''   # comment this line if you want GPU again"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1745340163491,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"pnU2iekEOITt"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2173,"status":"ok","timestamp":1745340163425,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"pyqs4AMC9RBn","outputId":"dd81ac5c-5f61-452c-90d3-0aac49eed110"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3312,"status":"ok","timestamp":1745340166805,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"NdAJJ_up9TaG"},"outputs":[],"source":["!pip install tqdm psutil plotly kaleido --quiet\n","import os\n","import sys\n","import random\n","import time\n","import threading\n","import IPython\n","from google.colab import output\n","from datetime import datetime"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5220,"status":"ok","timestamp":1745340172027,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"sIKijZZvPOCr","outputId":"b19dd450-7777-46ba-d5fe-88b6aca94ba5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing compatible package versions...\n","PyTorch post-install: 2.5.1+cu124\n","NumPy post-install: 2.0.1\n","CUDA setup: available=True, device count=1\n","Current CUDA device: 0, name: NVIDIA L4\n"]}],"source":["# Ensure version compatibility with local setup\n","print(\"Installing compatible package versions...\")\n","\n","# Install specific versions to match local setup\n","!pip install torch==2.5.1 numpy==2.0.1 --quiet\n","import numpy as np\n","\n","\n","# Verify PyTorch and NumPy versions after installation\n","!python -c \"import torch; print(f'PyTorch post-install: {torch.__version__}')\"\n","!python -c \"import numpy; print(f'NumPy post-install: {numpy.__version__}')\"\n","\n","# Force CUDA setup for PyTorch\n","import torch\n","print(f\"CUDA setup: available={torch.cuda.is_available()}, device count={torch.cuda.device_count() if torch.cuda.is_available() else 0}\")\n","if torch.cuda.is_available():\n","    print(f\"Current CUDA device: {torch.cuda.current_device()}, name: {torch.cuda.get_device_name()}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1745340172069,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"VTL-hrwJ9eno","outputId":"dfb130bd-1d25-48de-e02e-dcfc0388a361"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CatanRL\n"]}],"source":["# Set path to your project on Google Drive\n","DRIVE_PATH = '/content/drive/MyDrive/CatanRL'\n","\n","# Change to the project directory\n","%cd {DRIVE_PATH}\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1745340172088,"user":{"displayName":"Cole Miller","userId":"17932161436482262491"},"user_tz":240},"id":"f2u4zvSB_4S-","outputId":"2b618c05-1e54-4106-924f-33207629607f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting keep‑alive thread …\n"]}],"source":["import time, threading\n","from google.colab import output\n","\n","# 1. Define a dummy no‑op Python callback.\n","def _noop():\n","    return \"ok\"\n","\n","# 2. Register it once – gives us a handle \"keep_alive\"\n","output.register_callback('keep_alive', _noop)\n","\n","def keep_colab_alive(interval_sec: int = 60):\n","    \"\"\"Ping the front‑end every <interval_sec> seconds.\n","\n","    Works in 2025‑04 Colab because it uses the same mechanism Colab widgets use.\n","    \"\"\"\n","    while True:\n","        try:\n","            # JS in the page calls the Python no‑op; the round‑trip is what matters\n","            output.eval_js('google.colab.kernel.invokeFunction(\"keep_alive\", [], {})')\n","            print(\"♥\", end=\"\", flush=True)\n","        except Exception:\n","            # If the socket was momentarily closed, ignore and retry\n","            pass\n","        time.sleep(interval_sec)\n","\n","print(\"Starting keep‑alive thread …\")\n","threading.Thread(target=keep_colab_alive, daemon=True).start()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOm4-LNE9GuY","outputId":"f7cd7cb4-7286-4b62-c9ab-b13275085fe6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in OVERNIGHT mode\n","\n","=== AlphaZero Catan Training ===\n","Iterations: 300\n","Self-play games per iteration: 22\n","MCTS simulations per move: 150\n","Resume from: models/model_iter_280.pt\n","[2025-04-22 16:42:53] AlphaZero Catan Training started at 20250422_164253\n","[2025-04-22 16:42:53] Configuration: {'state_dim': 992, 'action_dim': 200, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_iterations': 300, 'self_play_games': 22, 'eval_games': 10, 'epochs': 10, 'batch_size': 256, 'buffer_size': 50000, 'num_simulations': 150, 'c_puct': 1.5, 'mcts_batch_size': 12, 'noise_eps': 0.25, 'noise_alpha': 0.3, 'max_moves': 200, 'device': 'cpu', 'placement_epochs': 10, 'placement_batch_size': 32, 'placement_lr': 0.001, 'placement_hidden_dim': 128, 'placement_train_frequency': 5, 'train_placement_network': True, 'use_placement_network': False, 'model_dir': 'models', 'starting_iter': 281}\n","[2025-04-22 16:42:53] Loading default placement network from models/placement_network.pt\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/CatanRL/AlphaZero/training/training_pipeline.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(placement_path, map_location=torch.device('cpu'))\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 16:42:54] Network dimensions: input_dim=260, hidden_dim=128, output_dim=54\n","[2025-04-22 16:42:54] Successfully loaded default placement network\n","[2025-04-22 16:42:55] Placement network components initialized successfully\n","[2025-04-22 16:42:55] Initial placement network training is ENABLED\n","[2025-04-22 16:42:55] Placement network settings:\n","[2025-04-22 16:42:55]   - Training epochs: 10\n","[2025-04-22 16:42:55]   - Batch size: 32\n","[2025-04-22 16:42:55]   - Learning rate: 0.001\n","[2025-04-22 16:42:55]   - Hidden dimensions: 128\n","[2025-04-22 16:42:55]   - Training frequency: Every 5 iterations\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/CatanRL/AlphaZero/training/training_pipeline.py:644: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(path)\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 16:42:57] Warning: Failed to load replay buffer: Ran out of input\n","[2025-04-22 16:42:59] Backup buffer loaded: 50000 examples, 201.6 MB\n","[2025-04-22 16:42:59] Checkpoint loaded from models/model_iter_280.pt, resuming from iteration 280\n","[2025-04-22 16:42:59] Resuming training from iteration 280\n","[2025-04-22 16:42:59] \n","=== Iteration 281/300 ===\n","[2025-04-22 16:42:59] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(checkpoint_path, map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22125.208540\n","Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  55%|█████▍    | 12/22 [03:46<03:36, 21.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.13 (moves: 149/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  59%|█████▉    | 13/22 [04:15<03:35, 23.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  73%|███████▎  | 16/22 [04:22<00:59,  9.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.02 (moves: 193/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  95%|█████████▌| 21/22 [04:55<00:07,  7.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [05:00<00:00, 13.66s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 16:47:59] Self-play completed in 300.48s, generated 5181 examples (17.2 games/s)\n","[2025-04-22 16:47:59] Processing 4 games for initial placement data\n","[2025-04-22 16:47:59] Extracted 720 initial placement examples\n","[2025-04-22 16:47:59] Skipping placement network training for iteration 281 (training every 5 iterations)\n","[2025-04-22 16:47:59] Training network...\n","Epoch 1/10: Loss 1.8781 (Value 0.0381, Policy 1.8400)\n","Epoch 2/10: Loss 1.8460 (Value 0.0252, Policy 1.8208)\n","Epoch 3/10: Loss 1.8234 (Value 0.0222, Policy 1.8012)\n","Epoch 4/10: Loss 1.8258 (Value 0.0170, Policy 1.8088)\n","Epoch 5/10: Loss 1.8045 (Value 0.0133, Policy 1.7912)\n","Epoch 6/10: Loss 1.7876 (Value 0.0115, Policy 1.7761)\n","Epoch 7/10: Loss 1.7918 (Value 0.0101, Policy 1.7817)\n","Epoch 8/10: Loss 1.7737 (Value 0.0097, Policy 1.7640)\n","Epoch 9/10: Loss 1.7527 (Value 0.0092, Policy 1.7435)\n","Epoch 10/10: Loss 1.7631 (Value 0.0084, Policy 1.7546)\n","[2025-04-22 16:48:17] Training completed in 18.17s\n","[2025-04-22 16:48:17] Iteration 281 done in 318.84s\n","[2025-04-22 16:48:17] Resource usage: CPU 68.8%, RAM 11.6%, GPU peak memory 0.00 GB\n","[2025-04-22 16:48:17] Network parameter sum after training: 22220.956830\n","[2025-04-22 16:48:17] \n","=== Iteration 282/300 ===\n","[2025-04-22 16:48:17] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22220.954881\n","Win reward: base=1.00, time_bonus=0.24 (moves: 105/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:29<31:16, 89.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.27 (moves: 93/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [01:47<08:24, 26.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  18%|█▊        | 4/22 [01:49<05:03, 16.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  41%|████      | 9/22 [02:33<02:00,  9.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [02:47<01:27,  7.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  55%|█████▍    | 12/22 [03:27<02:57, 17.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  59%|█████▉    | 13/22 [04:10<03:49, 25.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  73%|███████▎  | 16/22 [04:23<01:10, 11.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  77%|███████▋  | 17/22 [04:23<00:41,  8.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  82%|████████▏ | 18/22 [04:25<00:25,  6.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  91%|█████████ | 20/22 [04:35<00:11,  5.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  95%|█████████▌| 21/22 [04:35<00:03,  3.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:47<00:00, 13.06s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 16:53:05] Self-play completed in 287.29s, generated 5023 examples (17.5 games/s)\n","[2025-04-22 16:53:05] Processing 4 games for initial placement data\n","[2025-04-22 16:53:05] Extracted 585 initial placement examples\n","[2025-04-22 16:53:05] Skipping placement network training for iteration 282 (training every 5 iterations)\n","[2025-04-22 16:53:05] Training network...\n","Epoch 1/10: Loss 1.8723 (Value 0.0373, Policy 1.8350)\n","Epoch 2/10: Loss 1.8428 (Value 0.0265, Policy 1.8163)\n","Epoch 3/10: Loss 1.8281 (Value 0.0206, Policy 1.8075)\n","Epoch 4/10: Loss 1.8098 (Value 0.0184, Policy 1.7914)\n","Epoch 5/10: Loss 1.8002 (Value 0.0139, Policy 1.7863)\n","Epoch 6/10: Loss 1.7862 (Value 0.0135, Policy 1.7726)\n","Epoch 7/10: Loss 1.7956 (Value 0.0110, Policy 1.7846)\n","Epoch 8/10: Loss 1.7674 (Value 0.0109, Policy 1.7564)\n","Epoch 9/10: Loss 1.7571 (Value 0.0084, Policy 1.7488)\n","Epoch 10/10: Loss 1.7855 (Value 0.0089, Policy 1.7766)\n","[2025-04-22 16:53:23] Training completed in 18.18s\n","[2025-04-22 16:53:23] Iteration 282 done in 305.64s\n","[2025-04-22 16:53:23] Resource usage: CPU 83.5%, RAM 11.6%, GPU peak memory 0.00 GB\n","[2025-04-22 16:53:23] Network parameter sum after training: 22396.449889\n","[2025-04-22 16:53:23] \n","=== Iteration 283/300 ===\n","[2025-04-22 16:53:23] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22396.449641\n","Win reward: base=1.00, time_bonus=0.32 (moves: 73/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:07<23:34, 67.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [01:59<12:11, 38.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.33 (moves: 69/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  45%|████▌     | 10/22 [02:35<01:25,  7.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▎   | 14/22 [04:03<02:44, 20.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  82%|████████▏ | 18/22 [04:25<00:35,  8.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:53<00:00, 13.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 16:58:16] Self-play completed in 293.02s, generated 5040 examples (17.2 games/s)\n","[2025-04-22 16:58:16] Processing 4 games for initial placement data\n","[2025-04-22 16:58:16] Extracted 782 initial placement examples\n","[2025-04-22 16:58:16] Skipping placement network training for iteration 283 (training every 5 iterations)\n","[2025-04-22 16:58:16] Training network...\n","Epoch 1/10: Loss 1.8622 (Value 0.0358, Policy 1.8263)\n","Epoch 2/10: Loss 1.8535 (Value 0.0259, Policy 1.8277)\n","Epoch 3/10: Loss 1.8164 (Value 0.0205, Policy 1.7959)\n","Epoch 4/10: Loss 1.8315 (Value 0.0173, Policy 1.8141)\n","Epoch 5/10: Loss 1.8145 (Value 0.0166, Policy 1.7979)\n","Epoch 6/10: Loss 1.8164 (Value 0.0154, Policy 1.8010)\n","Epoch 7/10: Loss 1.7844 (Value 0.0115, Policy 1.7730)\n","Epoch 8/10: Loss 1.7686 (Value 0.0077, Policy 1.7609)\n","Epoch 9/10: Loss 1.7818 (Value 0.0072, Policy 1.7746)\n","Epoch 10/10: Loss 1.7678 (Value 0.0070, Policy 1.7608)\n","[2025-04-22 16:58:35] Training completed in 18.39s\n","[2025-04-22 16:58:35] Iteration 283 done in 311.63s\n","[2025-04-22 16:58:35] Resource usage: CPU 80.5%, RAM 11.7%, GPU peak memory 0.00 GB\n","[2025-04-22 16:58:35] Network parameter sum after training: 22318.442975\n","[2025-04-22 16:58:35] \n","=== Iteration 284/300 ===\n","[2025-04-22 16:58:35] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22318.444604\n","Win reward: base=1.00, time_bonus=0.28 (moves: 89/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:11<25:00, 71.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   9%|▉         | 2/22 [01:13<10:13, 30.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.12 (moves: 153/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  18%|█▊        | 4/22 [02:03<07:51, 26.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  27%|██▋       | 6/22 [02:11<03:35, 13.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  41%|████      | 9/22 [02:39<02:25, 11.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [04:07<03:35, 23.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  77%|███████▋  | 17/22 [04:32<00:49,  9.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  82%|████████▏ | 18/22 [04:34<00:30,  7.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:55<00:00, 13.42s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:03:30] Self-play completed in 295.32s, generated 5119 examples (17.3 games/s)\n","[2025-04-22 17:03:30] Processing 4 games for initial placement data\n","[2025-04-22 17:03:30] Extracted 466 initial placement examples\n","[2025-04-22 17:03:30] Skipping placement network training for iteration 284 (training every 5 iterations)\n","[2025-04-22 17:03:30] Training network...\n","Epoch 1/10: Loss 1.8594 (Value 0.0384, Policy 1.8210)\n","Epoch 2/10: Loss 1.8605 (Value 0.0284, Policy 1.8321)\n","Epoch 3/10: Loss 1.8312 (Value 0.0233, Policy 1.8079)\n","Epoch 4/10: Loss 1.8072 (Value 0.0166, Policy 1.7906)\n","Epoch 5/10: Loss 1.8091 (Value 0.0120, Policy 1.7971)\n","Epoch 6/10: Loss 1.8039 (Value 0.0107, Policy 1.7932)\n","Epoch 7/10: Loss 1.7760 (Value 0.0097, Policy 1.7663)\n","Epoch 8/10: Loss 1.7809 (Value 0.0082, Policy 1.7728)\n","Epoch 9/10: Loss 1.7578 (Value 0.0075, Policy 1.7503)\n","Epoch 10/10: Loss 1.7681 (Value 0.0080, Policy 1.7600)\n","[2025-04-22 17:03:48] Training completed in 18.21s\n","[2025-04-22 17:03:48] Iteration 284 done in 313.66s\n","[2025-04-22 17:03:48] Resource usage: CPU 82.7%, RAM 11.7%, GPU peak memory 0.00 GB\n","[2025-04-22 17:03:48] Network parameter sum after training: 22377.403015\n","[2025-04-22 17:03:48] \n","=== Iteration 285/300 ===\n","[2025-04-22 17:03:48] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22377.403046\n","Win reward: base=1.00, time_bonus=0.34 (moves: 65/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [00:52<18:27, 52.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.24 (moves: 105/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [02:04<12:45, 40.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  18%|█▊        | 4/22 [02:05<07:24, 24.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  27%|██▋       | 6/22 [02:24<04:21, 16.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.20 (moves: 121/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [02:39<00:56,  5.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  55%|█████▍    | 12/22 [02:53<01:16,  7.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.23 (moves: 109/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  59%|█████▉    | 13/22 [03:30<02:31, 16.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.17 (moves: 133/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  73%|███████▎  | 16/22 [04:20<01:47, 17.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  91%|█████████ | 20/22 [04:34<00:13,  6.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:53<00:00, 13.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:08:42] Self-play completed in 293.99s, generated 5060 examples (17.2 games/s)\n","[2025-04-22 17:08:42] Processing 4 games for initial placement data\n","[2025-04-22 17:08:43] Extracted 700 initial placement examples\n","[2025-04-22 17:08:43] Training initial placement network on 700 examples\n","Epoch 1/10: Loss = 1.4020, Accuracy = 0.0193\n","Epoch 2/10: Loss = 1.3867, Accuracy = 0.0432\n","Epoch 3/10: Loss = 1.3684, Accuracy = 0.0789\n","Epoch 4/10: Loss = 1.3402, Accuracy = 0.0848\n","Epoch 5/10: Loss = 1.3327, Accuracy = 0.0818\n","Epoch 6/10: Loss = 1.3100, Accuracy = 0.0833\n","Epoch 7/10: Loss = 1.2934, Accuracy = 0.0938\n","Epoch 8/10: Loss = 1.2844, Accuracy = 0.0893\n","Epoch 9/10: Loss = 1.2552, Accuracy = 0.0908\n","Epoch 10/10: Loss = 1.2643, Accuracy = 0.0893\n","[2025-04-22 17:08:43] Placement network training completed in 0.86s\n","[2025-04-22 17:08:43] Metrics: Loss = 1.3237, Accuracy = 0.0754\n","[2025-04-22 17:08:43] Saved placement network to models/placement_network.pt\n","[2025-04-22 17:08:43] Training network...\n","Epoch 1/10: Loss 1.8600 (Value 0.0399, Policy 1.8200)\n","Epoch 2/10: Loss 1.8446 (Value 0.0249, Policy 1.8197)\n","Epoch 3/10: Loss 1.8312 (Value 0.0172, Policy 1.8140)\n","Epoch 4/10: Loss 1.7992 (Value 0.0148, Policy 1.7844)\n","Epoch 5/10: Loss 1.7906 (Value 0.0114, Policy 1.7792)\n","Epoch 6/10: Loss 1.7927 (Value 0.0094, Policy 1.7833)\n","Epoch 7/10: Loss 1.7676 (Value 0.0095, Policy 1.7581)\n","Epoch 8/10: Loss 1.7792 (Value 0.0097, Policy 1.7695)\n","Epoch 9/10: Loss 1.7914 (Value 0.0092, Policy 1.7822)\n","Epoch 10/10: Loss 1.7487 (Value 0.0083, Policy 1.7404)\n","[2025-04-22 17:09:02] Training completed in 18.61s\n","[2025-04-22 17:09:02] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:15<02:22, 15.84s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:09:18] Game 1: duration=15.84s, moves=200, our_VP=3, winner=1 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:29<01:58, 14.78s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:09:32] Game 2: duration=14.04s, moves=200, our_VP=4, winner=2 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:45<01:47, 15.38s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:09:48] Game 3: duration=16.10s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [01:00<01:29, 14.94s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:10:02] Game 4: duration=14.26s, moves=200, our_VP=4, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:11<01:08, 13.61s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:10:14] Game 5: duration=11.26s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:26<00:55, 13.94s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:10:28] Game 6: duration=14.56s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:39<00:40, 13.64s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:10:41] Game 7: duration=13.03s, moves=200, our_VP=2, winner=3 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:56<00:29, 14.69s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:10:58] Game 8: duration=16.93s, moves=142, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:07<00:13, 13.62s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:11:09] Game 9: duration=11.26s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:21<00:00, 14.12s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:11:23] Game 10: duration=13.93s, moves=200, our_VP=4, winner=0 VP=4\n","[2025-04-22 17:11:23] Evaluated 10 games in 141.22s (0.07 games/s)\n","[2025-04-22 17:11:23] Evaluation results: win_rate=0.60, avg_vp=5.50, avg_length=194.20, total_moves=1942\n","[2025-04-22 17:11:23] Eval resource usage: CPU 58.2%, RAM 11.8%, GPU peak memory 0.00 GB\n","[2025-04-22 17:11:23] Evaluation completed in 141.23s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:11:25] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 17:11:25] Checkpoint saved: models/model_iter_285.pt\n","[2025-04-22 17:11:25] Saved placement network to models/placement_network.pt\n","[2025-04-22 17:11:26] Replay buffer saved: models/latest_buffer.pkl (50000 examples, 201.7 MB)\n","[2025-04-22 17:11:26] Iteration 285 done in 457.64s\n","[2025-04-22 17:11:26] Resource usage: CPU 9.7%, RAM 12.0%, GPU peak memory 0.00 GB\n","[2025-04-22 17:11:26] Network parameter sum after training: 22579.423485\n","[2025-04-22 17:11:26] \n","=== Iteration 286/300 ===\n","[2025-04-22 17:11:26] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22579.423046\n","Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  18%|█▊        | 4/22 [02:16<06:09, 20.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  23%|██▎       | 5/22 [02:21<04:14, 14.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.01 (moves: 197/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  45%|████▌     | 10/22 [02:49<01:27,  7.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 11/22 [02:58<01:24,  7.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.29 (moves: 85/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  55%|█████▍    | 12/22 [03:59<03:59, 23.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  95%|█████████▌| 21/22 [04:42<00:02,  2.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:44<00:00, 12.94s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:16:11] Self-play completed in 284.60s, generated 5121 examples (18.0 games/s)\n","[2025-04-22 17:16:11] Processing 4 games for initial placement data\n","[2025-04-22 17:16:11] Extracted 765 initial placement examples\n","[2025-04-22 17:16:11] Skipping placement network training for iteration 286 (training every 5 iterations)\n","[2025-04-22 17:16:11] Training network...\n","Epoch 1/10: Loss 1.8632 (Value 0.0317, Policy 1.8314)\n","Epoch 2/10: Loss 1.8755 (Value 0.0248, Policy 1.8507)\n","Epoch 3/10: Loss 1.8224 (Value 0.0177, Policy 1.8047)\n","Epoch 4/10: Loss 1.8171 (Value 0.0128, Policy 1.8043)\n","Epoch 5/10: Loss 1.8070 (Value 0.0118, Policy 1.7952)\n","Epoch 6/10: Loss 1.7993 (Value 0.0101, Policy 1.7892)\n","Epoch 7/10: Loss 1.7879 (Value 0.0074, Policy 1.7804)\n","Epoch 8/10: Loss 1.7790 (Value 0.0074, Policy 1.7716)\n","Epoch 9/10: Loss 1.7861 (Value 0.0076, Policy 1.7785)\n","Epoch 10/10: Loss 1.7853 (Value 0.0069, Policy 1.7785)\n","[2025-04-22 17:16:29] Training completed in 18.50s\n","[2025-04-22 17:16:29] Iteration 286 done in 303.31s\n","[2025-04-22 17:16:29] Resource usage: CPU 88.5%, RAM 12.0%, GPU peak memory 0.00 GB\n","[2025-04-22 17:16:29] Network parameter sum after training: 22625.208644\n","[2025-04-22 17:16:29] \n","=== Iteration 287/300 ===\n","[2025-04-22 17:16:29] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22625.207949\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [02:07<09:53, 31.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  27%|██▋       | 6/22 [02:17<02:46, 10.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▋      | 8/22 [02:23<01:26,  6.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  45%|████▌     | 10/22 [02:35<01:13,  6.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  55%|█████▍    | 12/22 [04:02<04:47, 28.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  77%|███████▋  | 17/22 [04:34<00:42,  8.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:47<00:00, 13.05s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:21:17] Self-play completed in 287.09s, generated 5117 examples (17.8 games/s)\n","[2025-04-22 17:21:17] Processing 4 games for initial placement data\n","[2025-04-22 17:21:17] Extracted 675 initial placement examples\n","[2025-04-22 17:21:17] Skipping placement network training for iteration 287 (training every 5 iterations)\n","[2025-04-22 17:21:17] Training network...\n","Epoch 1/10: Loss 1.8548 (Value 0.0366, Policy 1.8182)\n","Epoch 2/10: Loss 1.8569 (Value 0.0275, Policy 1.8294)\n","Epoch 3/10: Loss 1.8372 (Value 0.0222, Policy 1.8150)\n","Epoch 4/10: Loss 1.8144 (Value 0.0186, Policy 1.7957)\n","Epoch 5/10: Loss 1.7976 (Value 0.0137, Policy 1.7839)\n","Epoch 6/10: Loss 1.7769 (Value 0.0086, Policy 1.7682)\n","Epoch 7/10: Loss 1.7836 (Value 0.0084, Policy 1.7752)\n","Epoch 8/10: Loss 1.7686 (Value 0.0068, Policy 1.7618)\n","Epoch 9/10: Loss 1.7953 (Value 0.0068, Policy 1.7884)\n","Epoch 10/10: Loss 1.7603 (Value 0.0058, Policy 1.7545)\n","[2025-04-22 17:21:35] Training completed in 18.59s\n","[2025-04-22 17:21:35] Iteration 287 done in 305.86s\n","[2025-04-22 17:21:35] Resource usage: CPU 87.2%, RAM 12.0%, GPU peak memory 0.00 GB\n","[2025-04-22 17:21:35] Network parameter sum after training: 22598.877681\n","[2025-04-22 17:21:35] \n","=== Iteration 288/300 ===\n","[2025-04-22 17:21:35] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22598.876624\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [02:14<09:55, 31.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  18%|█▊        | 4/22 [02:26<07:06, 23.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.04 (moves: 185/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▋      | 8/22 [02:34<01:37,  6.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [03:02<01:43,  9.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.35 (moves: 61/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  55%|█████▍    | 12/22 [03:14<01:41, 10.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▎   | 14/22 [04:00<02:08, 16.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  82%|████████▏ | 18/22 [04:34<00:40, 10.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  86%|████████▋ | 19/22 [04:34<00:21,  7.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [05:03<00:00, 13.80s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:26:39] Self-play completed in 303.72s, generated 5118 examples (16.9 games/s)\n","[2025-04-22 17:26:39] Processing 4 games for initial placement data\n","[2025-04-22 17:26:39] Extracted 690 initial placement examples\n","[2025-04-22 17:26:39] Skipping placement network training for iteration 288 (training every 5 iterations)\n","[2025-04-22 17:26:39] Training network...\n","Epoch 1/10: Loss 1.8656 (Value 0.0338, Policy 1.8318)\n","Epoch 2/10: Loss 1.8434 (Value 0.0264, Policy 1.8170)\n","Epoch 3/10: Loss 1.8464 (Value 0.0235, Policy 1.8229)\n","Epoch 4/10: Loss 1.8266 (Value 0.0203, Policy 1.8063)\n","Epoch 5/10: Loss 1.8199 (Value 0.0170, Policy 1.8029)\n","Epoch 6/10: Loss 1.8159 (Value 0.0169, Policy 1.7990)\n","Epoch 7/10: Loss 1.7928 (Value 0.0125, Policy 1.7803)\n","Epoch 8/10: Loss 1.7641 (Value 0.0109, Policy 1.7533)\n","Epoch 9/10: Loss 1.7817 (Value 0.0096, Policy 1.7721)\n","Epoch 10/10: Loss 1.7685 (Value 0.0092, Policy 1.7593)\n","[2025-04-22 17:26:58] Training completed in 18.55s\n","[2025-04-22 17:26:58] Iteration 288 done in 322.46s\n","[2025-04-22 17:26:58] Resource usage: CPU 79.5%, RAM 12.1%, GPU peak memory 0.00 GB\n","[2025-04-22 17:26:58] Network parameter sum after training: 22730.979802\n","[2025-04-22 17:26:58] \n","=== Iteration 289/300 ===\n","[2025-04-22 17:26:58] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22730.980294\n","Win reward: base=1.00, time_bonus=0.24 (moves: 105/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  18%|█▊        | 4/22 [02:14<07:22, 24.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  23%|██▎       | 5/22 [02:22<05:16, 18.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  45%|████▌     | 10/22 [02:45<01:21,  6.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [04:01<03:01, 20.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▎   | 14/22 [04:25<02:49, 21.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▋ | 19/22 [04:48<00:19,  6.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [05:03<00:00, 13.81s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:32:02] Self-play completed in 303.94s, generated 5357 examples (17.6 games/s)\n","[2025-04-22 17:32:02] Processing 4 games for initial placement data\n","[2025-04-22 17:32:02] Extracted 773 initial placement examples\n","[2025-04-22 17:32:02] Skipping placement network training for iteration 289 (training every 5 iterations)\n","[2025-04-22 17:32:02] Training network...\n","Epoch 1/10: Loss 1.8827 (Value 0.0446, Policy 1.8381)\n","Epoch 2/10: Loss 1.8350 (Value 0.0334, Policy 1.8016)\n","Epoch 3/10: Loss 1.8565 (Value 0.0258, Policy 1.8307)\n","Epoch 4/10: Loss 1.8007 (Value 0.0183, Policy 1.7824)\n","Epoch 5/10: Loss 1.8102 (Value 0.0162, Policy 1.7940)\n","Epoch 6/10: Loss 1.8161 (Value 0.0123, Policy 1.8038)\n","Epoch 7/10: Loss 1.7898 (Value 0.0103, Policy 1.7795)\n","Epoch 8/10: Loss 1.7762 (Value 0.0091, Policy 1.7671)\n","Epoch 9/10: Loss 1.7836 (Value 0.0079, Policy 1.7757)\n","Epoch 10/10: Loss 1.7757 (Value 0.0064, Policy 1.7693)\n","[2025-04-22 17:32:21] Training completed in 18.79s\n","[2025-04-22 17:32:21] Iteration 289 done in 322.94s\n","[2025-04-22 17:32:21] Resource usage: CPU 84.5%, RAM 12.1%, GPU peak memory 0.00 GB\n","[2025-04-22 17:32:21] Network parameter sum after training: 22797.812849\n","[2025-04-22 17:32:21] \n","=== Iteration 290/300 ===\n","[2025-04-22 17:32:21] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22797.813437\n","Win reward: base=1.00, time_bonus=0.29 (moves: 85/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  18%|█▊        | 4/22 [02:02<06:48, 22.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  23%|██▎       | 5/22 [02:11<04:58, 17.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  32%|███▏      | 7/22 [02:29<03:23, 13.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [03:57<02:42, 18.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▋ | 19/22 [04:37<00:19,  6.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [05:08<00:00, 14.03s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:37:29] Self-play completed in 308.65s, generated 5226 examples (16.9 games/s)\n","[2025-04-22 17:37:29] Processing 4 games for initial placement data\n","[2025-04-22 17:37:30] Extracted 768 initial placement examples\n","[2025-04-22 17:37:30] Training initial placement network on 1468 examples\n","Epoch 1/10: Loss = 1.3614, Accuracy = 0.0604\n","Epoch 2/10: Loss = 1.3203, Accuracy = 0.0764\n","Epoch 3/10: Loss = 1.2925, Accuracy = 0.0757\n","Epoch 4/10: Loss = 1.2734, Accuracy = 0.0819\n","Epoch 5/10: Loss = 1.2617, Accuracy = 0.0882\n","Epoch 6/10: Loss = 1.2550, Accuracy = 0.0938\n","Epoch 7/10: Loss = 1.2467, Accuracy = 0.0951\n","Epoch 8/10: Loss = 1.2456, Accuracy = 0.0944\n","Epoch 9/10: Loss = 1.2427, Accuracy = 0.0944\n","Epoch 10/10: Loss = 1.2363, Accuracy = 0.0972\n","[2025-04-22 17:37:31] Placement network training completed in 1.77s\n","[2025-04-22 17:37:31] Metrics: Loss = 1.2736, Accuracy = 0.0858\n","[2025-04-22 17:37:31] Saved placement network to models/placement_network.pt\n","[2025-04-22 17:37:31] Training network...\n","Epoch 1/10: Loss 1.8873 (Value 0.0435, Policy 1.8438)\n","Epoch 2/10: Loss 1.8642 (Value 0.0314, Policy 1.8328)\n","Epoch 3/10: Loss 1.8431 (Value 0.0229, Policy 1.8202)\n","Epoch 4/10: Loss 1.8149 (Value 0.0164, Policy 1.7985)\n","Epoch 5/10: Loss 1.8156 (Value 0.0169, Policy 1.7986)\n","Epoch 6/10: Loss 1.8300 (Value 0.0156, Policy 1.8143)\n","Epoch 7/10: Loss 1.7730 (Value 0.0147, Policy 1.7582)\n","Epoch 8/10: Loss 1.7669 (Value 0.0133, Policy 1.7536)\n","Epoch 9/10: Loss 1.7705 (Value 0.0091, Policy 1.7614)\n","Epoch 10/10: Loss 1.7901 (Value 0.0084, Policy 1.7817)\n","[2025-04-22 17:37:51] Training completed in 19.03s\n","[2025-04-22 17:37:51] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:16<02:28, 16.46s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:38:07] Game 1: duration=16.46s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:26<01:40, 12.62s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:38:17] Game 2: duration=9.93s, moves=200, our_VP=2, winner=1 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:43<01:44, 14.88s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:38:34] Game 3: duration=17.56s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:59<01:30, 15.02s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:38:50] Game 4: duration=15.25s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:16<01:19, 15.90s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:39:07] Game 5: duration=17.46s, moves=178, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:32<01:03, 15.85s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:39:23] Game 6: duration=15.76s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:44<00:44, 14.76s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:39:35] Game 7: duration=12.51s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [02:05<00:33, 16.60s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:39:56] Game 8: duration=20.55s, moves=193, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:20<00:16, 16.05s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:40:11] Game 9: duration=14.82s, moves=200, our_VP=3, winner=1 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:36<00:00, 15.69s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:40:27] Game 10: duration=16.61s, moves=200, our_VP=4, winner=3 VP=8\n","[2025-04-22 17:40:27] Evaluated 10 games in 156.91s (0.06 games/s)\n","[2025-04-22 17:40:27] Evaluation results: win_rate=0.70, avg_vp=6.20, avg_length=197.10, total_moves=1971\n","[2025-04-22 17:40:27] Eval resource usage: CPU 58.8%, RAM 12.2%, GPU peak memory 0.00 GB\n","[2025-04-22 17:40:27] Evaluation completed in 156.91s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:40:28] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 17:40:28] Checkpoint saved: models/model_iter_290.pt\n","[2025-04-22 17:40:28] Saved placement network to models/placement_network.pt\n","[2025-04-22 17:40:29] Replay buffer saved: models/latest_buffer.pkl (50000 examples, 201.7 MB)\n","[2025-04-22 17:40:29] Iteration 290 done in 488.62s\n","[2025-04-22 17:40:29] Resource usage: CPU 9.4%, RAM 12.2%, GPU peak memory 0.00 GB\n","[2025-04-22 17:40:29] Network parameter sum after training: 22895.792985\n","[2025-04-22 17:40:29] \n","=== Iteration 291/300 ===\n","[2025-04-22 17:40:29] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22895.794266\n","Win reward: base=1.00, time_bonus=0.34 (moves: 65/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:   9%|▉         | 2/22 [02:15<22:59, 68.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▎        | 3/22 [02:15<11:53, 37.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  23%|██▎       | 5/22 [02:20<04:34, 16.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  41%|████      | 9/22 [02:27<01:02,  4.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [02:39<00:58,  5.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  68%|██████▊   | 15/22 [04:30<02:10, 18.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.04 (moves: 185/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:49<00:00, 13.16s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:45:19] Self-play completed in 289.43s, generated 5102 examples (17.6 games/s)\n","[2025-04-22 17:45:19] Processing 4 games for initial placement data\n","[2025-04-22 17:45:19] Extracted 830 initial placement examples\n","[2025-04-22 17:45:19] Skipping placement network training for iteration 291 (training every 5 iterations)\n","[2025-04-22 17:45:19] Training network...\n","Epoch 1/10: Loss 1.8867 (Value 0.0401, Policy 1.8465)\n","Epoch 2/10: Loss 1.8743 (Value 0.0261, Policy 1.8482)\n","Epoch 3/10: Loss 1.8588 (Value 0.0203, Policy 1.8384)\n","Epoch 4/10: Loss 1.8112 (Value 0.0133, Policy 1.7979)\n","Epoch 5/10: Loss 1.8073 (Value 0.0109, Policy 1.7964)\n","Epoch 6/10: Loss 1.8009 (Value 0.0112, Policy 1.7897)\n","Epoch 7/10: Loss 1.8004 (Value 0.0085, Policy 1.7919)\n","Epoch 8/10: Loss 1.7769 (Value 0.0087, Policy 1.7682)\n","Epoch 9/10: Loss 1.7638 (Value 0.0063, Policy 1.7575)\n","Epoch 10/10: Loss 1.7655 (Value 0.0069, Policy 1.7586)\n","[2025-04-22 17:45:38] Training completed in 19.06s\n","[2025-04-22 17:45:38] Iteration 291 done in 308.72s\n","[2025-04-22 17:45:38] Resource usage: CPU 85.4%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 17:45:38] Network parameter sum after training: 22996.531613\n","[2025-04-22 17:45:38] \n","=== Iteration 292/300 ===\n","[2025-04-22 17:45:38] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22996.534096\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:12<25:21, 72.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [02:18<13:47, 43.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  23%|██▎       | 5/22 [02:21<05:05, 17.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  32%|███▏      | 7/22 [02:26<02:20,  9.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  55%|█████▍    | 12/22 [03:34<03:11, 19.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  59%|█████▉    | 13/22 [03:40<02:16, 15.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.24 (moves: 105/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  68%|██████▊   | 15/22 [03:56<01:18, 11.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  73%|███████▎  | 16/22 [04:10<01:11, 11.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:44<00:00, 12.92s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:50:22] Self-play completed in 284.17s, generated 4917 examples (17.3 games/s)\n","[2025-04-22 17:50:22] Processing 4 games for initial placement data\n","[2025-04-22 17:50:23] Extracted 693 initial placement examples\n","[2025-04-22 17:50:23] Skipping placement network training for iteration 292 (training every 5 iterations)\n","[2025-04-22 17:50:23] Training network...\n","Epoch 1/10: Loss 1.8802 (Value 0.0306, Policy 1.8496)\n","Epoch 2/10: Loss 1.8602 (Value 0.0194, Policy 1.8409)\n","Epoch 3/10: Loss 1.8308 (Value 0.0181, Policy 1.8127)\n","Epoch 4/10: Loss 1.8151 (Value 0.0158, Policy 1.7993)\n","Epoch 5/10: Loss 1.8170 (Value 0.0156, Policy 1.8014)\n","Epoch 6/10: Loss 1.8104 (Value 0.0112, Policy 1.7992)\n","Epoch 7/10: Loss 1.7830 (Value 0.0082, Policy 1.7748)\n","Epoch 8/10: Loss 1.7804 (Value 0.0078, Policy 1.7726)\n","Epoch 9/10: Loss 1.7726 (Value 0.0085, Policy 1.7641)\n","Epoch 10/10: Loss 1.7667 (Value 0.0076, Policy 1.7591)\n","[2025-04-22 17:50:42] Training completed in 18.94s\n","[2025-04-22 17:50:42] Iteration 292 done in 303.30s\n","[2025-04-22 17:50:42] Resource usage: CPU 82.0%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 17:50:42] Network parameter sum after training: 22957.970041\n","[2025-04-22 17:50:42] \n","=== Iteration 293/300 ===\n","[2025-04-22 17:50:42] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22957.969260\n","Win reward: base=1.00, time_bonus=0.32 (moves: 73/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:06<23:13, 66.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   9%|▉         | 2/22 [01:12<10:13, 30.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  18%|█▊        | 4/22 [02:07<07:58, 26.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  23%|██▎       | 5/22 [02:09<04:59, 17.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▋      | 8/22 [02:34<02:49, 12.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [02:53<01:34,  8.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.31 (moves: 77/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [03:10<01:20,  8.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.38 (moves: 49/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  68%|██████▊   | 15/22 [03:52<01:53, 16.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  82%|████████▏ | 18/22 [04:18<00:43, 10.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:39<00:00, 12.69s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 17:55:21] Self-play completed in 279.23s, generated 4848 examples (17.4 games/s)\n","[2025-04-22 17:55:21] Processing 4 games for initial placement data\n","[2025-04-22 17:55:21] Extracted 597 initial placement examples\n","[2025-04-22 17:55:21] Skipping placement network training for iteration 293 (training every 5 iterations)\n","[2025-04-22 17:55:21] Training network...\n","Epoch 1/10: Loss 1.9058 (Value 0.0332, Policy 1.8726)\n","Epoch 2/10: Loss 1.8565 (Value 0.0279, Policy 1.8286)\n","Epoch 3/10: Loss 1.8090 (Value 0.0174, Policy 1.7916)\n","Epoch 4/10: Loss 1.8134 (Value 0.0128, Policy 1.8005)\n","Epoch 5/10: Loss 1.7927 (Value 0.0115, Policy 1.7812)\n","Epoch 6/10: Loss 1.7907 (Value 0.0086, Policy 1.7822)\n","Epoch 7/10: Loss 1.7883 (Value 0.0072, Policy 1.7811)\n","Epoch 8/10: Loss 1.7660 (Value 0.0076, Policy 1.7584)\n","Epoch 9/10: Loss 1.7625 (Value 0.0061, Policy 1.7563)\n","Epoch 10/10: Loss 1.7677 (Value 0.0058, Policy 1.7619)\n","[2025-04-22 17:55:40] Training completed in 19.16s\n","[2025-04-22 17:55:40] Iteration 293 done in 298.56s\n","[2025-04-22 17:55:40] Resource usage: CPU 79.7%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 17:55:40] Network parameter sum after training: 22993.238684\n","[2025-04-22 17:55:40] \n","=== Iteration 294/300 ===\n","[2025-04-22 17:55:40] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 22993.238661\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  23%|██▎       | 5/22 [02:16<04:24, 15.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  32%|███▏      | 7/22 [02:31<02:58, 11.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [02:42<00:51,  4.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.24 (moves: 105/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [03:37<02:16, 15.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.26 (moves: 97/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  95%|█████████▌| 21/22 [04:33<00:04,  4.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [05:04<00:00, 13.83s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:00:44] Self-play completed in 304.28s, generated 5026 examples (16.5 games/s)\n","[2025-04-22 18:00:44] Processing 4 games for initial placement data\n","[2025-04-22 18:00:45] Extracted 776 initial placement examples\n","[2025-04-22 18:00:45] Skipping placement network training for iteration 294 (training every 5 iterations)\n","[2025-04-22 18:00:45] Training network...\n","Epoch 1/10: Loss 1.8553 (Value 0.0388, Policy 1.8164)\n","Epoch 2/10: Loss 1.8704 (Value 0.0259, Policy 1.8445)\n","Epoch 3/10: Loss 1.8200 (Value 0.0213, Policy 1.7988)\n","Epoch 4/10: Loss 1.8178 (Value 0.0178, Policy 1.8000)\n","Epoch 5/10: Loss 1.7943 (Value 0.0133, Policy 1.7810)\n","Epoch 6/10: Loss 1.8126 (Value 0.0113, Policy 1.8013)\n","Epoch 7/10: Loss 1.7894 (Value 0.0110, Policy 1.7785)\n","Epoch 8/10: Loss 1.7655 (Value 0.0101, Policy 1.7554)\n","Epoch 9/10: Loss 1.7632 (Value 0.0094, Policy 1.7538)\n","Epoch 10/10: Loss 1.7707 (Value 0.0098, Policy 1.7609)\n","[2025-04-22 18:01:04] Training completed in 18.91s\n","[2025-04-22 18:01:04] Iteration 294 done in 323.40s\n","[2025-04-22 18:01:04] Resource usage: CPU 78.1%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:01:04] Network parameter sum after training: 23095.257092\n","[2025-04-22 18:01:04] \n","=== Iteration 295/300 ===\n","[2025-04-22 18:01:04] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23095.258406\n","Win reward: base=1.00, time_bonus=0.33 (moves: 69/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [00:51<18:08, 51.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   9%|▉         | 2/22 [02:08<22:03, 66.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  18%|█▊        | 4/22 [02:11<06:52, 22.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  32%|███▏      | 7/22 [02:22<02:16,  9.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  41%|████      | 9/22 [02:37<01:50,  8.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  55%|█████▍    | 12/22 [02:54<01:07,  6.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  68%|██████▊   | 15/22 [04:18<02:05, 17.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  82%|████████▏ | 18/22 [04:28<00:31,  7.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:54<00:00, 13.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:05:58] Self-play completed in 294.44s, generated 5079 examples (17.2 games/s)\n","[2025-04-22 18:05:58] Processing 4 games for initial placement data\n","[2025-04-22 18:05:58] Extracted 718 initial placement examples\n","[2025-04-22 18:05:58] Training initial placement network on 2186 examples\n","Epoch 1/10: Loss = 1.2671, Accuracy = 0.0749\n","Epoch 2/10: Loss = 1.2416, Accuracy = 0.0882\n","Epoch 3/10: Loss = 1.2316, Accuracy = 0.0896\n","Epoch 4/10: Loss = 1.2250, Accuracy = 0.0864\n","Epoch 5/10: Loss = 1.2178, Accuracy = 0.0960\n","Epoch 6/10: Loss = 1.2127, Accuracy = 0.0933\n","Epoch 7/10: Loss = 1.2061, Accuracy = 0.0933\n","Epoch 8/10: Loss = 1.2066, Accuracy = 0.0910\n","Epoch 9/10: Loss = 1.2025, Accuracy = 0.0970\n","Epoch 10/10: Loss = 1.2018, Accuracy = 0.0983\n","[2025-04-22 18:06:01] Placement network training completed in 2.77s\n","[2025-04-22 18:06:01] Metrics: Loss = 1.2213, Accuracy = 0.0908\n","[2025-04-22 18:06:01] Saved placement network to models/placement_network.pt\n","[2025-04-22 18:06:01] Training network...\n","Epoch 1/10: Loss 1.9200 (Value 0.0466, Policy 1.8733)\n","Epoch 2/10: Loss 1.8418 (Value 0.0330, Policy 1.8087)\n","Epoch 3/10: Loss 1.8553 (Value 0.0286, Policy 1.8267)\n","Epoch 4/10: Loss 1.8312 (Value 0.0272, Policy 1.8040)\n","Epoch 5/10: Loss 1.8368 (Value 0.0260, Policy 1.8108)\n","Epoch 6/10: Loss 1.8032 (Value 0.0184, Policy 1.7848)\n","Epoch 7/10: Loss 1.7947 (Value 0.0182, Policy 1.7766)\n","Epoch 8/10: Loss 1.7792 (Value 0.0145, Policy 1.7647)\n","Epoch 9/10: Loss 1.7673 (Value 0.0156, Policy 1.7516)\n","Epoch 10/10: Loss 1.7735 (Value 0.0112, Policy 1.7623)\n","[2025-04-22 18:06:20] Training completed in 18.94s\n","[2025-04-22 18:06:20] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:18<02:44, 18.27s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:06:38] Game 1: duration=18.27s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:33<02:13, 16.63s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:06:54] Game 2: duration=15.49s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:44<01:36, 13.79s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:07:04] Game 3: duration=10.39s, moves=200, our_VP=2, winner=1 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:57<01:21, 13.65s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:07:18] Game 4: duration=13.45s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:14<01:14, 14.81s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:07:34] Game 5: duration=16.87s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:27<00:56, 14.09s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:07:47] Game 6: duration=12.69s, moves=143, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:39<00:40, 13.67s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:08:00] Game 7: duration=12.79s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:52<00:26, 13.39s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:08:13] Game 8: duration=12.81s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:08<00:14, 14.01s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:08:28] Game 9: duration=15.37s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:20<00:00, 14.01s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:08:40] Game 10: duration=11.94s, moves=200, our_VP=2, winner=1 VP=6\n","[2025-04-22 18:08:40] Evaluated 10 games in 140.09s (0.07 games/s)\n","[2025-04-22 18:08:40] Evaluation results: win_rate=0.80, avg_vp=5.60, avg_length=194.30, total_moves=1943\n","[2025-04-22 18:08:40] Eval resource usage: CPU 60.5%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:08:40] Evaluation completed in 140.09s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:08:40] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 18:08:40] New best model at iteration 295 (win_rate=0.80)\n","[2025-04-22 18:08:41] Checkpoint saved: models/model_iter_295.pt\n","[2025-04-22 18:08:41] Best model saved: models/best_model.pt\n","[2025-04-22 18:08:41] Saved placement network to models/placement_network.pt\n","[2025-04-22 18:08:41] Copied best placement network to models/placement_network_iter_295.pt\n","[2025-04-22 18:08:42] Replay buffer saved: models/latest_buffer.pkl (50000 examples, 201.7 MB)\n","[2025-04-22 18:08:42] Iteration 295 done in 458.52s\n","[2025-04-22 18:08:42] Resource usage: CPU 9.2%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:08:42] Network parameter sum after training: 23162.527837\n","[2025-04-22 18:08:42] \n","=== Iteration 296/300 ===\n","[2025-04-22 18:08:42] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23162.527001\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [02:01<42:36, 121.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  18%|█▊        | 4/22 [02:17<06:33, 21.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  27%|██▋       | 6/22 [02:26<03:12, 12.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▋      | 8/22 [02:35<01:55,  8.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  41%|████      | 9/22 [02:36<01:16,  5.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  45%|████▌     | 10/22 [02:43<01:15,  6.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [04:12<03:29, 23.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.10 (moves: 161/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  77%|███████▋  | 17/22 [04:32<00:49,  9.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:48<00:00, 13.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:13:31] Self-play completed in 288.97s, generated 5128 examples (17.7 games/s)\n","[2025-04-22 18:13:31] Processing 4 games for initial placement data\n","[2025-04-22 18:13:31] Extracted 765 initial placement examples\n","[2025-04-22 18:13:31] Skipping placement network training for iteration 296 (training every 5 iterations)\n","[2025-04-22 18:13:31] Training network...\n","Epoch 1/10: Loss 1.8986 (Value 0.0409, Policy 1.8577)\n","Epoch 2/10: Loss 1.8545 (Value 0.0363, Policy 1.8182)\n","Epoch 3/10: Loss 1.8440 (Value 0.0283, Policy 1.8157)\n","Epoch 4/10: Loss 1.8187 (Value 0.0205, Policy 1.7981)\n","Epoch 5/10: Loss 1.8434 (Value 0.0168, Policy 1.8266)\n","Epoch 6/10: Loss 1.8069 (Value 0.0139, Policy 1.7930)\n","Epoch 7/10: Loss 1.7935 (Value 0.0127, Policy 1.7808)\n","Epoch 8/10: Loss 1.7891 (Value 0.0126, Policy 1.7764)\n","Epoch 9/10: Loss 1.7782 (Value 0.0107, Policy 1.7675)\n","Epoch 10/10: Loss 1.7686 (Value 0.0097, Policy 1.7589)\n","[2025-04-22 18:13:50] Training completed in 18.59s\n","[2025-04-22 18:13:50] Iteration 296 done in 307.77s\n","[2025-04-22 18:13:50] Resource usage: CPU 85.9%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:13:50] Network parameter sum after training: 23193.365117\n","[2025-04-22 18:13:50] \n","=== Iteration 297/300 ===\n","[2025-04-22 18:13:50] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23193.364165\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [02:06<44:15, 126.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   9%|▉         | 2/22 [02:19<19:49, 59.48s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▎        | 3/22 [02:21<10:33, 33.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▋      | 8/22 [02:35<01:21,  5.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  45%|████▌     | 10/22 [02:41<00:48,  4.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 11/22 [02:46<00:50,  4.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [04:07<02:58, 19.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.12 (moves: 153/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  77%|███████▋  | 17/22 [04:46<00:54, 10.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  82%|████████▏ | 18/22 [04:49<00:35,  8.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [05:15<00:00, 14.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:19:06] Self-play completed in 315.85s, generated 5460 examples (17.3 games/s)\n","[2025-04-22 18:19:06] Processing 4 games for initial placement data\n","[2025-04-22 18:19:06] Extracted 724 initial placement examples\n","[2025-04-22 18:19:06] Skipping placement network training for iteration 297 (training every 5 iterations)\n","[2025-04-22 18:19:06] Training network...\n","Epoch 1/10: Loss 1.8900 (Value 0.0365, Policy 1.8536)\n","Epoch 2/10: Loss 1.8638 (Value 0.0251, Policy 1.8387)\n","Epoch 3/10: Loss 1.8529 (Value 0.0211, Policy 1.8317)\n","Epoch 4/10: Loss 1.8357 (Value 0.0163, Policy 1.8195)\n","Epoch 5/10: Loss 1.8269 (Value 0.0125, Policy 1.8144)\n","Epoch 6/10: Loss 1.7899 (Value 0.0108, Policy 1.7791)\n","Epoch 7/10: Loss 1.7764 (Value 0.0112, Policy 1.7652)\n","Epoch 8/10: Loss 1.7694 (Value 0.0093, Policy 1.7602)\n","Epoch 9/10: Loss 1.7780 (Value 0.0078, Policy 1.7702)\n","Epoch 10/10: Loss 1.7610 (Value 0.0090, Policy 1.7520)\n","[2025-04-22 18:19:25] Training completed in 18.61s\n","[2025-04-22 18:19:25] Iteration 297 done in 334.66s\n","[2025-04-22 18:19:25] Resource usage: CPU 83.4%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:19:25] Network parameter sum after training: 23356.676966\n","[2025-04-22 18:19:25] \n","=== Iteration 298/300 ===\n","[2025-04-22 18:19:25] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23356.677812\n","Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  27%|██▋       | 6/22 [02:26<03:07, 11.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [02:44<00:48,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.27 (moves: 93/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [03:42<02:01, 13.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▎   | 14/22 [04:17<02:35, 19.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  77%|███████▋  | 17/22 [04:37<00:55, 11.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.12 (moves: 153/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:51<00:00, 13.23s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:24:16] Self-play completed in 291.12s, generated 5223 examples (17.9 games/s)\n","[2025-04-22 18:24:16] Processing 4 games for initial placement data\n","[2025-04-22 18:24:16] Extracted 523 initial placement examples\n","[2025-04-22 18:24:16] Skipping placement network training for iteration 298 (training every 5 iterations)\n","[2025-04-22 18:24:16] Training network...\n","Epoch 1/10: Loss 1.8922 (Value 0.0395, Policy 1.8526)\n","Epoch 2/10: Loss 1.8737 (Value 0.0304, Policy 1.8433)\n","Epoch 3/10: Loss 1.8740 (Value 0.0206, Policy 1.8534)\n","Epoch 4/10: Loss 1.8272 (Value 0.0188, Policy 1.8084)\n","Epoch 5/10: Loss 1.8124 (Value 0.0178, Policy 1.7946)\n","Epoch 6/10: Loss 1.7867 (Value 0.0168, Policy 1.7699)\n","Epoch 7/10: Loss 1.8037 (Value 0.0140, Policy 1.7896)\n","Epoch 8/10: Loss 1.7853 (Value 0.0110, Policy 1.7743)\n","Epoch 9/10: Loss 1.7788 (Value 0.0112, Policy 1.7676)\n","Epoch 10/10: Loss 1.7701 (Value 0.0112, Policy 1.7589)\n","[2025-04-22 18:24:35] Training completed in 19.22s\n","[2025-04-22 18:24:35] Iteration 298 done in 310.49s\n","[2025-04-22 18:24:35] Resource usage: CPU 85.3%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:24:35] Network parameter sum after training: 23393.088660\n","[2025-04-22 18:24:35] \n","=== Iteration 299/300 ===\n","[2025-04-22 18:24:35] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23393.091018\n","Win reward: base=1.00, time_bonus=0.25 (moves: 101/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:19<27:41, 79.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   9%|▉         | 2/22 [02:00<18:56, 56.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▎        | 3/22 [02:11<11:24, 36.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [03:12<02:17, 12.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.16 (moves: 137/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  68%|██████▊   | 15/22 [04:41<02:11, 18.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [05:08<00:00, 14.01s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:29:44] Self-play completed in 308.32s, generated 5436 examples (17.6 games/s)\n","[2025-04-22 18:29:44] Processing 4 games for initial placement data\n","[2025-04-22 18:29:44] Extracted 653 initial placement examples\n","[2025-04-22 18:29:44] Skipping placement network training for iteration 299 (training every 5 iterations)\n","[2025-04-22 18:29:44] Training network...\n","Epoch 1/10: Loss 1.8782 (Value 0.0362, Policy 1.8420)\n","Epoch 2/10: Loss 1.8528 (Value 0.0256, Policy 1.8272)\n","Epoch 3/10: Loss 1.8591 (Value 0.0214, Policy 1.8377)\n","Epoch 4/10: Loss 1.8116 (Value 0.0173, Policy 1.7944)\n","Epoch 5/10: Loss 1.8268 (Value 0.0154, Policy 1.8114)\n","Epoch 6/10: Loss 1.8200 (Value 0.0140, Policy 1.8060)\n","Epoch 7/10: Loss 1.7791 (Value 0.0130, Policy 1.7661)\n","Epoch 8/10: Loss 1.7768 (Value 0.0127, Policy 1.7641)\n","Epoch 9/10: Loss 1.7812 (Value 0.0118, Policy 1.7694)\n","Epoch 10/10: Loss 1.7552 (Value 0.0088, Policy 1.7464)\n","[2025-04-22 18:30:02] Training completed in 18.59s\n","[2025-04-22 18:30:02] Iteration 299 done in 327.09s\n","[2025-04-22 18:30:02] Resource usage: CPU 84.5%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:30:02] Network parameter sum after training: 23397.157966\n","[2025-04-22 18:30:02] \n","=== Iteration 300/300 ===\n","[2025-04-22 18:30:02] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23397.159261\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:42<35:45, 102.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.09 (moves: 165/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [02:09<11:02, 34.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.06 (moves: 177/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  23%|██▎       | 5/22 [02:13<04:12, 14.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [02:59<01:36,  8.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [04:06<02:52, 19.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.01 (moves: 197/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▎   | 14/22 [04:17<02:12, 16.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.13 (moves: 149/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  68%|██████▊   | 15/22 [04:24<01:35, 13.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  73%|███████▎  | 16/22 [04:28<01:03, 10.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.20 (moves: 121/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▋ | 19/22 [04:42<00:22,  7.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:48<00:00, 13.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:34:51] Self-play completed in 288.32s, generated 5162 examples (17.9 games/s)\n","[2025-04-22 18:34:51] Processing 4 games for initial placement data\n","[2025-04-22 18:34:51] Extracted 697 initial placement examples\n","[2025-04-22 18:34:51] Training initial placement network on 2883 examples\n","Epoch 1/10: Loss = 1.3036, Accuracy = 0.0806\n","Epoch 2/10: Loss = 1.2893, Accuracy = 0.0833\n","Epoch 3/10: Loss = 1.2783, Accuracy = 0.0840\n","Epoch 4/10: Loss = 1.2729, Accuracy = 0.0875\n","Epoch 5/10: Loss = 1.2677, Accuracy = 0.0875\n","Epoch 6/10: Loss = 1.2615, Accuracy = 0.0858\n","Epoch 7/10: Loss = 1.2606, Accuracy = 0.0847\n","Epoch 8/10: Loss = 1.2589, Accuracy = 0.0840\n","Epoch 9/10: Loss = 1.2561, Accuracy = 0.0875\n","Epoch 10/10: Loss = 1.2547, Accuracy = 0.0892\n","[2025-04-22 18:34:55] Placement network training completed in 3.99s\n","[2025-04-22 18:34:55] Metrics: Loss = 1.2704, Accuracy = 0.0854\n","[2025-04-22 18:34:55] Saved placement network to models/placement_network.pt\n","[2025-04-22 18:34:55] Training network...\n","Epoch 1/10: Loss 1.8687 (Value 0.0393, Policy 1.8294)\n","Epoch 2/10: Loss 1.8565 (Value 0.0282, Policy 1.8283)\n","Epoch 3/10: Loss 1.8282 (Value 0.0211, Policy 1.8071)\n","Epoch 4/10: Loss 1.8128 (Value 0.0204, Policy 1.7923)\n","Epoch 5/10: Loss 1.7792 (Value 0.0159, Policy 1.7633)\n","Epoch 6/10: Loss 1.7958 (Value 0.0125, Policy 1.7834)\n","Epoch 7/10: Loss 1.7925 (Value 0.0122, Policy 1.7803)\n","Epoch 8/10: Loss 1.7779 (Value 0.0093, Policy 1.7686)\n","Epoch 9/10: Loss 1.7297 (Value 0.0090, Policy 1.7207)\n","Epoch 10/10: Loss 1.7842 (Value 0.0085, Policy 1.7757)\n","[2025-04-22 18:35:14] Training completed in 18.89s\n","[2025-04-22 18:35:14] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:15<02:22, 15.80s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:35:30] Game 1: duration=15.80s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:30<02:01, 15.22s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:35:44] Game 2: duration=14.81s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:45<01:46, 15.28s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:36:00] Game 3: duration=15.34s, moves=200, our_VP=8, winner=0 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:58<01:24, 14.03s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:36:12] Game 4: duration=12.12s, moves=200, our_VP=6, winner=2 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:14<01:13, 14.73s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:36:28] Game 5: duration=15.96s, moves=200, our_VP=2, winner=1 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:26<00:56, 14.03s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:36:40] Game 6: duration=12.67s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:39<00:40, 13.56s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:36:53] Game 7: duration=12.61s, moves=200, our_VP=5, winner=0 VP=5\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:51<00:26, 13.19s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:37:05] Game 8: duration=12.39s, moves=200, our_VP=2, winner=1 VP=8\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:07<00:13, 13.88s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:37:21] Game 9: duration=15.39s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:21<00:00, 14.11s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:37:35] Game 10: duration=13.98s, moves=200, our_VP=5, winner=2 VP=6\n","[2025-04-22 18:37:35] Evaluated 10 games in 141.09s (0.07 games/s)\n","[2025-04-22 18:37:35] Evaluation results: win_rate=0.60, avg_vp=5.20, avg_length=200.00, total_moves=2000\n","[2025-04-22 18:37:35] Eval resource usage: CPU 63.4%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:37:35] Evaluation completed in 141.09s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:37:35] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 18:37:35] Checkpoint saved: models/model_iter_300.pt\n","[2025-04-22 18:37:35] Saved placement network to models/placement_network.pt\n","[2025-04-22 18:37:37] Replay buffer saved: models/latest_buffer.pkl (50000 examples, 201.6 MB)\n","[2025-04-22 18:37:37] Iteration 300 done in 454.45s\n","[2025-04-22 18:37:37] Resource usage: CPU 9.3%, RAM 12.4%, GPU peak memory 0.00 GB\n","[2025-04-22 18:37:37] Network parameter sum after training: 23373.752742\n","[2025-04-22 18:37:37] \n","=== Iteration 301/300 ===\n","[2025-04-22 18:37:37] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23373.750669\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:   9%|▉         | 2/22 [02:19<20:02, 60.14s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▎        | 3/22 [02:22<10:47, 34.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  23%|██▎       | 5/22 [02:28<04:18, 15.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  27%|██▋       | 6/22 [02:34<03:09, 11.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.02 (moves: 193/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  36%|███▋      | 8/22 [02:38<01:31,  6.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  41%|████      | 9/22 [02:41<01:09,  5.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.04 (moves: 185/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  45%|████▌     | 10/22 [02:44<00:54,  4.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 11/22 [02:44<00:35,  3.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.30 (moves: 81/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  59%|█████▉    | 13/22 [03:51<02:24, 16.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  73%|███████▎  | 16/22 [04:33<01:15, 12.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  86%|████████▋ | 19/22 [04:46<00:21,  7.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  95%|█████████▌| 21/22 [04:50<00:04,  4.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:54<00:00, 13.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:42:32] Self-play completed in 294.81s, generated 5220 examples (17.7 games/s)\n","[2025-04-22 18:42:32] Processing 4 games for initial placement data\n","[2025-04-22 18:42:32] Extracted 620 initial placement examples\n","[2025-04-22 18:42:32] Skipping placement network training for iteration 301 (training every 5 iterations)\n","[2025-04-22 18:42:32] Training network...\n","Epoch 1/10: Loss 1.8669 (Value 0.0344, Policy 1.8324)\n","Epoch 2/10: Loss 1.8446 (Value 0.0235, Policy 1.8211)\n","Epoch 3/10: Loss 1.8287 (Value 0.0187, Policy 1.8099)\n","Epoch 4/10: Loss 1.8099 (Value 0.0139, Policy 1.7961)\n","Epoch 5/10: Loss 1.7970 (Value 0.0103, Policy 1.7867)\n","Epoch 6/10: Loss 1.8006 (Value 0.0094, Policy 1.7912)\n","Epoch 7/10: Loss 1.7866 (Value 0.0099, Policy 1.7768)\n","Epoch 8/10: Loss 1.7815 (Value 0.0078, Policy 1.7737)\n","Epoch 9/10: Loss 1.7587 (Value 0.0076, Policy 1.7511)\n","Epoch 10/10: Loss 1.7640 (Value 0.0068, Policy 1.7572)\n","[2025-04-22 18:42:50] Training completed in 18.44s\n","[2025-04-22 18:42:50] Iteration 301 done in 313.43s\n","[2025-04-22 18:42:50] Resource usage: CPU 85.8%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:42:50] Network parameter sum after training: 23482.166954\n","[2025-04-22 18:42:50] \n","=== Iteration 302/300 ===\n","[2025-04-22 18:42:50] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23482.166683\n","Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  50%|█████     | 11/22 [03:01<01:21,  7.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.24 (moves: 105/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  55%|█████▍    | 12/22 [03:51<03:21, 20.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  59%|█████▉    | 13/22 [04:02<02:37, 17.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.14 (moves: 145/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▎   | 14/22 [04:17<02:13, 16.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  73%|███████▎  | 16/22 [04:29<01:10, 11.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.08 (moves: 169/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:49<00:00, 13.17s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:47:40] Self-play completed in 289.81s, generated 5169 examples (17.8 games/s)\n","[2025-04-22 18:47:40] Processing 4 games for initial placement data\n","[2025-04-22 18:47:40] Extracted 913 initial placement examples\n","[2025-04-22 18:47:40] Skipping placement network training for iteration 302 (training every 5 iterations)\n","[2025-04-22 18:47:40] Training network...\n","Epoch 1/10: Loss 1.9000 (Value 0.0422, Policy 1.8578)\n","Epoch 2/10: Loss 1.8768 (Value 0.0332, Policy 1.8436)\n","Epoch 3/10: Loss 1.8447 (Value 0.0240, Policy 1.8207)\n","Epoch 4/10: Loss 1.8059 (Value 0.0174, Policy 1.7884)\n","Epoch 5/10: Loss 1.8331 (Value 0.0189, Policy 1.8142)\n","Epoch 6/10: Loss 1.8040 (Value 0.0133, Policy 1.7907)\n","Epoch 7/10: Loss 1.7790 (Value 0.0110, Policy 1.7680)\n","Epoch 8/10: Loss 1.7654 (Value 0.0104, Policy 1.7550)\n","Epoch 9/10: Loss 1.7735 (Value 0.0090, Policy 1.7645)\n","Epoch 10/10: Loss 1.7769 (Value 0.0073, Policy 1.7695)\n","[2025-04-22 18:47:59] Training completed in 18.73s\n","[2025-04-22 18:47:59] Iteration 302 done in 308.79s\n","[2025-04-22 18:47:59] Resource usage: CPU 85.6%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:47:59] Network parameter sum after training: 23506.844934\n","[2025-04-22 18:47:59] \n","=== Iteration 303/300 ===\n","[2025-04-22 18:47:59] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23506.843164\n","Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:   9%|▉         | 2/22 [01:54<16:01, 48.09s/it] "]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.13 (moves: 149/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  14%|█▎        | 3/22 [02:02<09:26, 29.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  18%|█▊        | 4/22 [02:06<05:57, 19.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.10 (moves: 161/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  45%|████▌     | 10/22 [02:56<02:01, 10.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  55%|█████▍    | 12/22 [03:38<02:55, 17.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  64%|██████▎   | 14/22 [04:13<02:10, 16.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.11 (moves: 157/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  68%|██████▊   | 15/22 [04:14<01:23, 11.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  77%|███████▋  | 17/22 [04:31<00:51, 10.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  82%|████████▏ | 18/22 [04:32<00:30,  7.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.19 (moves: 125/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  95%|█████████▌| 21/22 [04:43<00:05,  5.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:45<00:00, 12.95s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:52:44] Self-play completed in 285.02s, generated 5092 examples (17.9 games/s)\n","[2025-04-22 18:52:44] Processing 4 games for initial placement data\n","[2025-04-22 18:52:44] Extracted 647 initial placement examples\n","[2025-04-22 18:52:44] Skipping placement network training for iteration 303 (training every 5 iterations)\n","[2025-04-22 18:52:44] Training network...\n","Epoch 1/10: Loss 1.8792 (Value 0.0349, Policy 1.8443)\n","Epoch 2/10: Loss 1.8616 (Value 0.0247, Policy 1.8369)\n","Epoch 3/10: Loss 1.8529 (Value 0.0206, Policy 1.8323)\n","Epoch 4/10: Loss 1.8230 (Value 0.0140, Policy 1.8090)\n","Epoch 5/10: Loss 1.8275 (Value 0.0126, Policy 1.8149)\n","Epoch 6/10: Loss 1.7975 (Value 0.0117, Policy 1.7858)\n","Epoch 7/10: Loss 1.8014 (Value 0.0098, Policy 1.7916)\n","Epoch 8/10: Loss 1.7756 (Value 0.0085, Policy 1.7671)\n","Epoch 9/10: Loss 1.7777 (Value 0.0078, Policy 1.7699)\n","Epoch 10/10: Loss 1.7714 (Value 0.0069, Policy 1.7645)\n","[2025-04-22 18:53:03] Training completed in 18.79s\n","[2025-04-22 18:53:03] Iteration 303 done in 304.00s\n","[2025-04-22 18:53:03] Resource usage: CPU 85.6%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:53:03] Network parameter sum after training: 23598.665593\n","[2025-04-22 18:53:03] \n","=== Iteration 304/300 ===\n","[2025-04-22 18:53:03] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23598.665479\n","Win reward: base=1.00, time_bonus=0.29 (moves: 85/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [01:27<30:33, 87.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.07 (moves: 173/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [02:06<10:33, 33.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  18%|█▊        | 4/22 [02:07<06:15, 20.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.09 (moves: 165/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  41%|████      | 9/22 [02:39<01:46,  8.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  45%|████▌     | 10/22 [02:40<01:12,  6.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  55%|█████▍    | 12/22 [04:04<04:44, 28.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.21 (moves: 117/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  59%|█████▉    | 13/22 [04:09<03:12, 21.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:53<00:00, 13.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 18:57:57] Self-play completed in 293.44s, generated 5136 examples (17.5 games/s)\n","[2025-04-22 18:57:57] Processing 4 games for initial placement data\n","[2025-04-22 18:57:57] Extracted 634 initial placement examples\n","[2025-04-22 18:57:57] Skipping placement network training for iteration 304 (training every 5 iterations)\n","[2025-04-22 18:57:57] Training network...\n","Epoch 1/10: Loss 1.8765 (Value 0.0381, Policy 1.8385)\n","Epoch 2/10: Loss 1.8345 (Value 0.0289, Policy 1.8057)\n","Epoch 3/10: Loss 1.8152 (Value 0.0187, Policy 1.7966)\n","Epoch 4/10: Loss 1.8325 (Value 0.0154, Policy 1.8171)\n","Epoch 5/10: Loss 1.8021 (Value 0.0101, Policy 1.7920)\n","Epoch 6/10: Loss 1.7999 (Value 0.0082, Policy 1.7917)\n","Epoch 7/10: Loss 1.7758 (Value 0.0075, Policy 1.7684)\n","Epoch 8/10: Loss 1.7623 (Value 0.0059, Policy 1.7565)\n","Epoch 9/10: Loss 1.7717 (Value 0.0056, Policy 1.7661)\n","Epoch 10/10: Loss 1.7624 (Value 0.0056, Policy 1.7567)\n","[2025-04-22 18:58:15] Training completed in 18.61s\n","[2025-04-22 18:58:15] Iteration 304 done in 312.23s\n","[2025-04-22 18:58:15] Resource usage: CPU 84.4%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 18:58:15] Network parameter sum after training: 23587.956033\n","[2025-04-22 18:58:15] \n","=== Iteration 305/300 ===\n","[2025-04-22 18:58:15] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23587.953780\n","Win reward: base=1.00, time_bonus=0.35 (moves: 61/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   5%|▍         | 1/22 [00:40<14:12, 40.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.05 (moves: 181/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  14%|█▎        | 3/22 [02:09<11:57, 37.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.15 (moves: 141/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  45%|████▌     | 10/22 [02:40<01:11,  5.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  50%|█████     | 11/22 [02:48<01:11,  6.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  55%|█████▍    | 12/22 [03:01<01:22,  8.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.28 (moves: 89/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  59%|█████▉    | 13/22 [03:15<01:32, 10.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.20 (moves: 121/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  64%|██████▎   | 14/22 [03:54<02:31, 18.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.22 (moves: 113/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  68%|██████▊   | 15/22 [03:58<01:39, 14.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.04 (moves: 185/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  73%|███████▎  | 16/22 [04:09<01:20, 13.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:  77%|███████▋  | 17/22 [04:24<01:09, 13.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games:  95%|█████████▌| 21/22 [04:37<00:05,  5.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Win reward: base=1.00, time_bonus=0.00 (moves: 200/200)\n"]},{"output_type":"stream","name":"stderr","text":["Self-play games: 100%|██████████| 22/22 [04:44<00:00, 12.93s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:03:00] Self-play completed in 284.47s, generated 5009 examples (17.6 games/s)\n","[2025-04-22 19:03:00] Processing 4 games for initial placement data\n","[2025-04-22 19:03:00] Extracted 540 initial placement examples\n","[2025-04-22 19:03:00] Training initial placement network on 3423 examples\n","Epoch 1/10: Loss = 1.3130, Accuracy = 0.0834\n","Epoch 2/10: Loss = 1.2991, Accuracy = 0.0834\n","Epoch 3/10: Loss = 1.2924, Accuracy = 0.0876\n","Epoch 4/10: Loss = 1.2879, Accuracy = 0.0881\n","Epoch 5/10: Loss = 1.2864, Accuracy = 0.0876\n","Epoch 6/10: Loss = 1.2846, Accuracy = 0.0858\n","Epoch 7/10: Loss = 1.2792, Accuracy = 0.0873\n","Epoch 8/10: Loss = 1.2792, Accuracy = 0.0855\n","Epoch 9/10: Loss = 1.2760, Accuracy = 0.0873\n","Epoch 10/10: Loss = 1.2725, Accuracy = 0.0884\n","[2025-04-22 19:03:05] Placement network training completed in 4.50s\n","[2025-04-22 19:03:05] Metrics: Loss = 1.2870, Accuracy = 0.0864\n","[2025-04-22 19:03:05] Saved placement network to models/placement_network.pt\n","[2025-04-22 19:03:05] Training network...\n","Epoch 1/10: Loss 1.8737 (Value 0.0262, Policy 1.8475)\n","Epoch 2/10: Loss 1.8502 (Value 0.0200, Policy 1.8303)\n","Epoch 3/10: Loss 1.8191 (Value 0.0126, Policy 1.8065)\n","Epoch 4/10: Loss 1.8098 (Value 0.0109, Policy 1.7989)\n","Epoch 5/10: Loss 1.7913 (Value 0.0097, Policy 1.7816)\n","Epoch 6/10: Loss 1.7864 (Value 0.0080, Policy 1.7784)\n","Epoch 7/10: Loss 1.7893 (Value 0.0076, Policy 1.7817)\n","Epoch 8/10: Loss 1.7682 (Value 0.0060, Policy 1.7622)\n","Epoch 9/10: Loss 1.7488 (Value 0.0053, Policy 1.7435)\n","Epoch 10/10: Loss 1.7765 (Value 0.0049, Policy 1.7716)\n","[2025-04-22 19:03:24] Training completed in 19.05s\n","[2025-04-22 19:03:24] Evaluating network...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games:  10%|█         | 1/10 [00:13<02:01, 13.53s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:03:37] Game 1: duration=13.53s, moves=120, our_VP=11, winner=0 VP=11\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  20%|██        | 2/10 [00:26<01:45, 13.19s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:03:50] Game 2: duration=12.95s, moves=200, our_VP=7, winner=0 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  30%|███       | 3/10 [00:46<01:53, 16.22s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:04:10] Game 3: duration=19.82s, moves=165, our_VP=10, winner=0 VP=10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  40%|████      | 4/10 [00:57<01:24, 14.08s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:04:21] Game 4: duration=10.81s, moves=200, our_VP=3, winner=2 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  50%|█████     | 5/10 [01:08<01:06, 13.24s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:04:32] Game 5: duration=11.75s, moves=200, our_VP=4, winner=1 VP=7\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  60%|██████    | 6/10 [01:25<00:57, 14.37s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:04:49] Game 6: duration=16.54s, moves=200, our_VP=4, winner=0 VP=4\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  70%|███████   | 7/10 [01:37<00:40, 13.58s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:05:01] Game 7: duration=11.96s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  80%|████████  | 8/10 [01:56<00:30, 15.46s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:05:20] Game 8: duration=19.48s, moves=200, our_VP=9, winner=0 VP=9\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation games:  90%|█████████ | 9/10 [02:11<00:15, 15.12s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:05:35] Game 9: duration=14.38s, moves=200, our_VP=6, winner=0 VP=6\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation games: 100%|██████████| 10/10 [02:24<00:00, 14.47s/it]"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:05:48] Game 10: duration=13.49s, moves=200, our_VP=2, winner=2 VP=6\n","[2025-04-22 19:05:48] Evaluated 10 games in 144.73s (0.07 games/s)\n","[2025-04-22 19:05:48] Evaluation results: win_rate=0.70, avg_vp=6.20, avg_length=188.50, total_moves=1885\n","[2025-04-22 19:05:48] Eval resource usage: CPU 59.5%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 19:05:48] Evaluation completed in 144.73s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:05:49] Plotly metrics visualization saved to plots/training_metrics.html\n","[2025-04-22 19:05:49] Checkpoint saved: models/model_iter_305.pt\n","[2025-04-22 19:05:49] Saved placement network to models/placement_network.pt\n","[2025-04-22 19:05:50] Replay buffer saved: models/latest_buffer.pkl (50000 examples, 201.8 MB)\n","[2025-04-22 19:05:50] Iteration 305 done in 454.89s\n","[2025-04-22 19:05:50] Resource usage: CPU 9.4%, RAM 12.3%, GPU peak memory 0.00 GB\n","[2025-04-22 19:05:50] Network parameter sum after training: 23622.341337\n","[2025-04-22 19:05:50] \n","=== Iteration 306/300 ===\n","[2025-04-22 19:05:50] Starting self-play...\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Worker 0] Network parameter sum: 23622.340637\n"]},{"output_type":"stream","name":"stderr","text":["\rSelf-play games:   0%|          | 0/22 [00:44<?, ?it/s]Process ForkPoolWorker-3:\n","Process ForkPoolWorker-8:\n","Process ForkPoolWorker-5:\n","Process ForkPoolWorker-4:\n","Process ForkPoolWorker-1:\n","Process ForkPoolWorker-10:\n","Process ForkPoolWorker-2:\n","Process ForkPoolWorker-11:\n","Process ForkPoolWorker-9:\n","Process ForkPoolWorker-7:\n","Process ForkPoolWorker-6:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line None, in search\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","KeyboardInterrupt\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 112, in expand\n","    base_state = copy.deepcopy(self.game_state)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 259, in _reconstruct\n","    def _reconstruct(x, memo, func, args,\n","    \n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 112, in expand\n","    base_state = copy.deepcopy(self.game_state)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 121, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 112, in expand\n","    base_state = copy.deepcopy(self.game_state)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:06:35] Training interrupted by user; saving current model...\n"]},{"output_type":"stream","name":"stderr","text":["  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 127, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 112, in expand\n","    base_state = copy.deepcopy(self.game_state)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 112, in expand\n","    base_state = copy.deepcopy(self.game_state)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 112, in expand\n","    base_state = copy.deepcopy(self.game_state)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 311, in search\n","    state_encoded = self.state_encoder.encode_state(node.game_state)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/model/state_encoder.py\", line 117, in encode_state\n","    state.extend(settlement_type_one_hot)\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 112, in expand\n","    base_state = copy.deepcopy(self.game_state)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 206, in _deepcopy_list\n","    append(deepcopy(a, memo))\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","      ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","      ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 144, in deepcopy\n","    copier = _deepcopy_dispatch.get(cls)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 144, in deepcopy\n","    copier = _deepcopy_dispatch.get(cls)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 206, in _deepcopy_list\n","    append(deepcopy(a, memo))\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 265, in _reconstruct\n","    y = func(*args)\n","        ^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 144, in deepcopy\n","    copier = _deepcopy_dispatch.get(cls)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 177, in deepcopy\n","    _keep_alive(x, memo) # Make sure x lives at least as long as d\n","    ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n","    y = _reconstruct(x, memo, *rv)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 264, in <genexpr>\n","    args = (deepcopy(arg, memo) for arg in args)\n","            ^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.11/copy.py\", line 243, in _keep_alive\n","    def _keep_alive(x, memo):\n","    \n","  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n","    state = deepcopy(state, memo)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 144, in deepcopy\n","    copier = _deepcopy_dispatch.get(cls)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n","    y = copier(x, memo)\n","        ^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n","    y[deepcopy(key, memo)] = deepcopy(value, memo)\n","                             ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/copy.py\", line 138, in deepcopy\n","    y = memo.get(d, _nil)\n","        ^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:06:35] Checkpoint saved: models/model_iter_305.pt\n","[2025-04-22 19:06:35] Saved placement network to models/placement_network.pt\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n","    result = (True, func(*args, **kwds))\n","                    ^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py\", line 98, in play_one_game_entry\n","    game.process_ai_turn()\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 121, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 121, in process_ai_turn\n","    action = agent.get_action(state)\n","             ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/agent/alpha_agent.py\", line 388, in get_action\n","    action_probs, value_estimate = self.mcts.search(state)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 327, in search\n","    self._process_evaluation(eval_node, eval_path, policy, value)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 121, in expand\n","    success = self._apply_action(next_state, action)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 404, in _process_evaluation\n","    node.expand(action_priors)\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 121, in expand\n","    success = self._apply_action(next_state, action)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 173, in _apply_action\n","    temp_game = GameLogic(state.board)\n","                ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 12, in __init__\n","    self.state = GameState(board)\n","                 ^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/game_state.py\", line 22, in __init__\n","    self.dev_card_deck = DevelopmentCardDeck()\n","                         ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/development_card.py\", line 23, in __init__\n","    self.cards.append(DevelopmentCard(DevCardType.KNIGHT, \"Knight\"))\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/AlphaZero/core/mcts.py\", line 173, in _apply_action\n","    temp_game = GameLogic(state.board)\n","                ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/CatanRL/game/development_card.py\", line 7, in __init__\n","    def __init__(self, card_type: DevCardType, name: str, turn_bought: int = None):\n","    \n","  File \"/content/drive/MyDrive/CatanRL/game/game_logic.py\", line 12, in __init__\n","    self.state = GameState(board)\n","                 ^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","  File \"/content/drive/MyDrive/CatanRL/game/game_state.py\", line 22, in __init__\n","    self.dev_card_deck = DevelopmentCardDeck()\n","                         ^^^^^^^^^^^^^^^^^^^^^\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","  File \"/content/drive/MyDrive/CatanRL/game/development_card.py\", line 45, in __init__\n","    random.shuffle(self.cards)\n","  File \"/usr/lib/python3.11/random.py\", line 380, in shuffle\n","    for i in reversed(range(1, len(x))):\n","                      ^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n","/content/drive/MyDrive/CatanRL/AlphaZero/training/self_play.py:31: FutureWarning:\n","\n","You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\n"]},{"output_type":"stream","name":"stdout","text":["[2025-04-22 19:06:39] Replay buffer saved: models/latest_buffer.pkl (50000 examples, 201.8 MB)\n","[2025-04-22 19:06:39] \n","=== Training Finished (8620.94s / 2.39h) ===\n","[2025-04-22 19:06:40] Checkpoint saved: models/model_iter_305.pt\n","[2025-04-22 19:06:40] Saved placement network to models/placement_network.pt\n","[2025-04-22 19:06:43] Replay buffer saved: models/latest_buffer.pkl (50000 examples, 201.8 MB)\n","[2025-04-22 19:06:46] Plotly metrics visualization saved to plots/training_metrics.html\n"]}],"source":["timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","device = torch.device('cpu')\n","\n","# Set random seeds for reproducibility\n","def set_random_seeds(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    # if torch.cuda.is_available():\n","    #     torch.cuda.manual_seed(seed)\n","    #     torch.backends.cudnn.deterministic = True\n","    #     torch.backends.cudnn.benchmark = False\n","\n","set_random_seeds()\n","\n","# Step 5: Set up training parameters\n","# You can customize these parameters\n","import argparse\n","\n","# Parse arguments from command line or use defaults\n","# This allows you to change parameters when running the notebook\n","parser = argparse.ArgumentParser(description=\"AlphaZero Catan Training\")\n","parser.add_argument(\"--iterations\", type=int, default=50, help=\"Number of training iterations\")\n","parser.add_argument(\"--resume\", type=str, default=None, help=\"Path to checkpoint to resume from\")\n","parser.add_argument(\"--games\", type=int, default=20, help=\"Number of self-play games per iteration\")\n","parser.add_argument(\"--sims\", type=int, default=100, help=\"Number of MCTS simulations per move\")\n","parser.add_argument(\"--eval-games\", type=int, default=10, help=\"Number of evaluation games\")\n","parser.add_argument(\"--quick\", action=\"store_true\", help=\"Quick training (1 iteration, 2 games)\")\n","parser.add_argument(\"--medium\", action=\"store_true\", help=\"Medium training (10 iterations, 5 games)\")\n","parser.add_argument(\"--full\", action=\"store_true\", help=\"Full training (50 iterations, 20 games)\")\n","parser.add_argument(\"--overnight\", action=\"store_true\", help=\"Overnight training (100 iterations, 30 games)\")\n","\n","# Parse the arguments directly\n","args = parser.parse_args(['--overnight', '--resume', 'models/model_iter_280.pt'])\n","#just overnight no resume\n","# args = parser.parse_args(['--medium'])\n","# Configure training mode\n","if args.quick:\n","    print(\"Running in QUICK mode\")\n","    args.iterations = 1\n","    args.games = 2\n","    args.sims = 10\n","    args.eval_games = 2\n","elif args.medium:\n","    print(\"Running in MEDIUM mode\")\n","    args.iterations = 10\n","    args.games = 5\n","    args.sims = 50\n","    args.eval_games = 5\n","elif args.full:\n","    print(\"Running in FULL mode\")\n","    args.iterations = 50\n","    args.games = 20\n","    args.sims = 100\n","    args.eval_games = 10\n","elif args.overnight:\n","    print(\"Running in OVERNIGHT mode\")\n","    args.iterations = 300\n","    args.games = 22\n","    args.sims = 150\n","    args.eval_games = 10\n","\n","print(f\"\\n=== AlphaZero Catan Training ===\")\n","print(f\"Iterations: {args.iterations}\")\n","print(f\"Self-play games per iteration: {args.games}\")\n","print(f\"MCTS simulations per move: {args.sims}\")\n","print(f\"Resume from: {args.resume if args.resume else 'Starting fresh'}\")\n","\n","# Step 6: Get configuration and modify for GPU\n","from AlphaZero.utils.config import get_config\n","config = get_config()\n","\n","# Customize config with command line arguments\n","config['num_iterations'] = args.iterations\n","config['self_play_games'] = args.games\n","config['num_simulations'] = args.sims\n","config['eval_games'] = args.eval_games\n","config['device'] = 'cpu'\n","config['starting_iter'] = 1 if args.resume is None else int(args.resume.split('_')[-1].split('.')[0]) + 1\n","\n","# Step 7: Create logs and models directories\n","!mkdir -p logs\n","!mkdir -p models\n","!mkdir -p plots\n","\n","# Step 8: Start the training\n","from AlphaZero.training.training_pipeline import TrainingPipeline\n","\n","try:\n","    # Start time tracking\n","    start_time = time.time()\n","\n","    # Create the training pipeline\n","    pipeline = TrainingPipeline(config)\n","\n","    # Train for the specified iterations\n","    pipeline.train(args.iterations, resume_from=args.resume)\n","\n","    # Calculate total training time\n","    total_time = time.time() - start_time\n","    hours = int(total_time // 3600)\n","    minutes = int((total_time % 3600) // 60)\n","    seconds = int(total_time % 60)\n","\n","    print(f\"\\nTraining completed in {hours}h {minutes}m {seconds}s\")\n","\n","except KeyboardInterrupt:\n","    print(\"\\nTraining interrupted! Saving checkpoint...\")\n","    pipeline.save_model(pipeline.current_iteration)\n","    print(\"Checkpoint saved. You can resume with this checkpoint later.\")\n","except Exception as e:\n","    print(f\"Error during training: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","# Step 9: Copy results back to Google Drive\n","!mkdir -p {DRIVE_PATH}/models_{timestamp}\n","!mkdir -p {DRIVE_PATH}/logs_{timestamp}\n","!mkdir -p {DRIVE_PATH}/plots_{timestamp}\n","\n","!cp -r models/* {DRIVE_PATH}/models_{timestamp}/\n","!cp -r logs/* {DRIVE_PATH}/logs_{timestamp}/\n","!cp -r plots/* {DRIVE_PATH}/plots_{timestamp}/\n","\"\"\n","print(f\"\\nTraining results saved to Google Drive in folders with timestamp {timestamp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHuFkqUoxFQS"},"outputs":[],"source":["# ===== CPU info, (self-play is cpu based for multiprocessing) =====\n","# ===== CPU model, core / thread counts, and base turbo freq =====\n","!lscpu | egrep 'Model name|Socket|Thread|Core|MHz'\n","\n","# ===== Current clock speed of every logical core (updates once) =====\n","!grep \\\"cpu MHz\\\" /proc/cpuinfo | head\n","\n","# ===== Simple “how fast is it?” micro‑benchmark =====\n","import time, numpy as np\n","N = 6000\n","a = np.random.randn(N, N).astype(np.float32)\n","b = np.random.randn(N, N).astype(np.float32)\n","\n","t0 = time.time()\n","c = a @ b          # single BLAS call – leverages all cores & any MKL/OPENBLAS\n","elapsed = time.time() - t0\n","gflops = 2*N**3 / elapsed / 1e9\n","\n","print(f\"\\n{elapsed:.3f} s   ≈ {gflops:.1f} GFLOP/s (single large mat‑mul)\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSyRcbrx8dKw"},"outputs":[],"source":["!grep -m1 'model name' /proc/cpuinfo\n","!nproc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTO50m079uNh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyO7gqUgjEkjkX3czq7Xi8bR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}